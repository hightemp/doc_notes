- –ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç (–ò–ò)  Artificial intelligence (AI)
	- –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ¬†(ML)
		- –î–µ–¥—É–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
		- –ò–Ω–¥—É–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
			- –û–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º
				- Linear Regression
				- Logistic Regression
				- Decision Trees
				- K-Nearest Neighbors
				- Naive Bayes
				- Support Vector Machines
				- Ensemble Learning Techniques
			- –û–±—É—á–µ–Ω–∏–µ –±–µ–∑ —É—á–∏—Ç–µ–ª—è
			- –û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º
			- –ê–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
---

![[Pasted image 20230226134948.png]]

---

- NATURAL LANGUAGE PROCESSING
	- Use tokenizers from ü§ó Tokenizers
	- Inference for multilingual models
	- Text generation strategies
	- TASK GUIDES
		- Text classification
		- Token classification
		- Question answering
		- Language modeling
		- Translation
		- Summarization
		- Multiple choice
- AUDIO
	- Audio classification
	- Automatic speech recognition
- COMPUTER VISION
	- Image classification
	- Semantic segmentation
	- Video classification
	- Object detection

**Data Pre-Processing**  
_1. Projection_  
- Principal Component Analysis (PCA)  
- Singular Value Decomposition (SVD)

_2. Feature Engineering_

1.  Feature Selection
    
    -   Filter based methods
    -   Wrapper methods
    -   Embedding methods
2.  Feature Extraction
    
3.  Feature Scaling
    
    -   Standard Scaler
    -   MinMax Scaler

_3. Data Resampling_  
- Undersampling  
- Oversampling

_4. Data imputation_  
- Mean/Median imputer  
- Most Frequent imputer  
- KNN Imputer  
- MICE Imputer

**Regression**

1.  Linear Regression
2.  Polynomial Regression
3.  Ridge Regression
4.  Lasso Regression
5.  Quantile Regression

**Classification**

1.  Logistic Regression
2.  K-Nearest Neighbours (KNN)
3.  Support Vector Machines (SVM)
4.  Naive Bayes
5.  Decision Tree classifier

**Clustering**

1.  K-Means
2.  Mixture Models

**Ensemble Learning**

1.  Bagging
    -   RandomForest
2.  Boosting
    -   Gradient Boost
    -   Ada Boost
    -   XG Boost

**Time Series Analysis**

1.  Naive Approach
2.  Moving Average
3.  Simple Exponential Smoothing
4.  Holt's Linear Trend Model
5.  Holt's Winter Model
6.  ARIMA
7.  SARIMAX

---
- 
- –ó–∞–¥–∞—á–∏ –∞–Ω–∞–ª–∏–∑–∞ –∏ —Å–∏–Ω—Ç–µ–∑–∞ –≤ –∫–æ–º–ø–ª–µ–∫—Å–µ:
- –ú–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ—Ñ–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ, –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–ª–∏ —É–ø—Ä–æ—â–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
- –û–±—â–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è:
- –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤
- –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π —Å–∏–º–≤–æ–ª–æ–≤
- –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π
- –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∞—Å—Ç–µ–π —Ä–µ—á–∏ —Å–ª–æ–≤
- –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ñ—Ä–∞–∑
- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞
- –°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è
- –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è
- –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω–æ–π —Ä–µ—á–∏
- –ú–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥
- –û–±–æ–±—â–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
- –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏
- –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞
- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
- –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫
- –ê–Ω–∞–ª–∏–∑ –≤—ã—Å–∫–∞–∑—ã–≤–∞–Ω–∏–π
- –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞
- –í–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã
- –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
- –°–∏–Ω—Ç–µ–∑ —Ä–µ—á–∏
- –æ–±—Ä–∞–±–æ—Ç–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞
- Neural architecture search
- Data preparation and ingestion (from raw data and miscellaneous formats)
- Column type detection; e.g., boolean, discrete numerical, continuous numerical, or text
- Column intent detection; e.g., target/label, stratification field, numerical feature, categorical text feature, or free text feature
- Task detection; e.g., binary classification, regression, clustering, or ranking
- Feature engineering
- Feature selection
- Feature extraction
- Meta-learning and transfer learning
- Detection and handling of skewed data and/or missing values
- Model selection - choosing which machine learning algorithm to use, often including multiple competing software implementations
- Ensembling - a form of consensus where using multiple models often gives better results than any single model[4]
- Hyperparameter optimization of the learning algorithm and featurization
- Pipeline selection under time, memory, and complexity constraints
- Selection of evaluation metrics and validation procedures
- Problem checking
- Leakage detection
- Misconfiguration detection
- Analysis of obtained results
- Creating user interfaces and visualizations
- Automated machine learning
- –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
- –ê–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏–µ –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏
- –ë–∞–π–µ—Å–æ–≤—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã
- –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
- –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
- –ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏
- –û—Ü–µ–Ω–∏–≤–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã—Ö —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π
- –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ
- –†–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑
- –†–µ—à–∞—é—â–∏–µ –¥–µ—Ä–µ–≤—å—è
- –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
- –¢–µ–æ—Ä–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
- BigARTM
- Computational Learning Theory
- International Conference on Machine Learning
- Journal of Machine Learning Research
- Neural Information Processing Systems
- TopicNet
- –ê–¥–¥–∏—Ç–∏–≤–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π
- –ê–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è
- –ë–∞–π–µ—Å–æ–≤—Å–∫–∏–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
- –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –ø–æ —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–º –¥–∞–Ω–Ω—ã–º
- –í—ã–±–æ—Ä–∫–∞
- –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
- –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
- –ö–æ–ª–ª–µ–∫—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è
- –ö—Ä–∏–ø—Ç–æ–≥—Ä–∞—Ñ–∏—è –∏ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
- –õ–∏–Ω–µ–π–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
- –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è
- –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
- –ú–µ—Ç–æ–¥ –≥–ª–∞–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
- –ú–µ—Ç–æ–¥ —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞
- –ú–µ—Ç—Ä–∏—á–µ—Å–∫–∏–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
- –ú–Ω–æ–≥–æ–º–µ—Ä–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –∏ –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–æ—Ä–∏–∏ —Å–ª—É—á–∞–π–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π
- –ú–æ–¥–µ–ª—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
- –ù–µ–π—Ä–æ—Å–µ—Ç—å
- –û–±—É—á–µ–Ω–∏–µ –±–µ–∑ —É—á–∏—Ç–µ–ª—è
- –û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º
- –û–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º
- –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ
- –ü—Ä–∏–∑–Ω–∞–∫–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
- –ü—Ä–æ–∫–ª—è—Ç–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
- –ü—Ä–æ—Å—Ç–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º —Å–∏–Ω–≥—É–ª—è—Ä–Ω–æ–≥–æ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏—è
- –°–∫–æ–ª—å–∑—è—â–∏–π –∫–æ–Ω—Ç—Ä–æ–ª—å
- –°—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫
- –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
- –¢–µ–æ—Ä–∏—è –í–∞–ø–Ω–∏–∫–∞-–ß–µ—Ä–≤–æ–Ω–µ–Ω–∫–∏—Å–∞
- –¢–µ–æ—Ä–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
- –¢—Ä–∞–Ω—Å–¥—É–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
- –ß–∞—Å—Ç–∏—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
- –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
- –≠–Ω—Ü–∏–∫–ª–æ–ø–µ–¥–∏—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö
- –ë—É—Å—Ç–∏–Ω–≥ boosting¬†‚Äî —É–ª—É—á—à–µ–Ω–∏–µ