<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>VuePress</title>
    <meta name="generator" content="VuePress 1.9.9">
    
    <meta name="description" content="">
    
    <link rel="preload" href="/assets/css/0.styles.e8e4f3a3.css" as="style"><link rel="preload" href="/assets/js/app.dec9f71a.js" as="script"><link rel="preload" href="/assets/js/2.733019b2.js" as="script"><link rel="preload" href="/assets/js/15.0cb65b1e.js" as="script"><link rel="prefetch" href="/assets/js/10.3c45a7ae.js"><link rel="prefetch" href="/assets/js/100.945d3075.js"><link rel="prefetch" href="/assets/js/101.c304dc04.js"><link rel="prefetch" href="/assets/js/102.c4ea246f.js"><link rel="prefetch" href="/assets/js/103.3fa17de4.js"><link rel="prefetch" href="/assets/js/104.10b346d7.js"><link rel="prefetch" href="/assets/js/105.37af8494.js"><link rel="prefetch" href="/assets/js/106.580b656c.js"><link rel="prefetch" href="/assets/js/107.bbf35ca8.js"><link rel="prefetch" href="/assets/js/108.ef31db2d.js"><link rel="prefetch" href="/assets/js/109.e0b67167.js"><link rel="prefetch" href="/assets/js/11.2369400d.js"><link rel="prefetch" href="/assets/js/110.f5f9eacf.js"><link rel="prefetch" href="/assets/js/111.a566b9a5.js"><link rel="prefetch" href="/assets/js/112.9872e360.js"><link rel="prefetch" href="/assets/js/113.55c7a352.js"><link rel="prefetch" href="/assets/js/114.12ee164e.js"><link rel="prefetch" href="/assets/js/115.1fb2093f.js"><link rel="prefetch" href="/assets/js/116.d46141e0.js"><link rel="prefetch" href="/assets/js/117.d60c7d89.js"><link rel="prefetch" href="/assets/js/118.c3b7612c.js"><link rel="prefetch" href="/assets/js/119.10367318.js"><link rel="prefetch" href="/assets/js/12.286d27d6.js"><link rel="prefetch" href="/assets/js/120.be792f55.js"><link rel="prefetch" href="/assets/js/121.9b95acc0.js"><link rel="prefetch" href="/assets/js/122.eaab2843.js"><link rel="prefetch" href="/assets/js/123.cc552dc6.js"><link rel="prefetch" href="/assets/js/124.cce7d459.js"><link rel="prefetch" href="/assets/js/125.c5c5f203.js"><link rel="prefetch" href="/assets/js/126.9fb20891.js"><link rel="prefetch" href="/assets/js/127.06235e8b.js"><link rel="prefetch" href="/assets/js/128.a7d21829.js"><link rel="prefetch" href="/assets/js/129.cf5cb2cd.js"><link rel="prefetch" href="/assets/js/13.47981763.js"><link rel="prefetch" href="/assets/js/130.53cfb33f.js"><link rel="prefetch" href="/assets/js/131.3a8b27b3.js"><link rel="prefetch" href="/assets/js/132.872dde3d.js"><link rel="prefetch" href="/assets/js/133.82865bbc.js"><link rel="prefetch" href="/assets/js/134.f3b5d42f.js"><link rel="prefetch" href="/assets/js/135.4c91805f.js"><link rel="prefetch" href="/assets/js/136.dcb8f9e2.js"><link rel="prefetch" href="/assets/js/137.07d2bab8.js"><link rel="prefetch" href="/assets/js/138.ea94033d.js"><link rel="prefetch" href="/assets/js/139.79ad652e.js"><link rel="prefetch" href="/assets/js/14.2f49c0ab.js"><link rel="prefetch" href="/assets/js/140.d6518035.js"><link rel="prefetch" href="/assets/js/141.6b166b92.js"><link rel="prefetch" href="/assets/js/142.c4466c2b.js"><link rel="prefetch" href="/assets/js/143.141061b6.js"><link rel="prefetch" href="/assets/js/144.5cee20dc.js"><link rel="prefetch" href="/assets/js/145.97956c27.js"><link rel="prefetch" href="/assets/js/146.23cb4066.js"><link rel="prefetch" href="/assets/js/147.bb1e6c1a.js"><link rel="prefetch" href="/assets/js/148.a5bd7077.js"><link rel="prefetch" href="/assets/js/149.57694ad8.js"><link rel="prefetch" href="/assets/js/150.faf7fa9f.js"><link rel="prefetch" href="/assets/js/151.2ee47c84.js"><link rel="prefetch" href="/assets/js/152.9a82c536.js"><link rel="prefetch" href="/assets/js/153.5bd8c4a5.js"><link rel="prefetch" href="/assets/js/154.7cdc00ab.js"><link rel="prefetch" href="/assets/js/155.a2a1c88e.js"><link rel="prefetch" href="/assets/js/156.92796b90.js"><link rel="prefetch" href="/assets/js/157.f247bf22.js"><link rel="prefetch" href="/assets/js/158.eaeb08c7.js"><link rel="prefetch" href="/assets/js/159.555a6593.js"><link rel="prefetch" href="/assets/js/16.6fd62cc7.js"><link rel="prefetch" href="/assets/js/160.bf19eb80.js"><link rel="prefetch" href="/assets/js/161.7ce1d12b.js"><link rel="prefetch" href="/assets/js/162.f6dc5f4b.js"><link rel="prefetch" href="/assets/js/163.16df5c09.js"><link rel="prefetch" href="/assets/js/164.bdd3ee47.js"><link rel="prefetch" href="/assets/js/165.ba85290c.js"><link rel="prefetch" href="/assets/js/166.6b1a560c.js"><link rel="prefetch" href="/assets/js/167.59b9445e.js"><link rel="prefetch" href="/assets/js/168.69cc230d.js"><link rel="prefetch" href="/assets/js/169.55882eed.js"><link rel="prefetch" href="/assets/js/17.94447945.js"><link rel="prefetch" href="/assets/js/170.129124fc.js"><link rel="prefetch" href="/assets/js/171.cf1a85f9.js"><link rel="prefetch" href="/assets/js/172.36f3cdfc.js"><link rel="prefetch" href="/assets/js/173.cddbfa1b.js"><link rel="prefetch" href="/assets/js/174.a32fe0ad.js"><link rel="prefetch" href="/assets/js/175.c1414217.js"><link rel="prefetch" href="/assets/js/176.a678371f.js"><link rel="prefetch" href="/assets/js/177.583b2ac2.js"><link rel="prefetch" href="/assets/js/178.6dd3c8d6.js"><link rel="prefetch" href="/assets/js/179.dac91cd2.js"><link rel="prefetch" href="/assets/js/18.75e97365.js"><link rel="prefetch" href="/assets/js/180.9dfd9728.js"><link rel="prefetch" href="/assets/js/181.9afd854f.js"><link rel="prefetch" href="/assets/js/182.d0dbacf1.js"><link rel="prefetch" href="/assets/js/183.1dbeb096.js"><link rel="prefetch" href="/assets/js/184.27b34aeb.js"><link rel="prefetch" href="/assets/js/185.13641773.js"><link rel="prefetch" href="/assets/js/186.d79c8675.js"><link rel="prefetch" href="/assets/js/187.484bb501.js"><link rel="prefetch" href="/assets/js/188.f73564d2.js"><link rel="prefetch" href="/assets/js/189.25f0971d.js"><link rel="prefetch" href="/assets/js/19.9abedc68.js"><link rel="prefetch" href="/assets/js/190.00b862d0.js"><link rel="prefetch" href="/assets/js/191.a255f76b.js"><link rel="prefetch" href="/assets/js/192.6b6a1c68.js"><link rel="prefetch" href="/assets/js/193.916ffb26.js"><link rel="prefetch" href="/assets/js/194.2c928fb3.js"><link rel="prefetch" href="/assets/js/195.20a9fc51.js"><link rel="prefetch" href="/assets/js/196.f2abf901.js"><link rel="prefetch" href="/assets/js/197.871f3d6a.js"><link rel="prefetch" href="/assets/js/198.3ea5cf81.js"><link rel="prefetch" href="/assets/js/199.3f37d030.js"><link rel="prefetch" href="/assets/js/20.1c33d690.js"><link rel="prefetch" href="/assets/js/200.7e20b89c.js"><link rel="prefetch" href="/assets/js/201.ae9ed05e.js"><link rel="prefetch" href="/assets/js/202.d686563a.js"><link rel="prefetch" href="/assets/js/203.5a9ac59e.js"><link rel="prefetch" href="/assets/js/204.4abf0169.js"><link rel="prefetch" href="/assets/js/205.94fe861e.js"><link rel="prefetch" href="/assets/js/206.76e93465.js"><link rel="prefetch" href="/assets/js/207.92c5e562.js"><link rel="prefetch" href="/assets/js/208.79f31d54.js"><link rel="prefetch" href="/assets/js/209.eb534898.js"><link rel="prefetch" href="/assets/js/21.de3ad1f7.js"><link rel="prefetch" href="/assets/js/210.38242c09.js"><link rel="prefetch" href="/assets/js/211.ca8aa9a4.js"><link rel="prefetch" href="/assets/js/212.189fa4e2.js"><link rel="prefetch" href="/assets/js/213.239f2bd6.js"><link rel="prefetch" href="/assets/js/214.b1a9ece6.js"><link rel="prefetch" href="/assets/js/215.823222d2.js"><link rel="prefetch" href="/assets/js/216.6c3fbaad.js"><link rel="prefetch" href="/assets/js/217.c2bd084b.js"><link rel="prefetch" href="/assets/js/218.7ad57fcc.js"><link rel="prefetch" href="/assets/js/219.4ef0266b.js"><link rel="prefetch" href="/assets/js/22.a45331de.js"><link rel="prefetch" href="/assets/js/220.e9fd4c5f.js"><link rel="prefetch" href="/assets/js/221.0b0f609a.js"><link rel="prefetch" href="/assets/js/222.ff25599e.js"><link rel="prefetch" href="/assets/js/223.8319d2d0.js"><link rel="prefetch" href="/assets/js/224.da5c4d3e.js"><link rel="prefetch" href="/assets/js/225.9378291a.js"><link rel="prefetch" href="/assets/js/226.0d8b2ef5.js"><link rel="prefetch" href="/assets/js/227.82cd2e2d.js"><link rel="prefetch" href="/assets/js/228.db49009d.js"><link rel="prefetch" href="/assets/js/229.e735fec4.js"><link rel="prefetch" href="/assets/js/23.de5de683.js"><link rel="prefetch" href="/assets/js/230.47e7011d.js"><link rel="prefetch" href="/assets/js/231.7f0e5905.js"><link rel="prefetch" href="/assets/js/232.a6cb0759.js"><link rel="prefetch" href="/assets/js/233.09bbd53b.js"><link rel="prefetch" href="/assets/js/234.b9341514.js"><link rel="prefetch" href="/assets/js/235.3563b4bf.js"><link rel="prefetch" href="/assets/js/236.7d5da9ca.js"><link rel="prefetch" href="/assets/js/237.c7a693f3.js"><link rel="prefetch" href="/assets/js/238.803df100.js"><link rel="prefetch" href="/assets/js/239.ef64fcf2.js"><link rel="prefetch" href="/assets/js/24.b363b117.js"><link rel="prefetch" href="/assets/js/240.2befed40.js"><link rel="prefetch" href="/assets/js/241.10a01506.js"><link rel="prefetch" href="/assets/js/242.3b369ac8.js"><link rel="prefetch" href="/assets/js/243.31958eb9.js"><link rel="prefetch" href="/assets/js/244.b9489a7d.js"><link rel="prefetch" href="/assets/js/245.b9b191e5.js"><link rel="prefetch" href="/assets/js/246.18aeb9f6.js"><link rel="prefetch" href="/assets/js/247.39cc492f.js"><link rel="prefetch" href="/assets/js/248.b08e232c.js"><link rel="prefetch" href="/assets/js/249.6d660682.js"><link rel="prefetch" href="/assets/js/25.5e2fe982.js"><link rel="prefetch" href="/assets/js/250.235cee85.js"><link rel="prefetch" href="/assets/js/251.66edac43.js"><link rel="prefetch" href="/assets/js/252.28ec4821.js"><link rel="prefetch" href="/assets/js/253.828cd3b6.js"><link rel="prefetch" href="/assets/js/254.6b19e7b7.js"><link rel="prefetch" href="/assets/js/255.7808e23e.js"><link rel="prefetch" href="/assets/js/256.33572d7d.js"><link rel="prefetch" href="/assets/js/257.8044242f.js"><link rel="prefetch" href="/assets/js/258.2eb93b76.js"><link rel="prefetch" href="/assets/js/259.6d222e72.js"><link rel="prefetch" href="/assets/js/26.b3336cf9.js"><link rel="prefetch" href="/assets/js/260.b3345d12.js"><link rel="prefetch" href="/assets/js/261.6f64a148.js"><link rel="prefetch" href="/assets/js/262.283e9e13.js"><link rel="prefetch" href="/assets/js/263.10934b95.js"><link rel="prefetch" href="/assets/js/264.52a220d8.js"><link rel="prefetch" href="/assets/js/265.3099a02f.js"><link rel="prefetch" href="/assets/js/266.d093c25c.js"><link rel="prefetch" href="/assets/js/267.ffd0ea6c.js"><link rel="prefetch" href="/assets/js/268.71a5b2a5.js"><link rel="prefetch" href="/assets/js/269.e02d0a12.js"><link rel="prefetch" href="/assets/js/27.bacb7462.js"><link rel="prefetch" href="/assets/js/270.717bdd21.js"><link rel="prefetch" href="/assets/js/271.ceef3037.js"><link rel="prefetch" href="/assets/js/272.b040f2d8.js"><link rel="prefetch" href="/assets/js/273.e06cd101.js"><link rel="prefetch" href="/assets/js/274.9c206962.js"><link rel="prefetch" href="/assets/js/275.46ae609d.js"><link rel="prefetch" href="/assets/js/276.16e277a9.js"><link rel="prefetch" href="/assets/js/277.fcb15734.js"><link rel="prefetch" href="/assets/js/278.0cc37dab.js"><link rel="prefetch" href="/assets/js/279.55e66e78.js"><link rel="prefetch" href="/assets/js/28.f9359f93.js"><link rel="prefetch" href="/assets/js/280.e4809dbc.js"><link rel="prefetch" href="/assets/js/281.0889dba5.js"><link rel="prefetch" href="/assets/js/282.459e1cec.js"><link rel="prefetch" href="/assets/js/283.0b2d1fb1.js"><link rel="prefetch" href="/assets/js/284.ffe54aa9.js"><link rel="prefetch" href="/assets/js/285.d9ed13ce.js"><link rel="prefetch" href="/assets/js/286.5574e754.js"><link rel="prefetch" href="/assets/js/287.f9187311.js"><link rel="prefetch" href="/assets/js/288.cd2b56c3.js"><link rel="prefetch" href="/assets/js/289.8552c742.js"><link rel="prefetch" href="/assets/js/29.9a6765b0.js"><link rel="prefetch" href="/assets/js/290.7d0673a7.js"><link rel="prefetch" href="/assets/js/291.5c185731.js"><link rel="prefetch" href="/assets/js/292.fab6d535.js"><link rel="prefetch" href="/assets/js/293.457fd94a.js"><link rel="prefetch" href="/assets/js/294.748e6f29.js"><link rel="prefetch" href="/assets/js/295.99a4bc36.js"><link rel="prefetch" href="/assets/js/296.f02a6f54.js"><link rel="prefetch" href="/assets/js/297.a2899d72.js"><link rel="prefetch" href="/assets/js/298.8e5a5ef7.js"><link rel="prefetch" href="/assets/js/299.eff3af7f.js"><link rel="prefetch" href="/assets/js/3.e54eedac.js"><link rel="prefetch" href="/assets/js/30.8899ae33.js"><link rel="prefetch" href="/assets/js/300.ea47d06a.js"><link rel="prefetch" href="/assets/js/301.0f51c16c.js"><link rel="prefetch" href="/assets/js/302.0672bd76.js"><link rel="prefetch" href="/assets/js/303.99557654.js"><link rel="prefetch" href="/assets/js/304.069ee3f6.js"><link rel="prefetch" href="/assets/js/305.bbbcb69f.js"><link rel="prefetch" href="/assets/js/306.458f04ac.js"><link rel="prefetch" href="/assets/js/307.d8a121f6.js"><link rel="prefetch" href="/assets/js/308.4e353bdf.js"><link rel="prefetch" href="/assets/js/309.41135b53.js"><link rel="prefetch" href="/assets/js/31.907a86ff.js"><link rel="prefetch" href="/assets/js/310.d9900cca.js"><link rel="prefetch" href="/assets/js/311.4c8d1049.js"><link rel="prefetch" href="/assets/js/312.e461412c.js"><link rel="prefetch" href="/assets/js/313.b481fec8.js"><link rel="prefetch" href="/assets/js/314.7c9d7531.js"><link rel="prefetch" href="/assets/js/315.493dd60b.js"><link rel="prefetch" href="/assets/js/316.b892fdb3.js"><link rel="prefetch" href="/assets/js/317.0488dbc0.js"><link rel="prefetch" href="/assets/js/318.e3024dfc.js"><link rel="prefetch" href="/assets/js/319.cbeb3017.js"><link rel="prefetch" href="/assets/js/32.566fa885.js"><link rel="prefetch" href="/assets/js/320.6bbdcb6f.js"><link rel="prefetch" href="/assets/js/321.393ba5b9.js"><link rel="prefetch" href="/assets/js/322.e1ac37db.js"><link rel="prefetch" href="/assets/js/323.ecada34d.js"><link rel="prefetch" href="/assets/js/324.b5a78957.js"><link rel="prefetch" href="/assets/js/325.78a07f6c.js"><link rel="prefetch" href="/assets/js/326.18acb16b.js"><link rel="prefetch" href="/assets/js/327.f1fdf85e.js"><link rel="prefetch" href="/assets/js/328.cb4e6a88.js"><link rel="prefetch" href="/assets/js/329.7064a56d.js"><link rel="prefetch" href="/assets/js/33.089e8d27.js"><link rel="prefetch" href="/assets/js/330.eb5d365c.js"><link rel="prefetch" href="/assets/js/331.7413cc1f.js"><link rel="prefetch" href="/assets/js/332.268810a7.js"><link rel="prefetch" href="/assets/js/333.14b9f266.js"><link rel="prefetch" href="/assets/js/334.79f3c205.js"><link rel="prefetch" href="/assets/js/335.4c1fa2f4.js"><link rel="prefetch" href="/assets/js/336.2a555b98.js"><link rel="prefetch" href="/assets/js/337.c8d1a7eb.js"><link rel="prefetch" href="/assets/js/338.027e65ab.js"><link rel="prefetch" href="/assets/js/339.e913eb02.js"><link rel="prefetch" href="/assets/js/34.5cbcf358.js"><link rel="prefetch" href="/assets/js/340.ccfb0701.js"><link rel="prefetch" href="/assets/js/341.2bf59012.js"><link rel="prefetch" href="/assets/js/342.43214bf6.js"><link rel="prefetch" href="/assets/js/343.860e5acc.js"><link rel="prefetch" href="/assets/js/344.af2b1413.js"><link rel="prefetch" href="/assets/js/345.6dfaf1a7.js"><link rel="prefetch" href="/assets/js/346.4bccf6c8.js"><link rel="prefetch" href="/assets/js/347.c42d14ab.js"><link rel="prefetch" href="/assets/js/348.f00c8f2c.js"><link rel="prefetch" href="/assets/js/349.47f740a6.js"><link rel="prefetch" href="/assets/js/35.e9eab731.js"><link rel="prefetch" href="/assets/js/350.30d3e969.js"><link rel="prefetch" href="/assets/js/351.1295792b.js"><link rel="prefetch" href="/assets/js/352.363b9177.js"><link rel="prefetch" href="/assets/js/353.601c5370.js"><link rel="prefetch" href="/assets/js/354.047a1d22.js"><link rel="prefetch" href="/assets/js/355.3f32b1c2.js"><link rel="prefetch" href="/assets/js/356.1816f3c9.js"><link rel="prefetch" href="/assets/js/357.c81f86b9.js"><link rel="prefetch" href="/assets/js/358.a2fdfdd3.js"><link rel="prefetch" href="/assets/js/359.a5e1fa6a.js"><link rel="prefetch" href="/assets/js/36.a655d7e3.js"><link rel="prefetch" href="/assets/js/360.cecc875b.js"><link rel="prefetch" href="/assets/js/361.c8915051.js"><link rel="prefetch" href="/assets/js/362.5c07a421.js"><link rel="prefetch" href="/assets/js/363.adc2e062.js"><link rel="prefetch" href="/assets/js/364.f0f96ce6.js"><link rel="prefetch" href="/assets/js/365.a4a5cc32.js"><link rel="prefetch" href="/assets/js/366.4dbc70ea.js"><link rel="prefetch" href="/assets/js/367.626ad073.js"><link rel="prefetch" href="/assets/js/368.033096e1.js"><link rel="prefetch" href="/assets/js/369.ba1ab9e8.js"><link rel="prefetch" href="/assets/js/37.46464e89.js"><link rel="prefetch" href="/assets/js/370.5a4de7d9.js"><link rel="prefetch" href="/assets/js/371.8e19304c.js"><link rel="prefetch" href="/assets/js/372.00d24aa5.js"><link rel="prefetch" href="/assets/js/373.5d43f401.js"><link rel="prefetch" href="/assets/js/374.eeea88f6.js"><link rel="prefetch" href="/assets/js/38.5710d281.js"><link rel="prefetch" href="/assets/js/39.22cd4ce1.js"><link rel="prefetch" href="/assets/js/4.d12742be.js"><link rel="prefetch" href="/assets/js/40.ac785c50.js"><link rel="prefetch" href="/assets/js/41.e52206b8.js"><link rel="prefetch" href="/assets/js/42.fbd0e0ce.js"><link rel="prefetch" href="/assets/js/43.e0830f9d.js"><link rel="prefetch" href="/assets/js/44.23c6ba45.js"><link rel="prefetch" href="/assets/js/45.b9a24221.js"><link rel="prefetch" href="/assets/js/46.83cf1192.js"><link rel="prefetch" href="/assets/js/47.220fb3ae.js"><link rel="prefetch" href="/assets/js/48.130760fa.js"><link rel="prefetch" href="/assets/js/49.d6f4ded1.js"><link rel="prefetch" href="/assets/js/5.216cae74.js"><link rel="prefetch" href="/assets/js/50.9cbfe92f.js"><link rel="prefetch" href="/assets/js/51.3446f694.js"><link rel="prefetch" href="/assets/js/52.72649f7e.js"><link rel="prefetch" href="/assets/js/53.6f0f60f7.js"><link rel="prefetch" href="/assets/js/54.a1d865f2.js"><link rel="prefetch" href="/assets/js/55.474dea5b.js"><link rel="prefetch" href="/assets/js/56.34577514.js"><link rel="prefetch" href="/assets/js/57.f6fc2205.js"><link rel="prefetch" href="/assets/js/58.a47a439e.js"><link rel="prefetch" href="/assets/js/59.06b99b85.js"><link rel="prefetch" href="/assets/js/6.2d0a63f8.js"><link rel="prefetch" href="/assets/js/60.20e5a404.js"><link rel="prefetch" href="/assets/js/61.06eb00d3.js"><link rel="prefetch" href="/assets/js/62.ef43b805.js"><link rel="prefetch" href="/assets/js/63.96d7abec.js"><link rel="prefetch" href="/assets/js/64.828a623b.js"><link rel="prefetch" href="/assets/js/65.8270a1b7.js"><link rel="prefetch" href="/assets/js/66.3da895e0.js"><link rel="prefetch" href="/assets/js/67.30e83906.js"><link rel="prefetch" href="/assets/js/68.c5db27df.js"><link rel="prefetch" href="/assets/js/69.561fe0b5.js"><link rel="prefetch" href="/assets/js/7.929bde26.js"><link rel="prefetch" href="/assets/js/70.0b3415d2.js"><link rel="prefetch" href="/assets/js/71.4351b7f3.js"><link rel="prefetch" href="/assets/js/72.ce1be2c6.js"><link rel="prefetch" href="/assets/js/73.d9f4c187.js"><link rel="prefetch" href="/assets/js/74.b8e2035e.js"><link rel="prefetch" href="/assets/js/75.f721109d.js"><link rel="prefetch" href="/assets/js/76.1fffa028.js"><link rel="prefetch" href="/assets/js/77.392e6f02.js"><link rel="prefetch" href="/assets/js/78.788b59e8.js"><link rel="prefetch" href="/assets/js/79.0118ba12.js"><link rel="prefetch" href="/assets/js/8.81ac42c3.js"><link rel="prefetch" href="/assets/js/80.41299197.js"><link rel="prefetch" href="/assets/js/81.d5c7efcc.js"><link rel="prefetch" href="/assets/js/82.0686785a.js"><link rel="prefetch" href="/assets/js/83.1c3c9afc.js"><link rel="prefetch" href="/assets/js/84.77cbfea0.js"><link rel="prefetch" href="/assets/js/85.a72511bb.js"><link rel="prefetch" href="/assets/js/86.ee7de8e7.js"><link rel="prefetch" href="/assets/js/87.6725a162.js"><link rel="prefetch" href="/assets/js/88.de798e46.js"><link rel="prefetch" href="/assets/js/89.e3b19fd4.js"><link rel="prefetch" href="/assets/js/9.0286aa6e.js"><link rel="prefetch" href="/assets/js/90.15a0b0a1.js"><link rel="prefetch" href="/assets/js/91.9c080a3d.js"><link rel="prefetch" href="/assets/js/92.2d7e99de.js"><link rel="prefetch" href="/assets/js/93.3a7c19cd.js"><link rel="prefetch" href="/assets/js/94.da299190.js"><link rel="prefetch" href="/assets/js/95.54d70fa0.js"><link rel="prefetch" href="/assets/js/96.68967f40.js"><link rel="prefetch" href="/assets/js/97.6ec5cb5a.js"><link rel="prefetch" href="/assets/js/98.4ce5bd4a.js"><link rel="prefetch" href="/assets/js/99.de11b345.js">
    <link rel="stylesheet" href="/assets/css/0.styles.e8e4f3a3.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <!----></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><!---->  <!----> </aside> <main class="page"> <div class="theme-default-content content__default"><p>https://github.com/NVIDIA/TensorRT/blob/main/demo/HuggingFace/notebooks/t5.ipynb</p> <h1 id="accelerating-huggingface-t5-inference-with-tensorrt"><a href="#accelerating-huggingface-t5-inference-with-tensorrt" class="header-anchor">#</a> Accelerating HuggingFace T5 Inference with TensorRT</h1> <p>T5 is an encoder-decoder model that converts all NLP problems into a text-to-text format. More specifically, it does so by encoding different tasks as text directives in the input stream. This enables a single model to be trained supervised on a wide variety of NLP tasks such as translation, classification, Q&amp;A and summarization.</p> <p>This notebook shows 3 easy steps to convert a  <a href="https://huggingface.co/transformers/model_doc/t5.html" target="_blank" rel="noopener noreferrer">HuggingFace PyTorch T5 model<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>  to a TensorRT engine for high-performance inference.</p> <ol><li><a href="https://github.com/NVIDIA/TensorRT/blob/59898c103f07e100d3c1108c038d767f5485e0b9/demo/HuggingFace/notebooks/#1" target="_blank" rel="noopener noreferrer">Download HuggingFace T5 model<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li><a href="https://github.com/NVIDIA/TensorRT/blob/59898c103f07e100d3c1108c038d767f5485e0b9/demo/HuggingFace/notebooks/#2" target="_blank" rel="noopener noreferrer">Convert to ONNX format<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li><a href="https://github.com/NVIDIA/TensorRT/blob/59898c103f07e100d3c1108c038d767f5485e0b9/demo/HuggingFace/notebooks/#3" target="_blank" rel="noopener noreferrer">Convert to TensorRT engine<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ol> <h2 id="prerequisite"><a href="#prerequisite" class="header-anchor">#</a> Prerequisite</h2> <p>Follow the instruction at  https://github.com/NVIDIA/TensorRT  to build the TensorRT-OSS docker container required to run this notebook.</p> <p>Next, we install some extra dependencies.</p> <p>In [ ]:</p> <p>%%capture
!pip3 install -r ../requirements.txt</p> <p><strong>Note:</strong>  After this step, you should restart the Jupyter kernel for the change to take effect.</p> <p>In [ ]:</p> <p>import os
import sys
ROOT_DIR = os.path.abspath(&quot;../&quot;)
sys.path.append(ROOT_DIR)</p> <p>import torch
import tensorrt as trt</p> <p># huggingface
from transformers import (
T5ForConditionalGeneration,
T5Tokenizer,
T5Config,
)</p> <h2 id="_1-download-huggingface-t5-model"><a href="#_1-download-huggingface-t5-model" class="header-anchor">#</a> 1. Download HuggingFace T5 model</h2> <p>First, we download the original HuggingFace PyTorch T5 model from HuggingFace model hubs, together with its associated tokernizer.</p> <p>The T5 variants that are suported by TensorRT 8 are: t5-small (60M), t5-base (220M), t5-large (770M), t5-3b(3B), t5-11b(11B)</p> <p>In [ ]:</p> <p>T5_VARIANT = 't5-small' # choices: t5-small | t5-base | t5-large | t5-3b | t5-11b</p> <p>t5_model = T5ForConditionalGeneration.from_pretrained(T5_VARIANT)
tokenizer = T5Tokenizer.from_pretrained(T5_VARIANT)
config = T5Config.from_pretrained(T5_VARIANT, use_cache = False)</p> <p>In [ ]:</p> <p># save model locally
pytorch_model_dir = './models/{}/pytorch'.format(T5_VARIANT)
!mkdir  -p  $pytorch_model_dir</p> <p>t5_model.save_pretrained(pytorch_model_dir)
print(&quot;Pytorch Model saved to {}&quot;.format(pytorch_model_dir))</p> <h3 id="inference-with-pytorch-model"><a href="#inference-with-pytorch-model" class="header-anchor">#</a> Inference with PyTorch model</h3> <p>Next, we will carry out inference with the PyTorch model.</p> <h4 id="single-example-inference"><a href="#single-example-inference" class="header-anchor">#</a> Single example inference</h4> <p>In [ ]:</p> <p>inputs = tokenizer(&quot;translate English to German: That is good.&quot;, return_tensors=&quot;pt&quot;).to('cuda:0')
num_beams = 1</p> <p># inference on a single example
t5_model.to('cuda:0').eval()
with torch.no_grad():
outputs = t5_model(**inputs, labels=inputs[&quot;input_ids&quot;])</p> <p>logits = outputs.logits</p> <p>In [ ]:</p> <p># Generate sequence for an input
outputs = t5_model.to('cuda:0').generate(inputs.input_ids.to('cuda:0'), num_beams=num_beams)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))</p> <h4 id="model-inference-benchmark-encoder-and-decoder-stacks"><a href="#model-inference-benchmark-encoder-and-decoder-stacks" class="header-anchor">#</a> Model inference benchmark: encoder and decoder stacks</h4> <p>For benchmarking purposes, we will employ a helper functions  <code>encoder_inference</code>  and  <code>decoder_inference</code>  which execute the inference repeatedly for the T5 encoder and decoder stacks separately, and measure end to end execution time. Let's take note of this execution time for comparison with TensorRT.</p> <p><code>TimingProfile</code>  is a named tuple that specifies the number of experiments and number of times to call the function per iteration (and number of warm-up calls although it is not used here).</p> <p>In [ ]:</p> <p>from T5.measurements import decoder_inference, encoder_inference, full_inference
from T5.export import T5EncoderTorchFile, T5DecoderTorchFile, T5EncoderTRTEngine, T5DecoderTRTEngine
from NNDF.networks import TimingProfile
from NNDF.torch_utils import expand_inputs_for_beam_search</p> <p>t5_torch_encoder = T5EncoderTorchFile.TorchModule(t5_model.encoder)
t5_torch_decoder = T5DecoderTorchFile.TorchModule(
t5_model.decoder, t5_model.lm_head, t5_model.config
)</p> <p>In [ ]:</p> <p>input_ids = inputs.input_ids</p> <p>encoder_last_hidden_state, encoder_e2e_median_time = encoder_inference(
t5_torch_encoder, input_ids, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)
)
encoder_e2e_median_time</p> <p>In [ ]:</p> <p>_, decoder_e2e_median_time = decoder_inference(
t5_torch_decoder, input_ids, encoder_last_hidden_state, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)
)
decoder_e2e_median_time</p> <h4 id="full-model-inference-and-benchmark"><a href="#full-model-inference-and-benchmark" class="header-anchor">#</a> Full model inference and benchmark</h4> <p>Next, we will try the T5 model for the task of translation from English to German.</p> <p>For benchmarking purposes, we will employ a helper function  <code>full_inference</code>  which executes the inference repeatedly and measures end to end execution time. Let's take note of this execution time for comparison with TensorRT.</p> <p>In [ ]:</p> <p>from T5.T5ModelConfig import T5ModelTRTConfig, T5Metadata
decoder_output, full_e2e_median_runtime = full_inference(
t5_torch_encoder,
t5_torch_decoder,
input_ids,
tokenizer,
TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50),
num_beams=num_beams,
max_length=T5ModelTRTConfig.MAX_SEQUENCE_LENGTH[T5_VARIANT],
)
full_e2e_median_runtime</p> <p>Let us decode the model's output back into text.</p> <p>In [ ]:</p> <p># De-tokenize output to raw text
print(tokenizer.decode(decoder_output[0], skip_special_tokens=True))</p> <h2 id="_2-convert-to-onnx"><a href="#_2-convert-to-onnx" class="header-anchor">#</a> 2. Convert to ONNX</h2> <p>Prior to converting the model to a TensorRT engine, we will first convert the PyTorch model to an intermediate universal format.</p> <p>ONNX is an open format for machine learning and deep learning models. It allows you to convert deep learning and machine learning models from different frameworks such as TensorFlow, PyTorch, MATLAB, Caffe, and Keras to a single format.</p> <p>The steps to convert a PyTorch model to TensorRT are as follows:</p> <ul><li>Convert the pretrained image segmentation PyTorch model into ONNX.</li> <li>Import the ONNX model into TensorRT.</li> <li>Apply optimizations and generate an engine.</li> <li>Perform inference on the GPU.</li></ul> <p>For the T5 model, we will convert the encoder and decoder seperately.</p> <p>In [ ]:</p> <p># helpers
from NNDF.networks import NetworkMetadata, Precision</p> <p>In [ ]:</p> <p>onnx_model_path = './models/{}/ONNX'.format(T5_VARIANT)
!mkdir  -p  $onnx_model_path</p> <p>metadata=NetworkMetadata(variant=T5_VARIANT, precision=Precision(fp16=True), other=T5Metadata(kv_cache=False))</p> <p>encoder_onnx_model_fpath = T5_VARIANT + &quot;-encoder.onnx&quot;
decoder_onnx_model_fpath = T5_VARIANT + &quot;-decoder-with-lm-head.onnx&quot;</p> <p>t5_encoder = T5EncoderTorchFile(t5_model.to('cpu'), metadata)
t5_decoder = T5DecoderTorchFile(t5_model.to('cpu'), metadata)</p> <p>onnx_t5_encoder = t5_encoder.as_onnx_model(
os.path.join(onnx_model_path, encoder_onnx_model_fpath), force_overwrite=False
)
onnx_t5_decoder = t5_decoder.as_onnx_model(
os.path.join(onnx_model_path, decoder_onnx_model_fpath), force_overwrite=False
)</p> <h2 id="_3-convert-to-tensorrt"><a href="#_3-convert-to-tensorrt" class="header-anchor">#</a> 3. Convert to TensorRT</h2> <p>Now we are ready to parse the ONNX encoder and decoder models and convert them to optimized TensorRT engines.</p> <p>Since the models contains dynamic input shapes, we can specify a valid input range with a TensorRT optimization profile.</p> <p>In [ ]:</p> <p>from T5.export import T5DecoderONNXFile, T5EncoderONNXFile
from polygraphy.backend.trt import Profile
from tensorrt import PreviewFeature</p> <p>In [ ]:</p> <p>tensorrt_model_path = './models/{}/tensorrt'.format(T5_VARIANT)
!mkdir  -p  tensorrt_model_path
# Decoder optimization profiles
batch_size = 1
max_sequence_length = T5ModelTRTConfig.MAX_SEQUENCE_LENGTH[T5_VARIANT]
decoder_profile = Profile()
decoder_profile.add(
&quot;input_ids&quot;,
min=(batch_size * num_beams, 1),
opt=(batch_size * num_beams, max_sequence_length // 2),
max=(batch_size * num_beams, max_sequence_length),
)
decoder_profile.add(
&quot;encoder_hidden_states&quot;,
min=(batch_size * num_beams, 1, max_sequence_length),
opt=(batch_size * num_beams, max_sequence_length // 2, max_sequence_length),
max=(batch_size * num_beams, max_sequence_length, max_sequence_length),
)</p> <p># Encoder optimization profiles
encoder_profile = Profile()
encoder_profile.add(
&quot;input_ids&quot;,
min=(batch_size, 1),
opt=(batch_size, max_sequence_length // 2),
max=(batch_size, max_sequence_length),
)</p> <p>In [ ]:</p> <p>preview_dynamic_shapes = True
engine_tag = f&quot;bs{batch_size}&quot;</p> <p>if num_beams &gt; 1:
engine_tag += &quot;-beam{}&quot;.format(num_beams)</p> <p>preview_features = []
if preview_dynamic_shapes:
preview_features = [PreviewFeature.FASTER_DYNAMIC_SHAPES_0805]
engine_tag += &quot;-previewFasterDynamicShapes&quot;</p> <p>encoder_engine_name = os.path.join(tensorrt_model_path, encoder_onnx_model_fpath) + f&quot;-{engine_tag}.engine&quot;.replace(f&quot;-beam{num_beams}&quot;, &quot;&quot;) # encoder engine not affected by beam search
decoder_engine_name = os.path.join(tensorrt_model_path, decoder_onnx_model_fpath) + f&quot;-{engine_tag}.engine&quot;</p> <p>if not os.path.exists(encoder_engine_name):
t5_trt_encoder_engine = T5EncoderONNXFile(os.path.join(onnx_model_path, encoder_onnx_model_fpath), metadata).as_trt_engine(
encoder_engine_name,
profiles=[encoder_profile],
preview_features=preview_features)
else:
t5_trt_encoder_engine = T5EncoderTRTEngine(encoder_engine_name, metadata)</p> <p>if not os.path.exists(decoder_engine_name):
t5_trt_decoder_engine = T5DecoderONNXFile(os.path.join(onnx_model_path, decoder_onnx_model_fpath), metadata).as_trt_engine(
decoder_engine_name,
profiles=[decoder_profile],
preview_features=preview_features)
else:
t5_trt_decoder_engine = T5DecoderTRTEngine(decoder_engine_name, metadata)</p> <h3 id="inference-with-tensorrt-engine"><a href="#inference-with-tensorrt-engine" class="header-anchor">#</a> Inference with TensorRT engine</h3> <p>Great, if you have reached this stage, it means we now have an optimized TensorRT engine for the T5 model, ready for us to carry out inference.</p> <h4 id="single-example-inference-2"><a href="#single-example-inference-2" class="header-anchor">#</a> Single example inference</h4> <p>The T5 model with TensorRT backend can now be employed in place of the original HuggingFace T5 model.</p> <p>In [ ]:</p> <p># Initialize TensorRT engines
from T5.trt import T5TRTEncoder, T5TRTDecoder</p> <p>t5_trt_encoder = T5TRTEncoder(
t5_trt_encoder_engine, metadata, config
)
t5_trt_decoder = T5TRTDecoder(
t5_trt_decoder_engine, metadata, config, num_beams=num_beams
)</p> <p>In [ ]:</p> <p># Inference on a single sample
encoder_last_hidden_state = t5_trt_encoder(input_ids=input_ids)
outputs = t5_trt_decoder(
expand_inputs_for_beam_search(input_ids, num_beams) if num_beams &gt; 1 else input_ids,
expand_inputs_for_beam_search(encoder_last_hidden_state, num_beams) if num_beams &gt; 1 else encoder_last_hidden_state)</p> <p>In [ ]:</p> <p># Generate sequence for an input
max_length = 64</p> <p>decoder_input_ids = torch.full(
(1, 1), tokenizer.convert_tokens_to_ids(tokenizer.pad_token), dtype=torch.int32
).to(&quot;cuda:0&quot;)</p> <p>encoder_last_hidden_state = t5_trt_encoder(input_ids=input_ids)</p> <h4 id="trt-engine-inference-benchmark-encoder-and-decoder-stacks"><a href="#trt-engine-inference-benchmark-encoder-and-decoder-stacks" class="header-anchor">#</a> TRT engine inference benchmark: encoder and decoder stacks</h4> <p>First, we will bechmark the encoder and decoder stacks as before.</p> <p>In [ ]:</p> <p>encoder_last_hidden_state, encoder_e2e_median_time = encoder_inference(
t5_trt_encoder, input_ids, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)
)
encoder_e2e_median_time</p> <p>In [ ]:</p> <p>_, decoder_e2e_median_time = decoder_inference(
t5_trt_decoder, expand_inputs_for_beam_search(input_ids, num_beams) if num_beams &gt; 1 else input_ids,
expand_inputs_for_beam_search(encoder_last_hidden_state, num_beams) if num_beams &gt; 1 else encoder_last_hidden_state, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)
)
decoder_e2e_median_time</p> <h3 id="full-model-inference-benchmark"><a href="#full-model-inference-benchmark" class="header-anchor">#</a> Full model inference benchmark</h3> <p>Next, we will try the full TensorRT T5 engine for the task of translation. As before, note the time difference.</p> <p>In [ ]:</p> <p>decoder_output, full_e2e_median_runtime = full_inference(
t5_trt_encoder,
t5_trt_decoder,
input_ids,
tokenizer,
TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50),
max_length=T5ModelTRTConfig.MAX_SEQUENCE_LENGTH[metadata.variant],
num_beams=num_beams,
use_cuda=True,
)</p> <p>print(tokenizer.decode(decoder_output[0], skip_special_tokens=True))
full_e2e_median_runtime</p> <p>You can now compare the output of the original PyTorch model and the TensorRT engine. Notice the speed difference. On an NVIDIA V100 32GB GPU, this results in upto ~10x performance improvement (from 0.0802s to 0.0082s for the T5-small variant).</p> <h2 id="conclusion-and-where-to-next"><a href="#conclusion-and-where-to-next" class="header-anchor">#</a> Conclusion and where-to next?</h2> <p>This notebook has walked you through the process of converting a HuggingFace PyTorch T5 model to an optimized TensorRT engine for inference in 3 easy steps. The TensorRT inference engine can be conviniently used as a drop-in replacement for the orginial HuggingFace T5 model while providing significant speed up.</p> <p>If you are interested in further details of the conversion process, check out  <a href="https://github.com/NVIDIA/TensorRT/blob/59898c103f07e100d3c1108c038d767f5485e0b9/demo/HuggingFace/T5/trt.py" target="_blank" rel="noopener noreferrer">T5/trt.py<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.dec9f71a.js" defer></script><script src="/assets/js/2.733019b2.js" defer></script><script src="/assets/js/15.0cb65b1e.js" defer></script>
  </body>
</html>
