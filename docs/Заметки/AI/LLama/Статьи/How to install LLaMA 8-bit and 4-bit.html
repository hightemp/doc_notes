<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Obsidian notes</title>
    <meta name="generator" content="VuePress 1.9.9">
    
    <meta name="description" content="Obsidian notes">
    <meta name="theme-color" content="#3eaf7c">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    
    <link rel="preload" href="/doc_notes/assets/css/0.styles.60b82a90.css" as="style"><link rel="preload" href="/doc_notes/assets/js/app.d7b7570d.js" as="script"><link rel="preload" href="/doc_notes/assets/js/2.733019b2.js" as="script"><link rel="preload" href="/doc_notes/assets/js/25.c7ae079a.js" as="script"><link rel="prefetch" href="/doc_notes/assets/js/10.466a3961.js"><link rel="prefetch" href="/doc_notes/assets/js/100.eed12daa.js"><link rel="prefetch" href="/doc_notes/assets/js/101.c2c72797.js"><link rel="prefetch" href="/doc_notes/assets/js/102.3a5b3674.js"><link rel="prefetch" href="/doc_notes/assets/js/103.4df54047.js"><link rel="prefetch" href="/doc_notes/assets/js/104.4e3d031e.js"><link rel="prefetch" href="/doc_notes/assets/js/105.fb56ad64.js"><link rel="prefetch" href="/doc_notes/assets/js/106.4cd1492c.js"><link rel="prefetch" href="/doc_notes/assets/js/107.a916259a.js"><link rel="prefetch" href="/doc_notes/assets/js/108.dfbcdb35.js"><link rel="prefetch" href="/doc_notes/assets/js/109.b85d5957.js"><link rel="prefetch" href="/doc_notes/assets/js/11.70fc3590.js"><link rel="prefetch" href="/doc_notes/assets/js/110.dc40183f.js"><link rel="prefetch" href="/doc_notes/assets/js/111.a9f83101.js"><link rel="prefetch" href="/doc_notes/assets/js/112.1b402c16.js"><link rel="prefetch" href="/doc_notes/assets/js/113.89928947.js"><link rel="prefetch" href="/doc_notes/assets/js/114.8be0db96.js"><link rel="prefetch" href="/doc_notes/assets/js/115.bd9e24bd.js"><link rel="prefetch" href="/doc_notes/assets/js/116.1979db39.js"><link rel="prefetch" href="/doc_notes/assets/js/117.7a439523.js"><link rel="prefetch" href="/doc_notes/assets/js/118.680aed03.js"><link rel="prefetch" href="/doc_notes/assets/js/119.dfdba03a.js"><link rel="prefetch" href="/doc_notes/assets/js/12.0f945969.js"><link rel="prefetch" href="/doc_notes/assets/js/120.37c405a9.js"><link rel="prefetch" href="/doc_notes/assets/js/121.672a368b.js"><link rel="prefetch" href="/doc_notes/assets/js/122.f1b5fad1.js"><link rel="prefetch" href="/doc_notes/assets/js/123.b0d4c3db.js"><link rel="prefetch" href="/doc_notes/assets/js/124.7f08733e.js"><link rel="prefetch" href="/doc_notes/assets/js/125.619db48f.js"><link rel="prefetch" href="/doc_notes/assets/js/126.048ed0c7.js"><link rel="prefetch" href="/doc_notes/assets/js/127.cc65fb73.js"><link rel="prefetch" href="/doc_notes/assets/js/128.27591c7f.js"><link rel="prefetch" href="/doc_notes/assets/js/129.e848d5ea.js"><link rel="prefetch" href="/doc_notes/assets/js/13.353aee93.js"><link rel="prefetch" href="/doc_notes/assets/js/130.0b6325e2.js"><link rel="prefetch" href="/doc_notes/assets/js/131.a4a8dc74.js"><link rel="prefetch" href="/doc_notes/assets/js/132.f78488ea.js"><link rel="prefetch" href="/doc_notes/assets/js/133.2c57ab36.js"><link rel="prefetch" href="/doc_notes/assets/js/134.c041a644.js"><link rel="prefetch" href="/doc_notes/assets/js/135.dd89210d.js"><link rel="prefetch" href="/doc_notes/assets/js/136.127b0247.js"><link rel="prefetch" href="/doc_notes/assets/js/137.1ff783eb.js"><link rel="prefetch" href="/doc_notes/assets/js/138.e72eaf60.js"><link rel="prefetch" href="/doc_notes/assets/js/139.135c74d8.js"><link rel="prefetch" href="/doc_notes/assets/js/14.a3283f94.js"><link rel="prefetch" href="/doc_notes/assets/js/140.4a7f191d.js"><link rel="prefetch" href="/doc_notes/assets/js/141.fa563aa9.js"><link rel="prefetch" href="/doc_notes/assets/js/142.266c8a69.js"><link rel="prefetch" href="/doc_notes/assets/js/143.3c3e596e.js"><link rel="prefetch" href="/doc_notes/assets/js/144.46a5cdc6.js"><link rel="prefetch" href="/doc_notes/assets/js/145.c33d2f51.js"><link rel="prefetch" href="/doc_notes/assets/js/146.ecf9ffa3.js"><link rel="prefetch" href="/doc_notes/assets/js/147.ecc8f162.js"><link rel="prefetch" href="/doc_notes/assets/js/148.57a0cc2c.js"><link rel="prefetch" href="/doc_notes/assets/js/149.243334a6.js"><link rel="prefetch" href="/doc_notes/assets/js/15.c66e3407.js"><link rel="prefetch" href="/doc_notes/assets/js/150.6811595e.js"><link rel="prefetch" href="/doc_notes/assets/js/151.a1b33f48.js"><link rel="prefetch" href="/doc_notes/assets/js/152.754c44e2.js"><link rel="prefetch" href="/doc_notes/assets/js/153.b6b30dd0.js"><link rel="prefetch" href="/doc_notes/assets/js/154.78d410b8.js"><link rel="prefetch" href="/doc_notes/assets/js/155.767f3fb1.js"><link rel="prefetch" href="/doc_notes/assets/js/156.f9095760.js"><link rel="prefetch" href="/doc_notes/assets/js/157.c538a726.js"><link rel="prefetch" href="/doc_notes/assets/js/158.9835a2a3.js"><link rel="prefetch" href="/doc_notes/assets/js/159.9d7c5a05.js"><link rel="prefetch" href="/doc_notes/assets/js/16.e7ecc933.js"><link rel="prefetch" href="/doc_notes/assets/js/160.addf636e.js"><link rel="prefetch" href="/doc_notes/assets/js/161.b9499ae3.js"><link rel="prefetch" href="/doc_notes/assets/js/162.9ebe738a.js"><link rel="prefetch" href="/doc_notes/assets/js/163.4860d1e7.js"><link rel="prefetch" href="/doc_notes/assets/js/164.148cebf3.js"><link rel="prefetch" href="/doc_notes/assets/js/165.41046f67.js"><link rel="prefetch" href="/doc_notes/assets/js/166.6deaed29.js"><link rel="prefetch" href="/doc_notes/assets/js/167.6d02a205.js"><link rel="prefetch" href="/doc_notes/assets/js/168.c86d38a7.js"><link rel="prefetch" href="/doc_notes/assets/js/169.eb313225.js"><link rel="prefetch" href="/doc_notes/assets/js/17.65a71bb5.js"><link rel="prefetch" href="/doc_notes/assets/js/170.889855cd.js"><link rel="prefetch" href="/doc_notes/assets/js/171.b95cf306.js"><link rel="prefetch" href="/doc_notes/assets/js/172.0b1e5fe3.js"><link rel="prefetch" href="/doc_notes/assets/js/173.99679181.js"><link rel="prefetch" href="/doc_notes/assets/js/174.f28ad52f.js"><link rel="prefetch" href="/doc_notes/assets/js/175.1c1f371e.js"><link rel="prefetch" href="/doc_notes/assets/js/176.662a9d28.js"><link rel="prefetch" href="/doc_notes/assets/js/177.6e464bd5.js"><link rel="prefetch" href="/doc_notes/assets/js/178.102c88e4.js"><link rel="prefetch" href="/doc_notes/assets/js/179.dddab088.js"><link rel="prefetch" href="/doc_notes/assets/js/18.8fedcbac.js"><link rel="prefetch" href="/doc_notes/assets/js/180.345e741e.js"><link rel="prefetch" href="/doc_notes/assets/js/181.59db7110.js"><link rel="prefetch" href="/doc_notes/assets/js/182.7f628252.js"><link rel="prefetch" href="/doc_notes/assets/js/183.e8653839.js"><link rel="prefetch" href="/doc_notes/assets/js/184.3eec36ec.js"><link rel="prefetch" href="/doc_notes/assets/js/185.cb0c13d5.js"><link rel="prefetch" href="/doc_notes/assets/js/186.1ccbe1a2.js"><link rel="prefetch" href="/doc_notes/assets/js/187.55e6bc99.js"><link rel="prefetch" href="/doc_notes/assets/js/188.1829998b.js"><link rel="prefetch" href="/doc_notes/assets/js/189.e86e8099.js"><link rel="prefetch" href="/doc_notes/assets/js/19.8fd4f833.js"><link rel="prefetch" href="/doc_notes/assets/js/190.3263ca68.js"><link rel="prefetch" href="/doc_notes/assets/js/191.6973d93a.js"><link rel="prefetch" href="/doc_notes/assets/js/192.8c648d4c.js"><link rel="prefetch" href="/doc_notes/assets/js/193.135dc6c4.js"><link rel="prefetch" href="/doc_notes/assets/js/194.2387809d.js"><link rel="prefetch" href="/doc_notes/assets/js/195.4e094a48.js"><link rel="prefetch" href="/doc_notes/assets/js/196.faae151e.js"><link rel="prefetch" href="/doc_notes/assets/js/197.6ca84b2f.js"><link rel="prefetch" href="/doc_notes/assets/js/198.99d5a7c7.js"><link rel="prefetch" href="/doc_notes/assets/js/199.5ac6f260.js"><link rel="prefetch" href="/doc_notes/assets/js/20.b7871733.js"><link rel="prefetch" href="/doc_notes/assets/js/200.434715d2.js"><link rel="prefetch" href="/doc_notes/assets/js/201.df5e4553.js"><link rel="prefetch" href="/doc_notes/assets/js/202.4f5738c7.js"><link rel="prefetch" href="/doc_notes/assets/js/203.7bdee8f2.js"><link rel="prefetch" href="/doc_notes/assets/js/204.5b4e69e3.js"><link rel="prefetch" href="/doc_notes/assets/js/205.58586a6e.js"><link rel="prefetch" href="/doc_notes/assets/js/206.b5603313.js"><link rel="prefetch" href="/doc_notes/assets/js/207.e1a4577a.js"><link rel="prefetch" href="/doc_notes/assets/js/208.2369d699.js"><link rel="prefetch" href="/doc_notes/assets/js/209.baadeea1.js"><link rel="prefetch" href="/doc_notes/assets/js/21.1cff06fb.js"><link rel="prefetch" href="/doc_notes/assets/js/210.2b107173.js"><link rel="prefetch" href="/doc_notes/assets/js/211.c8eea835.js"><link rel="prefetch" href="/doc_notes/assets/js/212.1fae6417.js"><link rel="prefetch" href="/doc_notes/assets/js/213.5696d97f.js"><link rel="prefetch" href="/doc_notes/assets/js/214.bbd783da.js"><link rel="prefetch" href="/doc_notes/assets/js/215.1c0c350c.js"><link rel="prefetch" href="/doc_notes/assets/js/216.498044dc.js"><link rel="prefetch" href="/doc_notes/assets/js/217.319f81c6.js"><link rel="prefetch" href="/doc_notes/assets/js/218.8911d413.js"><link rel="prefetch" href="/doc_notes/assets/js/219.f37e96ea.js"><link rel="prefetch" href="/doc_notes/assets/js/22.2cd72a62.js"><link rel="prefetch" href="/doc_notes/assets/js/220.b5ac8e75.js"><link rel="prefetch" href="/doc_notes/assets/js/221.cde91cfa.js"><link rel="prefetch" href="/doc_notes/assets/js/222.df059246.js"><link rel="prefetch" href="/doc_notes/assets/js/223.3ce433b3.js"><link rel="prefetch" href="/doc_notes/assets/js/224.36f2c1ff.js"><link rel="prefetch" href="/doc_notes/assets/js/225.fbee00f9.js"><link rel="prefetch" href="/doc_notes/assets/js/226.306f0e70.js"><link rel="prefetch" href="/doc_notes/assets/js/227.4e7373a0.js"><link rel="prefetch" href="/doc_notes/assets/js/228.44fd0171.js"><link rel="prefetch" href="/doc_notes/assets/js/229.f832a1d6.js"><link rel="prefetch" href="/doc_notes/assets/js/23.e9957abc.js"><link rel="prefetch" href="/doc_notes/assets/js/230.043a91e9.js"><link rel="prefetch" href="/doc_notes/assets/js/231.7616cdd1.js"><link rel="prefetch" href="/doc_notes/assets/js/232.c3eb262d.js"><link rel="prefetch" href="/doc_notes/assets/js/233.645ff598.js"><link rel="prefetch" href="/doc_notes/assets/js/234.bb763bdf.js"><link rel="prefetch" href="/doc_notes/assets/js/235.f6724c17.js"><link rel="prefetch" href="/doc_notes/assets/js/236.2fe5969c.js"><link rel="prefetch" href="/doc_notes/assets/js/237.c379ae13.js"><link rel="prefetch" href="/doc_notes/assets/js/238.bc474d07.js"><link rel="prefetch" href="/doc_notes/assets/js/239.b4b3094b.js"><link rel="prefetch" href="/doc_notes/assets/js/24.e4e0f07a.js"><link rel="prefetch" href="/doc_notes/assets/js/240.b7298fad.js"><link rel="prefetch" href="/doc_notes/assets/js/241.edc191c3.js"><link rel="prefetch" href="/doc_notes/assets/js/242.d79274ed.js"><link rel="prefetch" href="/doc_notes/assets/js/243.db97328a.js"><link rel="prefetch" href="/doc_notes/assets/js/244.68cd4704.js"><link rel="prefetch" href="/doc_notes/assets/js/245.f501295b.js"><link rel="prefetch" href="/doc_notes/assets/js/246.ddc4d5d9.js"><link rel="prefetch" href="/doc_notes/assets/js/247.ff22cc5b.js"><link rel="prefetch" href="/doc_notes/assets/js/248.4a61cd51.js"><link rel="prefetch" href="/doc_notes/assets/js/249.be49f5ca.js"><link rel="prefetch" href="/doc_notes/assets/js/250.43caa09e.js"><link rel="prefetch" href="/doc_notes/assets/js/251.26d5a6e9.js"><link rel="prefetch" href="/doc_notes/assets/js/252.b3bcf0c8.js"><link rel="prefetch" href="/doc_notes/assets/js/253.1a1b2b11.js"><link rel="prefetch" href="/doc_notes/assets/js/254.1c4090ef.js"><link rel="prefetch" href="/doc_notes/assets/js/255.a49e9f41.js"><link rel="prefetch" href="/doc_notes/assets/js/256.78de4e85.js"><link rel="prefetch" href="/doc_notes/assets/js/257.3f3d8408.js"><link rel="prefetch" href="/doc_notes/assets/js/258.2a59b276.js"><link rel="prefetch" href="/doc_notes/assets/js/259.8916e4fc.js"><link rel="prefetch" href="/doc_notes/assets/js/26.1ae4ceb3.js"><link rel="prefetch" href="/doc_notes/assets/js/260.9e38f578.js"><link rel="prefetch" href="/doc_notes/assets/js/261.bb658a8a.js"><link rel="prefetch" href="/doc_notes/assets/js/262.e3dc3020.js"><link rel="prefetch" href="/doc_notes/assets/js/263.f1020a9b.js"><link rel="prefetch" href="/doc_notes/assets/js/264.b8f1ff3a.js"><link rel="prefetch" href="/doc_notes/assets/js/265.d3eb13d8.js"><link rel="prefetch" href="/doc_notes/assets/js/266.f27ca21b.js"><link rel="prefetch" href="/doc_notes/assets/js/267.338c834f.js"><link rel="prefetch" href="/doc_notes/assets/js/268.b9ebda2b.js"><link rel="prefetch" href="/doc_notes/assets/js/269.dc7525cf.js"><link rel="prefetch" href="/doc_notes/assets/js/27.42caa565.js"><link rel="prefetch" href="/doc_notes/assets/js/270.137ed51c.js"><link rel="prefetch" href="/doc_notes/assets/js/271.b6fdb998.js"><link rel="prefetch" href="/doc_notes/assets/js/272.ddc1ee03.js"><link rel="prefetch" href="/doc_notes/assets/js/273.df6133aa.js"><link rel="prefetch" href="/doc_notes/assets/js/274.9115305d.js"><link rel="prefetch" href="/doc_notes/assets/js/275.13342d76.js"><link rel="prefetch" href="/doc_notes/assets/js/276.a9e1f1ec.js"><link rel="prefetch" href="/doc_notes/assets/js/277.f2728f30.js"><link rel="prefetch" href="/doc_notes/assets/js/278.7e726870.js"><link rel="prefetch" href="/doc_notes/assets/js/279.648ad6bd.js"><link rel="prefetch" href="/doc_notes/assets/js/28.45f1bbb9.js"><link rel="prefetch" href="/doc_notes/assets/js/280.720a2918.js"><link rel="prefetch" href="/doc_notes/assets/js/281.5a2d475b.js"><link rel="prefetch" href="/doc_notes/assets/js/282.62bc635b.js"><link rel="prefetch" href="/doc_notes/assets/js/283.ff6be5f5.js"><link rel="prefetch" href="/doc_notes/assets/js/284.3f7c0af1.js"><link rel="prefetch" href="/doc_notes/assets/js/285.c2f51624.js"><link rel="prefetch" href="/doc_notes/assets/js/286.a5b99138.js"><link rel="prefetch" href="/doc_notes/assets/js/287.829ac9bd.js"><link rel="prefetch" href="/doc_notes/assets/js/288.54aabaae.js"><link rel="prefetch" href="/doc_notes/assets/js/289.71b4d090.js"><link rel="prefetch" href="/doc_notes/assets/js/29.0fdf0aa1.js"><link rel="prefetch" href="/doc_notes/assets/js/290.df175e90.js"><link rel="prefetch" href="/doc_notes/assets/js/291.014245fe.js"><link rel="prefetch" href="/doc_notes/assets/js/292.8d375ce2.js"><link rel="prefetch" href="/doc_notes/assets/js/293.8591ceba.js"><link rel="prefetch" href="/doc_notes/assets/js/294.d979d20b.js"><link rel="prefetch" href="/doc_notes/assets/js/295.372c2d39.js"><link rel="prefetch" href="/doc_notes/assets/js/296.07d572ab.js"><link rel="prefetch" href="/doc_notes/assets/js/297.5de1073e.js"><link rel="prefetch" href="/doc_notes/assets/js/298.5a6ac1ca.js"><link rel="prefetch" href="/doc_notes/assets/js/299.e87a7ab3.js"><link rel="prefetch" href="/doc_notes/assets/js/3.8087b6c9.js"><link rel="prefetch" href="/doc_notes/assets/js/30.33c143cb.js"><link rel="prefetch" href="/doc_notes/assets/js/300.c011f877.js"><link rel="prefetch" href="/doc_notes/assets/js/301.479875ba.js"><link rel="prefetch" href="/doc_notes/assets/js/302.fe22cf89.js"><link rel="prefetch" href="/doc_notes/assets/js/303.fe59bc5d.js"><link rel="prefetch" href="/doc_notes/assets/js/304.17566e0f.js"><link rel="prefetch" href="/doc_notes/assets/js/305.235d62da.js"><link rel="prefetch" href="/doc_notes/assets/js/306.6a899aaa.js"><link rel="prefetch" href="/doc_notes/assets/js/307.a6302576.js"><link rel="prefetch" href="/doc_notes/assets/js/308.53d18516.js"><link rel="prefetch" href="/doc_notes/assets/js/309.44746635.js"><link rel="prefetch" href="/doc_notes/assets/js/31.f3c83a33.js"><link rel="prefetch" href="/doc_notes/assets/js/310.2b0b41b5.js"><link rel="prefetch" href="/doc_notes/assets/js/311.a2425d85.js"><link rel="prefetch" href="/doc_notes/assets/js/312.ee8bb224.js"><link rel="prefetch" href="/doc_notes/assets/js/313.cdb7a074.js"><link rel="prefetch" href="/doc_notes/assets/js/314.5c46337b.js"><link rel="prefetch" href="/doc_notes/assets/js/315.d2f5d6ef.js"><link rel="prefetch" href="/doc_notes/assets/js/316.1d1b10d7.js"><link rel="prefetch" href="/doc_notes/assets/js/317.18978adf.js"><link rel="prefetch" href="/doc_notes/assets/js/318.725aaa9e.js"><link rel="prefetch" href="/doc_notes/assets/js/319.10c27323.js"><link rel="prefetch" href="/doc_notes/assets/js/32.2e4a5d25.js"><link rel="prefetch" href="/doc_notes/assets/js/320.be122112.js"><link rel="prefetch" href="/doc_notes/assets/js/321.21d59327.js"><link rel="prefetch" href="/doc_notes/assets/js/322.1c7aa8b5.js"><link rel="prefetch" href="/doc_notes/assets/js/323.61f2e664.js"><link rel="prefetch" href="/doc_notes/assets/js/324.d5ef212c.js"><link rel="prefetch" href="/doc_notes/assets/js/325.8dcaad1a.js"><link rel="prefetch" href="/doc_notes/assets/js/326.27ae755d.js"><link rel="prefetch" href="/doc_notes/assets/js/327.23295181.js"><link rel="prefetch" href="/doc_notes/assets/js/328.b1038029.js"><link rel="prefetch" href="/doc_notes/assets/js/329.685f438a.js"><link rel="prefetch" href="/doc_notes/assets/js/33.78d67de4.js"><link rel="prefetch" href="/doc_notes/assets/js/330.29d01e38.js"><link rel="prefetch" href="/doc_notes/assets/js/331.56ede5a6.js"><link rel="prefetch" href="/doc_notes/assets/js/332.26f5337e.js"><link rel="prefetch" href="/doc_notes/assets/js/333.7a1b5bed.js"><link rel="prefetch" href="/doc_notes/assets/js/334.ccc83ffa.js"><link rel="prefetch" href="/doc_notes/assets/js/335.dd4d24f2.js"><link rel="prefetch" href="/doc_notes/assets/js/336.5cd8a3be.js"><link rel="prefetch" href="/doc_notes/assets/js/337.725cd669.js"><link rel="prefetch" href="/doc_notes/assets/js/338.ca370789.js"><link rel="prefetch" href="/doc_notes/assets/js/339.4db35fe1.js"><link rel="prefetch" href="/doc_notes/assets/js/34.10602eab.js"><link rel="prefetch" href="/doc_notes/assets/js/340.11e5684e.js"><link rel="prefetch" href="/doc_notes/assets/js/341.87af888b.js"><link rel="prefetch" href="/doc_notes/assets/js/342.6288246d.js"><link rel="prefetch" href="/doc_notes/assets/js/343.10836901.js"><link rel="prefetch" href="/doc_notes/assets/js/344.85db1640.js"><link rel="prefetch" href="/doc_notes/assets/js/345.f1a97cf1.js"><link rel="prefetch" href="/doc_notes/assets/js/346.dd502824.js"><link rel="prefetch" href="/doc_notes/assets/js/347.b211385a.js"><link rel="prefetch" href="/doc_notes/assets/js/348.f255c270.js"><link rel="prefetch" href="/doc_notes/assets/js/349.36014517.js"><link rel="prefetch" href="/doc_notes/assets/js/35.03fbbb65.js"><link rel="prefetch" href="/doc_notes/assets/js/350.817bd0e1.js"><link rel="prefetch" href="/doc_notes/assets/js/351.abf45be4.js"><link rel="prefetch" href="/doc_notes/assets/js/352.4254451f.js"><link rel="prefetch" href="/doc_notes/assets/js/353.c0b15b9d.js"><link rel="prefetch" href="/doc_notes/assets/js/354.d725c37e.js"><link rel="prefetch" href="/doc_notes/assets/js/355.10f1ce9b.js"><link rel="prefetch" href="/doc_notes/assets/js/356.a5e2bfda.js"><link rel="prefetch" href="/doc_notes/assets/js/357.80f39a19.js"><link rel="prefetch" href="/doc_notes/assets/js/358.5c69b1d1.js"><link rel="prefetch" href="/doc_notes/assets/js/359.12d21c81.js"><link rel="prefetch" href="/doc_notes/assets/js/36.aa74ba81.js"><link rel="prefetch" href="/doc_notes/assets/js/360.1183658a.js"><link rel="prefetch" href="/doc_notes/assets/js/361.33b745b6.js"><link rel="prefetch" href="/doc_notes/assets/js/362.a56596c0.js"><link rel="prefetch" href="/doc_notes/assets/js/363.1fadef9e.js"><link rel="prefetch" href="/doc_notes/assets/js/364.e43e06a1.js"><link rel="prefetch" href="/doc_notes/assets/js/365.1b191379.js"><link rel="prefetch" href="/doc_notes/assets/js/366.77112bc2.js"><link rel="prefetch" href="/doc_notes/assets/js/367.cbbf1410.js"><link rel="prefetch" href="/doc_notes/assets/js/368.6e4af412.js"><link rel="prefetch" href="/doc_notes/assets/js/369.9d8214a4.js"><link rel="prefetch" href="/doc_notes/assets/js/37.bad965a6.js"><link rel="prefetch" href="/doc_notes/assets/js/370.5dcbc931.js"><link rel="prefetch" href="/doc_notes/assets/js/371.3e798a72.js"><link rel="prefetch" href="/doc_notes/assets/js/372.99fbe027.js"><link rel="prefetch" href="/doc_notes/assets/js/373.783fc78c.js"><link rel="prefetch" href="/doc_notes/assets/js/374.8fc2e9fe.js"><link rel="prefetch" href="/doc_notes/assets/js/375.2831b29c.js"><link rel="prefetch" href="/doc_notes/assets/js/376.8a690d9f.js"><link rel="prefetch" href="/doc_notes/assets/js/377.c6401c22.js"><link rel="prefetch" href="/doc_notes/assets/js/38.57a79422.js"><link rel="prefetch" href="/doc_notes/assets/js/39.7ab71035.js"><link rel="prefetch" href="/doc_notes/assets/js/4.da3ef268.js"><link rel="prefetch" href="/doc_notes/assets/js/40.ffa22594.js"><link rel="prefetch" href="/doc_notes/assets/js/41.b2fc54d1.js"><link rel="prefetch" href="/doc_notes/assets/js/42.c43bc930.js"><link rel="prefetch" href="/doc_notes/assets/js/43.5c93d7a1.js"><link rel="prefetch" href="/doc_notes/assets/js/44.605a54a6.js"><link rel="prefetch" href="/doc_notes/assets/js/45.f85a75a3.js"><link rel="prefetch" href="/doc_notes/assets/js/46.2cd25346.js"><link rel="prefetch" href="/doc_notes/assets/js/47.93f7d1a6.js"><link rel="prefetch" href="/doc_notes/assets/js/48.7fd0b6dc.js"><link rel="prefetch" href="/doc_notes/assets/js/49.c64bb839.js"><link rel="prefetch" href="/doc_notes/assets/js/5.dac787d8.js"><link rel="prefetch" href="/doc_notes/assets/js/50.d2fe7364.js"><link rel="prefetch" href="/doc_notes/assets/js/51.b9189617.js"><link rel="prefetch" href="/doc_notes/assets/js/52.4689b562.js"><link rel="prefetch" href="/doc_notes/assets/js/53.f0b1d96b.js"><link rel="prefetch" href="/doc_notes/assets/js/54.f52a48ab.js"><link rel="prefetch" href="/doc_notes/assets/js/55.98bfca4a.js"><link rel="prefetch" href="/doc_notes/assets/js/56.6c0a81f6.js"><link rel="prefetch" href="/doc_notes/assets/js/57.e17dfa82.js"><link rel="prefetch" href="/doc_notes/assets/js/58.f9a5000b.js"><link rel="prefetch" href="/doc_notes/assets/js/59.3296ea1c.js"><link rel="prefetch" href="/doc_notes/assets/js/6.2d0a63f8.js"><link rel="prefetch" href="/doc_notes/assets/js/60.610d4eca.js"><link rel="prefetch" href="/doc_notes/assets/js/61.64b18740.js"><link rel="prefetch" href="/doc_notes/assets/js/62.87e34293.js"><link rel="prefetch" href="/doc_notes/assets/js/63.fbd2d78b.js"><link rel="prefetch" href="/doc_notes/assets/js/64.493b857a.js"><link rel="prefetch" href="/doc_notes/assets/js/65.0a60479c.js"><link rel="prefetch" href="/doc_notes/assets/js/66.800419f9.js"><link rel="prefetch" href="/doc_notes/assets/js/67.343f5dab.js"><link rel="prefetch" href="/doc_notes/assets/js/68.fed77f78.js"><link rel="prefetch" href="/doc_notes/assets/js/69.8b1601da.js"><link rel="prefetch" href="/doc_notes/assets/js/7.c9d31aa2.js"><link rel="prefetch" href="/doc_notes/assets/js/70.aedd0ab9.js"><link rel="prefetch" href="/doc_notes/assets/js/71.b166238f.js"><link rel="prefetch" href="/doc_notes/assets/js/72.27a66c82.js"><link rel="prefetch" href="/doc_notes/assets/js/73.1d9b487f.js"><link rel="prefetch" href="/doc_notes/assets/js/74.84702901.js"><link rel="prefetch" href="/doc_notes/assets/js/75.2f6ae560.js"><link rel="prefetch" href="/doc_notes/assets/js/76.4856c210.js"><link rel="prefetch" href="/doc_notes/assets/js/77.9fc6950e.js"><link rel="prefetch" href="/doc_notes/assets/js/78.cb2f07a0.js"><link rel="prefetch" href="/doc_notes/assets/js/79.5e43d0b7.js"><link rel="prefetch" href="/doc_notes/assets/js/8.da9db8f9.js"><link rel="prefetch" href="/doc_notes/assets/js/80.d59ea8af.js"><link rel="prefetch" href="/doc_notes/assets/js/81.9fc53f43.js"><link rel="prefetch" href="/doc_notes/assets/js/82.5838801e.js"><link rel="prefetch" href="/doc_notes/assets/js/83.d2006fd2.js"><link rel="prefetch" href="/doc_notes/assets/js/84.9b965f14.js"><link rel="prefetch" href="/doc_notes/assets/js/85.de0c498b.js"><link rel="prefetch" href="/doc_notes/assets/js/86.ce67f8b8.js"><link rel="prefetch" href="/doc_notes/assets/js/87.5d47b003.js"><link rel="prefetch" href="/doc_notes/assets/js/88.eee9be64.js"><link rel="prefetch" href="/doc_notes/assets/js/89.42153946.js"><link rel="prefetch" href="/doc_notes/assets/js/9.0ccc34a6.js"><link rel="prefetch" href="/doc_notes/assets/js/90.13a9f4b2.js"><link rel="prefetch" href="/doc_notes/assets/js/91.832ae5d2.js"><link rel="prefetch" href="/doc_notes/assets/js/92.bd7c25ca.js"><link rel="prefetch" href="/doc_notes/assets/js/93.8a6f3f34.js"><link rel="prefetch" href="/doc_notes/assets/js/94.221348bd.js"><link rel="prefetch" href="/doc_notes/assets/js/95.86b48f96.js"><link rel="prefetch" href="/doc_notes/assets/js/96.7c6d0372.js"><link rel="prefetch" href="/doc_notes/assets/js/97.307341f8.js"><link rel="prefetch" href="/doc_notes/assets/js/98.14120c7c.js"><link rel="prefetch" href="/doc_notes/assets/js/99.40560c36.js">
    <link rel="stylesheet" href="/doc_notes/assets/css/0.styles.60b82a90.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/doc_notes/" class="home-link router-link-active"><!----> <span class="site-name">Obsidian notes</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/doc_notes/guide/" class="nav-link">
  Guide
</a></div><div class="nav-item"><a href="/doc_notes/config/" class="nav-link">
  Config
</a></div><div class="nav-item"><a href="https://v1.vuepress.vuejs.org" target="_blank" rel="noopener noreferrer" class="nav-link external">
  VuePress
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/doc_notes/guide/" class="nav-link">
  Guide
</a></div><div class="nav-item"><a href="/doc_notes/config/" class="nav-link">
  Config
</a></div><div class="nav-item"><a href="https://v1.vuepress.vuejs.org" target="_blank" rel="noopener noreferrer" class="nav-link external">
  VuePress
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav>  <!----> </aside> <main class="page"> <div class="theme-default-content content__default"><p>https://www.reddit.com/r/LocalLLaMA/comments/11o6o3f/how_to_install_llama_8bit_and_4bit/</p> <p>FAQ</p> <p><strong>Q:</strong> What is LLaMA?</p> <p><strong>A:</strong> LLaMA (Large Language Model Meta AI) is a foundational large language model designed primarily for researchers. Like other large language models, LLaMA works by taking a sequence of words as an input and predicts a next word to recursively generate text.</p> <p><strong>Q:</strong> Is LLaMA like ChatGPT?</p> <p><strong>A:</strong> No. LLaMA models are not finetuned for question answering. They should be prompted so that the expected answer is the natural continuation of the prompt. Nonetheless, it is possible to chat with LLaMA models in a way similar to ChatGPT but not near the same quality.</p> <p><strong>Q:</strong> What languages does LLaMA support?</p> <p><strong>A:</strong> Primarily English, but it should have limited capabilities for the following languages: bg, ca, cs, da, de, es, fr, hr, hu, it, nl, pl, pt, ro, ru, sl, sr, sv, uk.</p> <p><strong>Q:</strong> I've heard about Alpaca. What is that?</p> <p><strong>A:</strong> That refers to the <a href="https://crfm.stanford.edu/2023/03/13/alpaca.html" target="_blank" rel="noopener noreferrer">Stanford Alpaca<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> project, an effort to build an instruction-following LLaMA model from the standard 7B LLaMA model. It has been shown to produce results similar to OpenAI's text-davinci-003. This guide contains instructions on trying out Alpaca using a few different methods.</p> <p>8-bit Model Requirements</p> <p>Model</p> <p>VRAM Used</p> <p>Minimum Total VRAM</p> <p>Card examples</p> <p>RAM/Swap to Load*</p> <p>LLaMA-7B</p> <p>9.2GB</p> <p>10GB</p> <p>3060 12GB, 3080 10GB</p> <p>24 GB</p> <p>LLaMA-13B</p> <p>16.3GB</p> <p>20GB</p> <p>3090, 3090 Ti, 4090</p> <p>32GB</p> <p>LLaMA-30B</p> <p>36GB</p> <p>40GB</p> <p>A6000 48GB, A100 40GB</p> <p>72GB</p> <p>LLaMA-65B</p> <p>74GB</p> <p>80GB</p> <p>A100 80GB</p> <p>144GB</p> <p>*<em>System RAM, not VRAM, required to load the model, in addition to having enough VRAM. NOT required to RUN the model. You can use swap space if you do not have enough RAM.</em></p> <p>4-bit Model Requirements</p> <p>Model</p> <p>Minimum Total VRAM</p> <p>Card examples</p> <p>RAM/Swap to Load</p> <p>LLaMA-7B</p> <p>6GB</p> <p>GTX 1660, 2060, AMD 5700 XT, RTX 3050, 3060</p> <p>16 GB</p> <p>LLaMA-13B</p> <p>10GB</p> <p>AMD 6900 XT, RTX 2060 12GB, 3060 12GB, 3080, A2000</p> <p>32 GB</p> <p>LLaMA-30B</p> <p>20GB</p> <p>RTX 3080 20GB, A4500, A5000, 3090, 4090, 6000, Tesla V100</p> <p>64 GB</p> <p>LLaMA-65B</p> <p>40GB</p> <p>A100 40GB, 2x3090, 2x4090, A40, RTX A6000, 8000</p> <p>128 GB</p> <p>Installing Windows Subsystem for Linux (WSL)</p> <blockquote><blockquote><p><strong>WSL installation is optional.&lt;&lt; If you do not want to install this, you can skip over to the Windows specific instructions below for 8-bit or 4-bit. This section requires an NVIDIA GPU.</strong></p></blockquote></blockquote> <p>On Windows, you may receive better performance when using WSL. To install WSL using the instructions below, first ensure you are running at least Windows 10 version 2004 and higher (Build 19041 and higher) or Windows 11. To check for this, type info in the search box on your taskbar and then select System Information. Alternatively, hit Windows+R, type msinfo32 into the &quot;Open&quot; field, and then hit enter. Look at &quot;Version&quot; to see what version you are running.</p> <p><strong>Instructions:</strong></p> <ol><li><p>Open Powershell in administrator mode</p></li> <li><p>Enter the following command then restart your machine: wsl --install</p></li></ol> <p>This command will enable WSL, download and install the lastest Linux Kernel, use WSL2 as default, and download and install the Ubuntu Linux distribution.</p> <ol start="3"><li><p>After restart, Windows will finish installing Ubuntu. You'll be asked to create a username and password for Ubuntu. It has no bearing on your Windows username.</p></li> <li><p>Windows will not automatically update or upgrade Ubuntu. Update and upgrade your packages by running the following command in the Ubuntu terminal (search for Ubuntu in the Start menu or taskbar and open the app): sudo apt update &amp;&amp; sudo apt upgrade</p></li> <li><p>You can now continue by following the Linux setup instructions for LLaMA. <strong>Check the necessary troubleshooting info below to resolve errors</strong>. If you plan on using 4-bit LLaMA with WSL, you will need to install the WSL-Ubuntu CUDA toolkit using the instructions below.</p></li></ol> <p><strong>Extra tips:</strong></p> <p>To install conda, run the following inside the Ubuntu environment:</p> <div class="language- extra-class"><pre class="language-text"><code>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash Miniconda3-latest-Linux-x86_64.sh
</code></pre></div><p>To find the name of a WSL distribution and uninstall it (afterward, you can create a new virtual machine environment by opening the app again):</p> <div class="language- extra-class"><pre class="language-text"><code>wsl -l
wsl --unregister &lt;DistributionName&gt;
</code></pre></div><p>To access the web UI from another device on your local network, you will need to configure port forwarding:</p> <div class="language- extra-class"><pre class="language-text"><code>netsh interface portproxy add v4tov4 listenaddress=0.0.0.0 listenport=7860 connectaddress=localhost connectport=7860
</code></pre></div><p><strong>Troubleshooting:</strong></p> <p>If you will use 4-bit LLaMA with WSL, you must install the WSL-Ubuntu CUDA toolkit, and it must be 11.7. This CUDA toolkit will not overwrite your WSL2 driver unlike the default CUDA toolkit. Follow these steps:</p> <div class="language- extra-class"><pre class="language-text"><code>sudo apt-key del 7fa2af80 
wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-wsl-ubuntu.pin
sudo mv cuda-wsl-ubuntu.pin /etc/apt/preferences.d/cuda-repository-pin-600
wget https://developer.download.nvidia.com/compute/cuda/11.7.0/local_installers/cuda-repo-wsl-ubuntu-11-7-local_11.7.0-1_amd64.deb
sudo dpkg -i cuda-repo-wsl-ubuntu-11-7-local_11.7.0-1_amd64.deb
sudo cp /var/cuda-repo-wsl-ubuntu-11-7-local/cuda-*-keyring.gpg /usr/share/keyrings/
sudo apt-get update
sudo apt-get -y install cuda
</code></pre></div><p>In order to avoid a CUDA error when starting the web UI, you will need to apply the following fix as seen in <a href="https://github.com/TimDettmers/bitsandbytes/issues/156#issuecomment-1462329713" target="_blank" rel="noopener noreferrer">this comment<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> and issue <a href="https://github.com/oobabooga/text-generation-webui/issues/400#issuecomment-1474876859" target="_blank" rel="noopener noreferrer">#400<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>:</p> <div class="language- extra-class"><pre class="language-text"><code>cd /home/USERNAME/miniconda3/envs/textgen/lib/python3.10/site-packages/bitsandbytes/
cp libbitsandbytes_cuda117.so libbitsandbytes_cpu.so
conda install cudatoolkit
</code></pre></div><p>If for some reason installing the WSL-Ubuntu CUDA toolkit does not work for you, <a href="https://github.com/oobabooga/text-generation-webui/issues/416#issuecomment-1475078571" target="_blank" rel="noopener noreferrer">this alternate fix<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> should resolve any errors relating to that.</p> <p>You may also need to create symbolic links to get everything working correctly. Do not do this if the above commands resolve your errors. To create the symlinks, follow the instructions <a href="https://github.com/microsoft/WSL/issues/5548#issuecomment-1292858815" target="_blank" rel="noopener noreferrer">here<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> then restart your machine.</p> <p>Installing 8-bit LLaMA with text-generation-webui</p> <p>Linux:</p> <ol><li><p>Follow the <a href="https://github.com/oobabooga/text-generation-webui" target="_blank" rel="noopener noreferrer">instructions here<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> under &quot;Installation&quot;</p></li> <li><p>Download the desired Hugging Face converted model for LLaMA <a href="https://huggingface.co/decapoda-research" target="_blank" rel="noopener noreferrer">here<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></li> <li><p>Copy the entire model folder, for example llama-13b-hf, into text-generation-webui\models</p></li> <li><p>Run the following command in your conda environment: python server.py --model llama-13b-hf --load-in-8bit</p></li></ol> <p>Windows:</p> <ol><li><p>Install <a href="https://docs.conda.io/en/latest/miniconda.html" target="_blank" rel="noopener noreferrer">miniconda<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></li> <li><p>Activate conda via powershell, replacing USERNAME with your username: powershell -ExecutionPolicy ByPass -NoExit -Command &quot;&amp; 'C:\Users\USERNAME\miniconda3\shell\condabin\conda-hook.ps1' ; conda activate 'C:\Users\USERNAME\miniconda3' &quot;</p></li> <li><p>Follow the <a href="https://github.com/oobabooga/text-generation-webui" target="_blank" rel="noopener noreferrer">instructions here<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> under &quot;Installation&quot;, starting with the step &quot;Create a new conda environment.&quot;</p></li> <li><p>Download the desired Hugging Face converted model for LLaMA <a href="https://huggingface.co/decapoda-research" target="_blank" rel="noopener noreferrer">here<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></li> <li><p>Copy the entire model folder, for example llama-13b-hf, into text-generation-webui\models</p></li> <li><p>Download <a href="https://github.com/DeXtmL/bitsandbytes-win-prebuilt" target="_blank" rel="noopener noreferrer">libbitsandbytes_cuda116.dll<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> and put it in C:\Users\xxx\miniconda3\envs\textgen\lib\site-packages\bitsandbytes\</p></li> <li><p><strong>In</strong> \bitsandbytes\cuda_setup\main.py <strong>search for:</strong> <code>if not torch.cuda.is_available(): return 'libsbitsandbytes_cpu.so', None, None, None, None</code> <strong>and replace with:</strong> <code>if torch.cuda.is_available(): return 'libbitsandbytes_cuda116.dll', None, None, None, None</code></p></li> <li><p><strong>In</strong> \bitsandbytes\cuda_setup\main.py <strong>search for this</strong> <strong>twice:</strong> <code>self.lib = ct.cdll.LoadLibrary(binary_path)</code> <strong>and replace with:</strong> <code>self.lib = ct.cdll.LoadLibrary(str(binary_path))</code></p></li> <li><p>Run the following command in your conda environment: python server.py --model llama-13b-hf --load-in-8bit</p></li></ol> <p><strong>Note:</strong> for decapoda-research models, you must <strong>change</strong> &quot;tokenizer_class&quot;: &quot;LLaMATokenizer&quot; <strong>to</strong> &quot;tokenizer_class&quot;: &quot;LlamaTokenizer&quot; <strong>in</strong> text-generation-webui/models/llama-13b-hf/tokenizer_config.json</p> <p>Installing 4-bit LLaMA with text-generation-webui</p> <p>Linux:</p> <ol><li><p>Follow the <a href="https://github.com/oobabooga/text-generation-webui" target="_blank" rel="noopener noreferrer">instructions here<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> under &quot;Installation&quot;</p></li> <li><p>Continue with the 4-bit specific <a href="https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model#4-bit-mode" target="_blank" rel="noopener noreferrer">instructions here<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></li></ol> <p>Windows (Step-by-Step):</p> <ol><li><p>Install Build Tools for Visual Studio 2019 (has to be 2019) <a href="https://learn.microsoft.com/en-us/visualstudio/releases/2019/history#release-dates-and-build-numbers" target="_blank" rel="noopener noreferrer">here<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>. Check &quot;Desktop development with C++&quot; when installing.</p></li> <li><p>Install <a href="https://docs.conda.io/en/latest/miniconda.html" target="_blank" rel="noopener noreferrer">miniconda<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></li> <li><p>Install Git from the <a href="https://git-scm.com/download/win" target="_blank" rel="noopener noreferrer">website<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> or simply with cmd prompt: winget install --id Git.Git -e --source winget</p></li> <li><p>Open &quot;x64 native tools command prompt&quot; as admin</p></li> <li><p>Activate conda, replacing USERNAME with your username: powershell -ExecutionPolicy ByPass -NoExit -Command &quot;&amp; 'C:\Users\USERNAME\miniconda3\shell\condabin\conda-hook.ps1' ; conda activate 'C:\Users\USERNAME\miniconda3' &quot;</p></li> <li><p>conda create -n textgen python=3.10.9</p></li> <li><p>conda activate textgen</p></li> <li><p>conda install cuda -c nvidia/label/cuda-11.3.0 -c nvidia/label/cuda-11.3.1</p></li> <li><p>git clone <a href="https://github.com/oobabooga/text-generation-webui" target="_blank" rel="noopener noreferrer">https://github.com/oobabooga/text-generation-webui<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></li> <li><p>cd text-generation-webui</p></li> <li><p>pip install -r requirements.txt</p></li> <li><p>pip install torch==1.12+cu113 -f <a href="https://download.pytorch.org/whl/torch_stable.html" target="_blank" rel="noopener noreferrer">https://download.pytorch.org/whl/torch_stable.html<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></li> <li><p>mkdir repositories</p></li> <li><p>cd repositories</p></li> <li><p>git clone <a href="https://github.com/qwopqwop200/GPTQ-for-LLaMa" target="_blank" rel="noopener noreferrer">https://github.com/qwopqwop200/GPTQ-for-LLaMa<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></li> <li><p>cd GPTQ-for-LLaMa</p></li> <li><p>git reset --hard c589c5456cc1c9e96065a5d285f8e3fac2cdb0fd</p></li> <li><p>pip install ninja</p></li> <li><p>$env:DISTUTILS_USE_SDK=1</p></li> <li><p>python setup_cuda.py install</p></li> <li><p>Download the 4-bit model of your choice and place it directly into your models folder. For instance, models/llama-13b-4bit-128g. The links for the updated 4-bit models are listed below in the models directory section. If you will use 7B 4-bit, download <strong>without group-size</strong>. For 13B 4-bit and up, download <strong>with group-size</strong>.</p></li> <li><p>Run the following command in your conda environment: <em><strong>without group-size</strong></em> python server.py --model llama-7b-4bit --wbits 4 --no-stream <em><strong>with group-size</strong></em> python server.py --model llama-13b-4bit-128g --wbits 4 --groupsize 128 --no-stream</p></li></ol> <p><strong>Note:</strong> If you get the error &quot;CUDA Setup failed despite GPU being available&quot;, do the patch in steps 6-8 of the 8-bit instructions above.</p> <p>Using Alpaca-LoRA with text-generation-webui</p> <p><strong>Make sure to follow the installation instructions for 8-bit LLaMA before using this.</strong></p> <p>This is to reproduce the <a href="https://crfm.stanford.edu/2023/03/13/alpaca.html" target="_blank" rel="noopener noreferrer">Stanford Alpaca<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> results using low-rank adaptation (LoRA). The LoRA model produces outputs comparable to the Stanford Alpaca model, which itself can produce results of similar quality to text-davinci-003. You will need the baseline <a href="https://huggingface.co/decapoda-research/llama-7b-hf" target="_blank" rel="noopener noreferrer">7B LLaMA model<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> downloaded and placed inside the models folder before following these steps.</p> <p><strong>Instructions:</strong></p> <ol><li><p>Navigate to the text-generation-webui folder</p></li> <li><p>Ensure it's up to date with: git pull <a href="https://github.com/oobabooga/text-generation-webui" target="_blank" rel="noopener noreferrer">https://github.com/oobabooga/text-generation-webui<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></li> <li><p>Re-install the requirements <em>if needed</em>: pip install -r requirements.txt</p></li> <li><p>Navigate to the loras folder and download the LoRA with: git lfs install &amp;&amp; git clone <a href="https://huggingface.co/tloen/alpaca-lora-7b" target="_blank" rel="noopener noreferrer">https://huggingface.co/tloen/alpaca-lora-7b<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></li> <li><p>Load LLaMa-7B in <strong>8-bit mode only</strong>: python server.py --model llama-7b-hf --load-in-8bit</p></li> <li><p>Select the LoRA in the Parameters tab</p></li></ol> <p><strong>Notes:</strong></p> <p>For this particular LoRA, the prompt must be formatted like this (the starting line must be below &quot;Response&quot;):</p> <div class="language- extra-class"><pre class="language-text"><code>Below is an instruction that describes a task. Write a response that appropriately completes the request.
### Instruction:
Tell me about alpacas.
### Response:
&lt;your cursor should be on this line&gt;
</code></pre></div><p>Message from the creator:</p> <blockquote><p>We're continually fixing bugs and conducting training runs, and the weights on the Hugging Face Hub are being updated accordingly. In particular, those facing issues with response lengths should make sure that they have the latest version of the weights and code.</p></blockquote> <p>Tips and Output Settings in text-generation-webui</p> <ul><li><p>For a ChatGPT/CharacterAI style chat, pass --cai-chat to server.py. For more info on flags, check <a href="https://github.com/oobabooga/text-generation-webui#starting-the-web-ui" target="_blank" rel="noopener noreferrer">here<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.</p></li> <li><p>Character cards can be used to guide responses toward a desired output and improve results.</p></li> <li><p>For a more creative chat, use: temp 0.72, rep pen 1.1, top_k 0, and top_p 0.73</p></li> <li><p>For a more precise chat, use temp 0.7, repetition_penalty 1.1764705882352942 (1/0.85), top_k 40, and top_p 0.1</p></li> <li><p>The <a href="https://www.reddit.com/r/LocalLLaMA/wiki/index" target="_blank" rel="noopener noreferrer">Getting Started<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> page of the wiki has a few extra tips on prompts and parameters.</p></li></ul> <p>For a quick reference, here is an example chat with LLaMA 13B:</p> <p><a href="https://preview.redd.it/yhyuzgg6d8oa1.png?width=634&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=e703895ad863c57239d9d4602f105558ce6f1a8f" target="_blank" rel="noopener noreferrer"><img src="https://preview.redd.it/yhyuzgg6d8oa1.png?width=634&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=e703895ad863c57239d9d4602f105558ce6f1a8f" alt="r/LocalLLaMA - How to install LLaMA: 8-bit and 4-bit"><span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p>Other ways to run LLaMA</p> <p><strong>If you have the hardware, it is recommended to use text-generation-webui for the best user experience.</strong></p> <p><a href="https://github.com/ggerganov/llama.cpp" target="_blank" rel="noopener noreferrer">llama.cpp<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>: a plain C/C++ implementation that runs on the CPU. There is full documentation on the GitHub page for getting started with it.</p> <p><a href="https://github.com/antimatter15/alpaca.cpp" target="_blank" rel="noopener noreferrer">alpaca.cpp<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>: a quick and easy way to try a reproduction of the Stanford Alpaca model. The GitHub page explains the setup process. With the introduction of Alpaca into llama.cpp, alpaca.cpp is deprecated but functional.</p> <p>Models Directory</p> <p><strong>Standard</strong> LLaMA models</p> <p>7B: <a href="https://huggingface.co/decapoda-research/llama-7b-hf" target="_blank" rel="noopener noreferrer">https://huggingface.co/decapoda-research/llama-7b-hf<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p>13B: <a href="https://huggingface.co/decapoda-research/llama-13b-hf" target="_blank" rel="noopener noreferrer">https://huggingface.co/decapoda-research/llama-13b-hf<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p>30B: <a href="https://huggingface.co/decapoda-research/llama-30b-hf" target="_blank" rel="noopener noreferrer">https://huggingface.co/decapoda-research/llama-30b-hf<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p>65B: <a href="https://huggingface.co/decapoda-research/llama-65b-hf" target="_blank" rel="noopener noreferrer">https://huggingface.co/decapoda-research/llama-65b-hf<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p>7B-65B 4-bit without group-size (torrent file): <a href="https://github.com/oobabooga/text-generation-webui/files/11069779/LLaMA-HF-4bit.zip" target="_blank" rel="noopener noreferrer">https://github.com/oobabooga/text-generation-webui/files/11069779/LLaMA-HF-4bit.zip<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p>7B-65B 4-bit with group-size (torrent file): <a href="https://github.com/oobabooga/text-generation-webui/files/11070361/LLaMA-HF-4bit-128g.zip" target="_blank" rel="noopener noreferrer">https://github.com/oobabooga/text-generation-webui/files/11070361/LLaMA-HF-4bit-128g.zip<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p><strong>Finetuned</strong> LLaMA models and LoRAs</p> <p><a href="https://huggingface.co/tloen/alpaca-lora-7b" target="_blank" rel="noopener noreferrer">Alpaca-LoRA 7B<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p><a href="https://huggingface.co/chansung/alpaca-lora-13b" target="_blank" rel="noopener noreferrer">Alpaca LoRA 13B<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p><a href="https://huggingface.co/chansung/alpaca-lora-30b" target="_blank" rel="noopener noreferrer">Alpaca LoRA 30B<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p><a href="https://huggingface.co/chavinlo/alpaca-native" target="_blank" rel="noopener noreferrer">Alpaca Native (7B)<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p><a href="https://huggingface.co/ozcur/alpaca-native-4bit" target="_blank" rel="noopener noreferrer">Alpaca Native 4-bit (7B)<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p><a href="https://huggingface.co/nomic-ai/gpt4all-lora" target="_blank" rel="noopener noreferrer">gpt4all LoRA 7B<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p>Resources used for this guide</p> <p><a href="https://github.com/oobabooga/text-generation-webui" target="_blank" rel="noopener noreferrer">GitHub - oobabooga/text-generation-webui<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p><a href="https://github.com/oobabooga/text-generation-webui/issues/147" target="_blank" rel="noopener noreferrer">Support for LLaMA models · Issue #147<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p><a href="https://huggingface.co/decapoda-research" target="_blank" rel="noopener noreferrer">Hugging Face Models<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p><a href="https://github.com/tloen/alpaca-lora" target="_blank" rel="noopener noreferrer">Alpaca-LoRA<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p><a href="https://huggingface.co/docs/api-inference/detailed_parameters" target="_blank" rel="noopener noreferrer">Detailed parameters<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p><a href="https://github.com/qwopqwop200/GPTQ-for-LLaMa" target="_blank" rel="noopener noreferrer">GPTQ for LLaMA<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p><a href="https://rentry.org/llama-tard-v2" target="_blank" rel="noopener noreferrer">Rentry<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p><a href="https://docs.nvidia.com/cuda/wsl-user-guide/index.html" target="_blank" rel="noopener noreferrer">NVIDIA GPU Accelerated Computing on WSL 2<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p><a href="https://gist.github.com/shawwn/03ee2422c41ef253cbef61d8c317d9ab" target="_blank" rel="noopener noreferrer">65b_sample.txt<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div><div class="global-ui"></div></div>
    <script src="/doc_notes/assets/js/app.d7b7570d.js" defer></script><script src="/doc_notes/assets/js/2.733019b2.js" defer></script><script src="/doc_notes/assets/js/25.c7ae079a.js" defer></script>
  </body>
</html>
