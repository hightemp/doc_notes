(window.webpackJsonp=window.webpackJsonp||[]).push([[18],{291:function(e,t,o){"use strict";o.r(t);var r=o(14),a=Object(r.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("p",[e._v("https://betterprogramming.pub/how-to-summarize-text-with-googles-t5-4dd1ae6238b6")]),e._v(" "),t("h1",{attrs:{id:"how-to-summarize-text-with-google-s-t5"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#how-to-summarize-text-with-google-s-t5"}},[e._v("#")]),e._v(" How to Summarize Text With Google’s T5")]),e._v(" "),t("h2",{attrs:{id:"how-to-use-cutting-edge-nlp-in-text-summarization"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#how-to-use-cutting-edge-nlp-in-text-summarization"}},[e._v("#")]),e._v(" How to use cutting-edge NLP in text summarization")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://miro.medium.com/max/700/0*VLds44FJMfhocuXs",alt:"Colored lights in the shape of letters spelling out Google."}})]),e._v(" "),t("p",[e._v("Photo by "),t("a",{attrs:{href:"https://unsplash.com/@mitchel3uo?utm_source=medium&utm_medium=referral",target:"_blank",rel:"noopener noreferrer"}},[e._v("Mitchell Luo"),t("OutboundLink")],1),e._v(" on "),t("a",{attrs:{href:"https://unsplash.com/?utm_source=medium&utm_medium=referral",target:"_blank",rel:"noopener noreferrer"}},[e._v("Unsplash"),t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("Automatic text summarization allows us to shorten long pieces of text into easy-to-read, short snippets that still convey the most important and relevant information of the original text.")]),e._v(" "),t("p",[e._v("In this article, we’ll build a simple but incredibly powerful text summarizer using Google’s T5. We’ll be using the PyTorch and Hugging Face’s Transformers frameworks.")]),e._v(" "),t("p",[e._v("This is split into three parts:")]),e._v(" "),t("ol",[t("li",[t("a",{attrs:{href:"https://betterprogramming.pub/how-to-summarize-text-with-googles-t5-4dd1ae6238b6#2d88",target:"_blank",rel:"noopener noreferrer"}},[e._v("Import and Initialization"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://betterprogramming.pub/how-to-summarize-text-with-googles-t5-4dd1ae6238b6#6976",target:"_blank",rel:"noopener noreferrer"}},[e._v("Data and Tokenization"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://betterprogramming.pub/how-to-summarize-text-with-googles-t5-4dd1ae6238b6#47b0",target:"_blank",rel:"noopener noreferrer"}},[e._v("Summary Generation"),t("OutboundLink")],1)])]),e._v(" "),t("p",[e._v("Check out the video version of this article here:")]),e._v(" "),t("h1",{attrs:{id:"imports-and-initialization"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#imports-and-initialization"}},[e._v("#")]),e._v(" Imports and Initialization")]),e._v(" "),t("p",[e._v("We need to import PyTorch and the "),t("code",[e._v("AutoTokenizer")]),e._v(" and "),t("code",[e._v("AutoModelWIthLMHead")]),e._v(" objects from the Transformers library:")]),e._v(" "),t("p",[e._v("import torch"),t("br"),e._v("\nfrom transformers import AutoTokenizer, AutoModelWithLMHead")]),e._v(" "),t("p",[e._v("PyTorch can be installed by following the instructions here, and Transformers can be installed using "),t("code",[e._v("pip install transformers")]),e._v(". If you need help setting up your ML environment in Python, check out "),t("a",{attrs:{href:"https://towardsdatascience.com/how-to-setup-python-for-machine-learning-173cb25f0206",target:"_blank",rel:"noopener noreferrer"}},[e._v("this article"),t("OutboundLink")],1),e._v(".")]),e._v(" "),t("p",[e._v("Once you have everything imported, we can initialize the tokenizer and model:")]),e._v(" "),t("p",[e._v("tokenizer = AutoTokenizer.from_pretrained('t5-base')"),t("br"),e._v("\nmodel = AutoModelWithLMHead.from_pretrained('t5-base', return_dict=True)")]),e._v(" "),t("p",[e._v("And we’re set to start processing some text data!")]),e._v(" "),t("h1",{attrs:{id:"data-and-tokenization"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#data-and-tokenization"}},[e._v("#")]),e._v(" Data and Tokenization")]),e._v(" "),t("p",[e._v("The data used here is a snippet of text from the "),t("a",{attrs:{href:"https://en.wikipedia.org/wiki/Winston_Churchill",target:"_blank",rel:"noopener noreferrer"}},[e._v("Wikipedia page of Winston Churchill"),t("OutboundLink")],1),e._v(". Of course, you can use anything you like! But if you’d like to follow along with the same data, you can copy it from here:")]),e._v(" "),t("p",[e._v("Once we have our data, we need to tokenize it using our "),t("code",[e._v("tokenizer")]),e._v(". This will take every word or punctuation character and convert them into numeric IDs, which the T5 model will read and map to a pretrained word embedding.")]),e._v(" "),t("p",[e._v("Tokenization is incredibly easy. We just call "),t("code",[e._v("tokenizer.encode")]),e._v(" on our input data:")]),e._v(" "),t("p",[e._v('inputs = tokenizer.encode("summarize: " + text,'),t("br"),e._v("\nreturn_tensors='pt',"),t("br"),e._v("\nmax_length=512,"),t("br"),e._v("\ntruncation=True)")]),e._v(" "),t("h1",{attrs:{id:"summary-generation"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#summary-generation"}},[e._v("#")]),e._v(" Summary Generation")]),e._v(" "),t("p",[e._v("We summarize our tokenized data using T5 by calling "),t("code",[e._v("model.generate")]),e._v(", like so:")]),e._v(" "),t("p",[e._v("summary_ids = model.generate(inputs, max_length=150, min_length=80, length_penalty=5., num_beams=2)")]),e._v(" "),t("ul",[t("li",[t("code",[e._v("max_length")]),e._v(" defines the maximum number of tokens we’d like in our summary")]),e._v(" "),t("li",[t("code",[e._v("min_length")]),e._v(" defines the minimum number of tokens we’d like")]),e._v(" "),t("li",[t("code",[e._v("length_penalty")]),e._v(" allows us to penalize the model more or less for producing a summary below/above the min/max thresholds we defined")]),e._v(" "),t("li",[t("code",[e._v("num_beams")]),e._v(" sets the number of beams that explore the potential tokens for the most promising predictions [1]")])]),e._v(" "),t("p",[e._v("Once we have our summary tokens, we can decode them back into a human-readable language using "),t("code",[e._v("tokenizer.decode")]),e._v(":")]),e._v(" "),t("p",[e._v("summary = tokenizer.decode(summary_ids[0])")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://miro.medium.com/max/700/1*VtDxtpWwGdl56HiGJRnvGw.png",alt:"Our T5-generated summary of the Wikipedia text"}})]),e._v(" "),t("p",[e._v("Our T5-generated summary of the Wikipedia text")]),e._v(" "),t("p",[e._v("And with that, we’ve built a text summarizer with Google’s T5!")]),e._v(" "),t("h1",{attrs:{id:"conclusion"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#conclusion"}},[e._v("#")]),e._v(" Conclusion")]),e._v(" "),t("p",[e._v("That really is all there is to it. A total of seven lines of code to begin working with one of Google’s most advanced machine learning algorithms on complex natural-language problems.")]),e._v(" "),t("p",[e._v("I find it astonishing how easy it is to put something like this together, and I hope this short tutorial has proved how accessible NLP can (sometimes) be — and perhaps solved a few problems too.")]),e._v(" "),t("p",[e._v("I hope you enjoyed this article. Thanks for reading!")])])}),[],!1,null,null,null);t.default=a.exports}}]);