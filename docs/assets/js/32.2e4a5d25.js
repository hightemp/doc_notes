(window.webpackJsonp=window.webpackJsonp||[]).push([[32],{307:function(e,t,r){"use strict";r.r(t);var n=r(14),o=Object(n.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("p",[e._v("https://pytorch.org/tutorials/beginner/chatbot_tutorial.html?highlight=chat%20bot")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/44a84f8c1764dbf61662d306ff9ed43a/chatbot_tutorial.ipynb",target:"_blank",rel:"noopener noreferrer"}},[e._v("Запустить в Google Colab"),t("OutboundLink")],1),e._v(" "),t("a",{attrs:{href:"https://pytorch.org/tutorials/_downloads/44a84f8c1764dbf61662d306ff9ed43a/chatbot_tutorial.ipynb",target:"_blank",rel:"noopener noreferrer"}},[e._v("Скачать блокнот"),t("OutboundLink")],1),e._v(" "),t("a",{attrs:{href:"https://github.com/pytorch/tutorials/blob/master/beginner_source/chatbot_tutorial.py",target:"_blank",rel:"noopener noreferrer"}},[e._v("Посмотреть на GitHub"),t("OutboundLink")],1)]),e._v(" "),t("h1",{attrs:{id:"учебник-по-чат-боту"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#учебник-по-чат-боту"}},[e._v("#")]),e._v(" УЧЕБНИК ПО ЧАТ- БОТУ"),t("a",{attrs:{href:"https://pytorch.org/tutorials/beginner/chatbot_tutorial.html?highlight=chat%20bot#chatbot-tutorial",target:"_blank",rel:"noopener noreferrer"}},[t("OutboundLink")],1)]),e._v(" "),t("p",[t("strong",[e._v("Автор:")]),e._v(" "),t("a",{attrs:{href:"https://github.com/MatthewInkawhich",target:"_blank",rel:"noopener noreferrer"}},[e._v("Мэтью Инкавич"),t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("В этом руководстве мы исследуем забавный и интересный вариант использования рекуррентных моделей последовательностей. Мы будем обучать простого чат- бота , используя сценарии фильмов из "),t("a",{attrs:{href:"https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("Cornell Movie-Dialogs Corpus"),t("OutboundLink")],1),e._v(" .")]),e._v(" "),t("p",[e._v("Разговорные модели — горячая тема в исследованиях искусственного интеллекта. Чат- ботов можно найти в самых разных условиях, включая приложения для обслуживания клиентов и службы онлайн-поддержки. Эти боты часто основаны на поисковых моделях, которые выводят предопределенные ответы на вопросы определенной формы. В строго ограниченной области, такой как служба ИТ-поддержки компании, этих моделей может быть достаточно, однако они недостаточно надежны для более общих случаев использования. Научить машину вести осмысленный разговор с человеком в нескольких областях — это исследовательский вопрос, который далек от решения. В последнее время бум глубокого обучения позволил создать мощные генеративные модели, такие как "),t("a",{attrs:{href:"https://arxiv.org/abs/1506.05869",target:"_blank",rel:"noopener noreferrer"}},[e._v("Neural Conversational Model от Google."),t("OutboundLink")],1),e._v(", что знаменует собой большой шаг к мультидоменным генеративным диалоговым моделям. В этом уроке мы реализуем такую ​​модель в PyTorch.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://pytorch.org/tutorials/_images/bot.png",alt:"бот"}})]),e._v(" "),t("blockquote",[t("p",[e._v("hello?\nBot: hello .\nwhere am I?\nBot: you re in a hospital .\nwho are you?\nBot: i m a lawyer .\nhow are you doing?\nBot: i m fine .\nare you my friend?\nBot: no .\nyou're under arrest\nBot: i m trying to help you !\ni'm just kidding\nBot: i m sorry .\nwhere are you from?\nBot: san francisco .\nit's time for me to leave\nBot: i know .\ngoodbye\nBot: goodbye .")])]),e._v(" "),t("p",[t("strong",[e._v("Основные моменты учебника")])]),e._v(" "),t("ul",[t("li",[t("p",[e._v("Обрабатывать загрузку и предварительную обработку набора данных "),t("a",{attrs:{href:"https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("Cornell Movie-Dialogs Corpus."),t("OutboundLink")],1)])]),e._v(" "),t("li",[t("p",[e._v("Реализуйте модель последовательностей с "),t("a",{attrs:{href:"https://arxiv.org/abs/1508.04025",target:"_blank",rel:"noopener noreferrer"}},[e._v("механизмами внимания Luong."),t("OutboundLink")],1)])]),e._v(" "),t("li",[t("p",[e._v("Совместное обучение моделей кодировщика и декодера с использованием мини-пакетов")])]),e._v(" "),t("li",[t("p",[e._v("Реализовать модуль декодирования жадного поиска")])]),e._v(" "),t("li",[t("p",[e._v("Взаимодействовать с обученным чат -ботом")])])]),e._v(" "),t("p",[t("strong",[e._v("Благодарности")])]),e._v(" "),t("p",[e._v("Этот учебник заимствует код из следующих источников:")]),e._v(" "),t("ol",[t("li",[t("p",[e._v("Реализация Pytorch- Chatbot от Yuan-Kuei Wu : "),t("a",{attrs:{href:"https://github.com/ywk991112/pytorch-chatbot",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://github.com/ywk991112/pytorch-chatbot"),t("OutboundLink")],1)])]),e._v(" "),t("li",[t("p",[e._v("Пример практического-pytorch seq2seq-translation Шона Робертсона: "),t("a",{attrs:{href:"https://github.com/spro/practical-pytorch/tree/master/seq2seq-translation",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://github.com/spro/practical-pytorch/tree/master/seq2seq-translation"),t("OutboundLink")],1)])]),e._v(" "),t("li",[t("p",[e._v("Код предварительной обработки FloydHub Cornell Movie Corpus: "),t("a",{attrs:{href:"https://github.com/floydhub/textutil-preprocess-cornell-movie-corpus",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://github.com/floydhub/textutil-preprocess-cornell-movie-corpus"),t("OutboundLink")],1)])])]),e._v(" "),t("h2",{attrs:{id:"подготовка"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#подготовка"}},[e._v("#")]),e._v(" Подготовка"),t("a",{attrs:{href:"https://pytorch.org/tutorials/beginner/chatbot_tutorial.html?highlight=chat%20bot#preparations",target:"_blank",rel:"noopener noreferrer"}},[t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("Для начала загрузите ZIP-файл данных "),t("a",{attrs:{href:"https://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip",target:"_blank",rel:"noopener noreferrer"}},[e._v("здесь."),t("OutboundLink")],1)]),e._v(" "),t("h1",{attrs:{id:"and-put-in-a-data-directory-under-the-current-directory"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#and-put-in-a-data-directory-under-the-current-directory"}},[e._v("#")]),e._v(" and put in a "),t("code",[e._v("data/")]),e._v(" directory under the current directory.")]),e._v(" "),t("h1",{attrs:{id:""}},[t("a",{staticClass:"header-anchor",attrs:{href:"#"}},[e._v("#")])]),e._v(" "),t("h1",{attrs:{id:"after-that-let-s-import-some-necessities"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#after-that-let-s-import-some-necessities"}},[e._v("#")]),e._v(" After that, let’s import some necessities.")]),e._v(" "),t("h1",{attrs:{id:"-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#-2"}},[e._v("#")])]),e._v(" "),t("p",[e._v("from "),t("strong",[e._v("future")]),e._v(" import absolute_import\nfrom "),t("strong",[e._v("future")]),e._v(" import division\nfrom "),t("strong",[e._v("future")]),e._v(" import print_function\nfrom "),t("strong",[e._v("future")]),e._v(" import unicode_literals")]),e._v(" "),t("p",[e._v("import torch\nfrom torch.jit import script, trace\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\nimport csv\nimport random\nimport re\nimport os\nimport unicodedata\nimport codecs\nfrom io import open\nimport itertools\nimport math\nimport json")]),e._v(" "),t("p",[e._v("USE_CUDA = "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.cuda.is_available.html#torch.cuda.is_available",title:"torch.cuda.is_available",target:"_blank",rel:"noopener noreferrer"}},[e._v("torch.cuda.is_available"),t("OutboundLink")],1),e._v("()\n"),t("a",{attrs:{href:"https://pytorch.org/docs/stable/tensor_attributes.html#torch.device",title:"torch.device",target:"_blank",rel:"noopener noreferrer"}},[e._v("device"),t("OutboundLink")],1),e._v(" = "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/tensor_attributes.html#torch.device",title:"torch.device",target:"_blank",rel:"noopener noreferrer"}},[e._v("torch.device"),t("OutboundLink")],1),e._v('("cuda" if USE_CUDA else "cpu")')]),e._v(" "),t("h2",{attrs:{id:"загрузка-и-предварительная-обработка-данных"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#загрузка-и-предварительная-обработка-данных"}},[e._v("#")]),e._v(" Загрузка и предварительная обработка данных"),t("a",{attrs:{href:"https://pytorch.org/tutorials/beginner/chatbot_tutorial.html?highlight=chat%20bot#load-preprocess-data",target:"_blank",rel:"noopener noreferrer"}},[t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("Следующим шагом является переформатирование нашего файла данных и загрузка данных в структуры, с которыми мы можем работать.")]),e._v(" "),t("p",[e._v("Корпус Cornell "),t("a",{attrs:{href:"https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("Movie-Dialogs Corpus"),t("OutboundLink")],1),e._v(" представляет собой богатый набор данных диалогов персонажей фильмов:")]),e._v(" "),t("ul",[t("li",[t("p",[e._v("220 579 диалогов между 10 292 парами киногероев")])]),e._v(" "),t("li",[t("p",[e._v("9035 персонажей из 617 фильмов")])]),e._v(" "),t("li",[t("p",[e._v("всего 304 713 высказываний")])])]),e._v(" "),t("p",[e._v("Этот набор данных большой и разнообразный, и существует большое разнообразие языковой формальности, периодов времени, настроений и т. д. Мы надеемся, что это разнообразие сделает нашу модель устойчивой ко многим формам входных данных и запросов.")]),e._v(" "),t("p",[e._v("Во-первых, мы посмотрим на некоторые строки нашего файла данных, чтобы увидеть исходный формат.")]),e._v(" "),t("p",[e._v('corpus_name = "movie-corpus"\ncorpus = os.path.join("data", corpus_name)')]),e._v(" "),t("p",[e._v("def printLines(file, n=10):\nwith open(file, 'rb') as datafile:\nlines = datafile.readlines()\nfor line in lines[:n]:\nprint(line)")]),e._v(" "),t("p",[e._v('printLines(os.path.join(corpus, "utterances.jsonl"))')]),e._v(" "),t("p",[e._v('b\'{"id": "L1045", "conversation_id": "L1044", "text": "They do not!", "speaker": "u0", "meta": {"movie_id": "m0", "parsed": [{"rt": 1, "toks": [{"tok": "They", "tag": "PRP", "dep": "nsubj", "up": 1, "dn": []}, {"tok": "do", "tag": "VBP", "dep": "ROOT", "dn": [0, 2, 3]}, {"tok": "not", "tag": "RB", "dep": "neg", "up": 1, "dn": []}, {"tok": "!", "tag": ".", "dep": "punct", "up": 1, "dn": []}]}]}, "reply-to": "L1044", "timestamp": null, "vectors": []}\\n\'\nb\'{"id": "L1044", "conversation_id": "L1044", "text": "They do to!", "speaker": "u2", "meta": {"movie_id": "m0", "parsed": [{"rt": 1, "toks": [{"tok": "They", "tag": "PRP", "dep": "nsubj", "up": 1, "dn": []}, {"tok": "do", "tag": "VBP", "dep": "ROOT", "dn": [0, 2, 3]}, {"tok": "to", "tag": "TO", "dep": "dobj", "up": 1, "dn": []}, {"tok": "!", "tag": ".", "dep": "punct", "up": 1, "dn": []}]}]}, "reply-to": null, "timestamp": null, "vectors": []}\\n\'\nb\'{"id": "L985", "conversation_id": "L984", "text": "I hope so.", "speaker": "u0", "meta": {"movie_id": "m0", "parsed": [{"rt": 1, "toks": [{"tok": "I", "tag": "PRP", "dep": "nsubj", "up": 1, "dn": []}, {"tok": "hope", "tag": "VBP", "dep": "ROOT", "dn": [0, 2, 3]}, {"tok": "so", "tag": "RB", "dep": "advmod", "up": 1, "dn": []}, {"tok": ".", "tag": ".", "dep": "punct", "up": 1, "dn": []}]}]}, "reply-to": "L984", "timestamp": null, "vectors": []}\\n\'\nb\'{"id": "L984", "conversation_id": "L984", "text": "She okay?", "speaker": "u2", "meta": {"movie_id": "m0", "parsed": [{"rt": 1, "toks": [{"tok": "She", "tag": "PRP", "dep": "nsubj", "up": 1, "dn": []}, {"tok": "okay", "tag": "RB", "dep": "ROOT", "dn": [0, 2]}, {"tok": "?", "tag": ".", "dep": "punct", "up": 1, "dn": []}]}]}, "reply-to": null, "timestamp": null, "vectors": []}\\n\'\nb\'{"id": "L925", "conversation_id": "L924", "text": "Let\'s go.", "speaker": "u0", "meta": {"movie_id": "m0", "parsed": [{"rt": 0, "toks": [{"tok": "Let", "tag": "VB", "dep": "ROOT", "dn": [2, 3]}, {"tok": "\'s", "tag": "PRP", "dep": "nsubj", "up": 2, "dn": []}, {"tok": "go", "tag": "VB", "dep": "ccomp", "up": 0, "dn": [1]}, {"tok": ".", "tag": ".", "dep": "punct", "up": 0, "dn": []}]}]}, "reply-to": "L924", "timestamp": null, "vectors": []}\\n\'\nb\'{"id": "L924", "conversation_id": "L924", "text": "Wow", "speaker": "u2", "meta": {"movie_id": "m0", "parsed": [{"rt": 0, "toks": [{"tok": "Wow", "tag": "UH", "dep": "ROOT", "dn": []}]}]}, "reply-to": null, "timestamp": null, "vectors": []}\\n\'\nb\'{"id": "L872", "conversation_id": "L870", "text": "Okay -- you\'re gonna need to learn how to lie.", "speaker": "u0", "meta": {"movie_id": "m0", "parsed": [{"rt": 4, "toks": [{"tok": "Okay", "tag": "UH", "dep": "intj", "up": 4, "dn": []}, {"tok": "--", "tag": ":", "dep": "punct", "up": 4, "dn": []}, {"tok": "you", "tag": "PRP", "dep": "nsubj", "up": 4, "dn": []}, {"tok": "\'re", "tag": "VBP", "dep": "aux", "up": 4, "dn": []}, {"tok": "gon", "tag": "VBG", "dep": "ROOT", "dn": [0, 1, 2, 3, 6, 12]}, {"tok": "na", "tag": "TO", "dep": "aux", "up": 6, "dn": []}, {"tok": "need", "tag": "VB", "dep": "xcomp", "up": 4, "dn": [5, 8]}, {"tok": "to", "tag": "TO", "dep": "aux", "up": 8, "dn": []}, {"tok": "learn", "tag": "VB", "dep": "xcomp", "up": 6, "dn": [7, 11]}, {"tok": "how", "tag": "WRB", "dep": "advmod", "up": 11, "dn": []}, {"tok": "to", "tag": "TO", "dep": "aux", "up": 11, "dn": []}, {"tok": "lie", "tag": "VB", "dep": "xcomp", "up": 8, "dn": [9, 10]}, {"tok": ".", "tag": ".", "dep": "punct", "up": 4, "dn": []}]}]}, "reply-to": "L871", "timestamp": null, "vectors": []}\\n\'\nb\'{"id": "L871", "conversation_id": "L870", "text": "No", "speaker": "u2", "meta": {"movie_id": "m0", "parsed": [{"rt": 0, "toks": [{"tok": "No", "tag": "UH", "dep": "ROOT", "dn": []}]}]}, "reply-to": "L870", "timestamp": null, "vectors": []}\\n\'\nb\'{"id": "L870", "conversation_id": "L870", "text": "I\'m kidding.  You know how sometimes you just become this \\"persona\\"?  And you don\'t know how to quit?", "speaker": "u0", "meta": {"movie_id": "m0", "parsed": [{"rt": 2, "toks": [{"tok": "I", "tag": "PRP", "dep": "nsubj", "up": 2, "dn": []}, {"tok": "\'m", "tag": "VBP", "dep": "aux", "up": 2, "dn": []}, {"tok": "kidding", "tag": "VBG", "dep": "ROOT", "dn": [0, 1, 3]}, {"tok": ".", "tag": ".", "dep": "punct", "up": 2, "dn": [4]}, {"tok": " ", "tag": "_SP", "dep": "", "up": 3, "dn": []}]}, {"rt": 1, "toks": [{"tok": "You", "tag": "PRP", "dep": "nsubj", "up": 1, "dn": []}, {"tok": "know", "tag": "VBP", "dep": "ROOT", "dn": [0, 6, 11]}, {"tok": "how", "tag": "WRB", "dep": "advmod", "up": 3, "dn": []}, {"tok": "sometimes", "tag": "RB", "dep": "advmod", "up": 6, "dn": [2]}, {"tok": "you", "tag": "PRP", "dep": "nsubj", "up": 6, "dn": []}, {"tok": "just", "tag": "RB", "dep": "advmod", "up": 6, "dn": []}, {"tok": "become", "tag": "VBP", "dep": "ccomp", "up": 1, "dn": [3, 4, 5, 9]}, {"tok": "this", "tag": "DT", "dep": "det", "up": 9, "dn": []}, {"tok": "\\"", "tag": "``", "dep": "punct", "up": 9, "dn": []}, {"tok": "persona", "tag": "NN", "dep": "attr", "up": 6, "dn": [7, 8, 10]}, {"tok": "\\"", "tag": "\'\'", "dep": "punct", "up": 9, "dn": []}, {"tok": "?", "tag": ".", "dep": "punct", "up": 1, "dn": [12]}, {"tok": " ", "tag": "_SP", "dep": "", "up": 11, "dn": []}]}, {"rt": 4, "toks": [{"tok": "And", "tag": "CC", "dep": "cc", "up": 4, "dn": []}, {"tok": "you", "tag": "PRP", "dep": "nsubj", "up": 4, "dn": []}, {"tok": "do", "tag": "VBP", "dep": "aux", "up": 4, "dn": []}, {"tok": "n\'t", "tag": "RB", "dep": "neg", "up": 4, "dn": []}, {"tok": "know", "tag": "VB", "dep": "ROOT", "dn": [0, 1, 2, 3, 7, 8]}, {"tok": "how", "tag": "WRB", "dep": "advmod", "up": 7, "dn": []}, {"tok": "to", "tag": "TO", "dep": "aux", "up": 7, "dn": []}, {"tok": "quit", "tag": "VB", "dep": "xcomp", "up": 4, "dn": [5, 6]}, {"tok": "?", "tag": ".", "dep": "punct", "up": 4, "dn": []}]}]}, "reply-to": null, "timestamp": null, "vectors": []}\\n\'\nb\'{"id": "L869", "conversation_id": "L866", "text": "Like my fear of wearing pastels?", "speaker": "u0", "meta": {"movie_id": "m0", "parsed": [{"rt": 0, "toks": [{"tok": "Like", "tag": "IN", "dep": "ROOT", "dn": [2, 6]}, {"tok": "my", "tag": "PRP$", "dep": "poss", "up": 2, "dn": []}, {"tok": "fear", "tag": "NN", "dep": "pobj", "up": 0, "dn": [1, 3]}, {"tok": "of", "tag": "IN", "dep": "prep", "up": 2, "dn": [4]}, {"tok": "wearing", "tag": "VBG", "dep": "pcomp", "up": 3, "dn": [5]}, {"tok": "pastels", "tag": "NNS", "dep": "dobj", "up": 4, "dn": []}, {"tok": "?", "tag": ".", "dep": "punct", "up": 0, "dn": []}]}]}, "reply-to": "L868", "timestamp": null, "vectors": []}\\n\'')]),e._v(" "),t("h3",{attrs:{id:"создать-форматированныи-фаил-данных"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#создать-форматированныи-фаил-данных"}},[e._v("#")]),e._v(" Создать форматированный файл данных"),t("a",{attrs:{href:"https://pytorch.org/tutorials/beginner/chatbot_tutorial.html?highlight=chat%20bot#create-formatted-data-file",target:"_blank",rel:"noopener noreferrer"}},[t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("Для удобства мы создадим хорошо отформатированный файл данных, в котором каждая строка будет содержать разделенное табуляцией предложение- "),t("em",[e._v("запрос")]),e._v(" и пару "),t("em",[e._v("предложений-ответов")]),e._v(" .")]),e._v(" "),t("p",[e._v("Следующие функции упрощают разбор необработанного файла данных "),t("em",[e._v("utterances.jsonl")]),e._v(" .")]),e._v(" "),t("ul",[t("li",[t("p",[t("code",[e._v("loadLinesAndConversations")]),e._v("разбивает каждую строку файла на словарь строк с полями: lineID, characterID и текст, а затем группирует их в диалоги с полями: talkID, movieID и строки.")])]),e._v(" "),t("li",[t("p",[t("code",[e._v("extractSentencePairs")]),e._v("извлекает пары предложений из разговоров")])])]),e._v(" "),t("h1",{attrs:{id:"splits-each-line-of-the-file-to-create-lines-and-conversations"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#splits-each-line-of-the-file-to-create-lines-and-conversations"}},[e._v("#")]),e._v(" Splits each line of the file to create lines and conversations")]),e._v(" "),t("p",[e._v('def loadLinesAndConversations(fileName):\nlines = {}\nconversations = {}\nwith open(fileName, \'r\', encoding=\'iso-8859-1\') as f:\nfor line in f:\nlineJson = json.loads(line)\n# Extract fields for line object\nlineObj = {}\nlineObj["lineID"] = lineJson["id"]\nlineObj["characterID"] = lineJson["speaker"]\nlineObj["text"] = lineJson["text"]\nlines[lineObj[\'lineID\']] = lineObj')]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",[t("code",[e._v('        # Extract fields for conversation object\n        if lineJson["conversation_id"] not in conversations:\n            convObj = {}\n            convObj["conversationID"] = lineJson["conversation_id"]\n            convObj["movieID"] = lineJson["meta"]["movie_id"]\n            convObj["lines"] = [lineObj]\n        else:\n            convObj = conversations[lineJson["conversation_id"]]\n            convObj["lines"].insert(0, lineObj)\n        conversations[convObj["conversationID"]] = convObj\n\nreturn lines, conversations\n')])])]),t("h1",{attrs:{id:"extracts-pairs-of-sentences-from-conversations"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#extracts-pairs-of-sentences-from-conversations"}},[e._v("#")]),e._v(" Extracts pairs of sentences from conversations")]),e._v(" "),t("p",[e._v('def extractSentencePairs(conversations):\nqa_pairs = []\nfor conversation in conversations.values():\n# Iterate over all the lines of the conversation\nfor i in range(len(conversation["lines"]) - 1):  # We ignore the last line (no answer for it)\ninputLine = conversation["lines"][i]["text"].strip()\ntargetLine = conversation["lines"][i+1]["text"].strip()\n# Filter wrong samples (if one of the lists is empty)\nif inputLine and targetLine:\nqa_pairs.append([inputLine, targetLine])\nreturn qa_pairs')]),e._v(" "),t("p",[e._v("Теперь мы вызовем эти функции и создадим файл. Мы назовем его "),t("em",[e._v("formatted_movie_lines.txt")]),e._v(" .")]),e._v(" "),t("h1",{attrs:{id:"define-path-to-new-file"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#define-path-to-new-file"}},[e._v("#")]),e._v(" Define path to new file")]),e._v(" "),t("p",[e._v('datafile = os.path.join(corpus, "formatted_movie_lines.txt")')]),e._v(" "),t("p",[e._v("delimiter = '\\t'")]),e._v(" "),t("h1",{attrs:{id:"unescape-the-delimiter"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#unescape-the-delimiter"}},[e._v("#")]),e._v(" Unescape the delimiter")]),e._v(" "),t("p",[e._v('delimiter = str(codecs.decode(delimiter, "unicode_escape"))')]),e._v(" "),t("h1",{attrs:{id:"initialize-lines-dict-and-conversations-dict"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#initialize-lines-dict-and-conversations-dict"}},[e._v("#")]),e._v(" Initialize lines dict and conversations dict")]),e._v(" "),t("p",[e._v("lines = {}\nconversations = {}")]),e._v(" "),t("h1",{attrs:{id:"load-lines-and-conversations"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#load-lines-and-conversations"}},[e._v("#")]),e._v(" Load lines and conversations")]),e._v(" "),t("p",[e._v('print("\\nProcessing corpus into lines and conversations...")\nlines, conversations = loadLinesAndConversations(os.path.join(corpus, "utterances.jsonl"))')]),e._v(" "),t("h1",{attrs:{id:"write-new-csv-file"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#write-new-csv-file"}},[e._v("#")]),e._v(" Write new csv file")]),e._v(" "),t("p",[e._v("print(\"\\nWriting newly formatted file...\")\nwith open(datafile, 'w', encoding='utf-8') as outputfile:\nwriter = csv.writer(outputfile, delimiter=delimiter, lineterminator='\\n')\nfor pair in extractSentencePairs(conversations):\nwriter.writerow(pair)")]),e._v(" "),t("h1",{attrs:{id:"print-a-sample-of-lines"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#print-a-sample-of-lines"}},[e._v("#")]),e._v(" Print a sample of lines")]),e._v(" "),t("p",[e._v('print("\\nSample lines from file:")\nprintLines(datafile)')]),e._v(" "),t("p",[e._v("Processing corpus into lines and conversations...")]),e._v(" "),t("p",[e._v("Writing newly formatted file...")]),e._v(" "),t("p",[e._v('Sample lines from file:\nb\'They do to!\\tThey do not!\\n\'\nb\'She okay?\\tI hope so.\\n\'\nb"Wow\\tLet\'s go.\\n"\nb\'"I\'m kidding.  You know how sometimes you just become this ""persona""?  And you don\'t know how to quit?"\\tNo\\n\'\nb"No\\tOkay -- you\'re gonna need to learn how to lie.\\n"\nb"I figured you\'d get to the good stuff eventually.\\tWhat good stuff?\\n"\nb\'What good stuff?\\t"The ""real you""."\\n\'\nb\'"The ""real you""."\\tLike my fear of wearing pastels?\\n\'\nb\'do you listen to this crap?\\tWhat crap?\\n\'\nb"What crap?\\tMe.  This endless ...blonde babble. I\'m like, boring myself.\\n"')]),e._v(" "),t("h3",{attrs:{id:"загрузка-и-обрезка-данных"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#загрузка-и-обрезка-данных"}},[e._v("#")]),e._v(" Загрузка и обрезка данных"),t("a",{attrs:{href:"https://pytorch.org/tutorials/beginner/chatbot_tutorial.html?highlight=chat%20bot#load-and-trim-data",target:"_blank",rel:"noopener noreferrer"}},[t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("Наша следующая задача — создать словарь и загрузить в память пары предложений «запрос/ответ».")]),e._v(" "),t("p",[e._v("Обратите внимание, что мы имеем дело с последовательностями "),t("strong",[e._v("слов")]),e._v(" , которые не имеют неявного отображения в дискретное числовое пространство. Таким образом, мы должны создать его, сопоставив каждое уникальное слово, которое мы встречаем в нашем наборе данных, со значением индекса.")]),e._v(" "),t("p",[e._v("Для этого мы определяем "),t("code",[e._v("Voc")]),e._v("класс, который хранит отображение слов в индексы, обратное отображение индексов в слова, количество каждого слова и общее количество слов. Класс предоставляет методы для добавления слова в словарь ( "),t("code",[e._v("addWord")]),e._v("), добавления всех слов в предложение ( "),t("code",[e._v("addSentence")]),e._v(") и удаления редко встречающихся слов ( "),t("code",[e._v("trim")]),e._v("). Подробнее о обрезке позже.")]),e._v(" "),t("h1",{attrs:{id:"default-word-tokens"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#default-word-tokens"}},[e._v("#")]),e._v(" Default word tokens")]),e._v(" "),t("p",[e._v("PAD_token = 0  # Used for padding short sentences\nSOS_token = 1  # Start-of-sentence token\nEOS_token = 2  # End-of-sentence token")]),e._v(" "),t("p",[e._v("class Voc:\ndef "),t("strong",[e._v("init")]),e._v('(self, name):\nself.name = name\nself.trimmed = False\nself.word2index = {}\nself.word2count = {}\nself.index2word = {PAD_token: "PAD", SOS_token: "SOS", EOS_token: "EOS"}\nself.num_words = 3  # Count SOS, EOS, PAD')]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",[t("code",[e._v('def addSentence(self, sentence):\n    for word in sentence.split(\' \'):\n        self.addWord(word)\n\ndef addWord(self, word):\n    if word not in self.word2index:\n        self.word2index[word] = self.num_words\n        self.word2count[word] = 1\n        self.index2word[self.num_words] = word\n        self.num_words += 1\n    else:\n        self.word2count[word] += 1\n\n# Remove words below a certain count threshold\ndef trim(self, min_count):\n    if self.trimmed:\n        return\n    self.trimmed = True\n\n    keep_words = []\n\n    for k, v in self.word2count.items():\n        if v >= min_count:\n            keep_words.append(k)\n\n    print(\'keep_words {} / {} = {:.4f}\'.format(\n        len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n    ))\n\n    # Reinitialize dictionaries\n    self.word2index = {}\n    self.word2count = {}\n    self.index2word = {PAD_token: "PAD", SOS_token: "SOS", EOS_token: "EOS"}\n    self.num_words = 3 # Count default tokens\n\n    for word in keep_words:\n        self.addWord(word)\n')])])]),t("p",[e._v("Теперь мы можем собрать наш словарный запас и пары предложений «запрос/ответ». Прежде чем мы будем готовы использовать эти данные, мы должны выполнить некоторую предварительную обработку.")]),e._v(" "),t("p",[e._v("Во-первых, мы должны преобразовать строки Unicode в ASCII, используя файлы "),t("code",[e._v("unicodeToAscii")]),e._v(". Затем мы должны преобразовать все буквы в нижний регистр и обрезать все небуквенные символы, кроме основных знаков препинания ( "),t("code",[e._v("normalizeString")]),e._v("). Наконец, чтобы помочь в обучении сходимости, мы будем отфильтровывать предложения, длина которых превышает "),t("code",[e._v("MAX_LENGTH")]),e._v(" пороговое значение ( "),t("code",[e._v("filterPairs")]),e._v(").")]),e._v(" "),t("p",[e._v("MAX_LENGTH = 10  # Maximum sentence length to consider")]),e._v(" "),t("h1",{attrs:{id:"turn-a-unicode-string-to-plain-ascii-thanks-to"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#turn-a-unicode-string-to-plain-ascii-thanks-to"}},[e._v("#")]),e._v(" Turn a Unicode string to plain ASCII, thanks to")]),e._v(" "),t("h1",{attrs:{id:"https-stackoverflow-com-a-518232-2809427"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#https-stackoverflow-com-a-518232-2809427"}},[e._v("#")]),e._v(" https://stackoverflow.com/a/518232/2809427")]),e._v(" "),t("p",[e._v("def unicodeToAscii(s):\nreturn ''.join(\nc for c in unicodedata.normalize('NFD', s)\nif unicodedata.category(c) != 'Mn'\n)")]),e._v(" "),t("h1",{attrs:{id:"lowercase-trim-and-remove-non-letter-characters"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#lowercase-trim-and-remove-non-letter-characters"}},[e._v("#")]),e._v(" Lowercase, trim, and remove non-letter characters")]),e._v(" "),t("p",[e._v('def normalizeString(s):\ns = unicodeToAscii(s.lower().strip())\ns = re.sub(r"([.!?])", r" \\1", s)\ns = re.sub(r"[^a-zA-Z.!?]+", r" ", s)\ns = re.sub(r"\\s+", r" ", s).strip()\nreturn s')]),e._v(" "),t("h1",{attrs:{id:"read-query-response-pairs-and-return-a-voc-object"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#read-query-response-pairs-and-return-a-voc-object"}},[e._v("#")]),e._v(" Read query/response pairs and return a voc object")]),e._v(" "),t("p",[e._v("def readVocs(datafile, corpus_name):\nprint(\"Reading lines...\")\n# Read the file and split into lines\nlines = open(datafile, encoding='utf-8')."),t("br"),e._v("\nread().strip().split('\\n')\n# Split every line into pairs and normalize\npairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\nvoc = Voc(corpus_name)\nreturn voc, pairs")]),e._v(" "),t("h1",{attrs:{id:"returns-true-iff-both-sentences-in-a-pair-p-are-under-the-max-length-threshold"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#returns-true-iff-both-sentences-in-a-pair-p-are-under-the-max-length-threshold"}},[e._v("#")]),e._v(" Returns True iff both sentences in a pair 'p' are under the MAX_LENGTH threshold")]),e._v(" "),t("p",[e._v("def filterPair(p):\n# Input sequences need to preserve the last word for EOS token\nreturn len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH")]),e._v(" "),t("h1",{attrs:{id:"filter-pairs-using-filterpair-condition"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#filter-pairs-using-filterpair-condition"}},[e._v("#")]),e._v(" Filter pairs using filterPair condition")]),e._v(" "),t("p",[e._v("def filterPairs(pairs):\nreturn [pair for pair in pairs if filterPair(pair)]")]),e._v(" "),t("h1",{attrs:{id:"using-the-functions-defined-above-return-a-populated-voc-object-and-pairs-list"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#using-the-functions-defined-above-return-a-populated-voc-object-and-pairs-list"}},[e._v("#")]),e._v(" Using the functions defined above, return a populated voc object and pairs list")]),e._v(" "),t("p",[e._v('def loadPrepareData(corpus, corpus_name, datafile, save_dir):\nprint("Start preparing training data ...")\nvoc, pairs = readVocs(datafile, corpus_name)\nprint("Read {!s} sentence pairs".format(len(pairs)))\npairs = filterPairs(pairs)\nprint("Trimmed to {!s} sentence pairs".format(len(pairs)))\nprint("Counting words...")\nfor pair in pairs:\nvoc.addSentence(pair[0])\nvoc.addSentence(pair[1])\nprint("Counted words:", voc.num_words)\nreturn voc, pairs')]),e._v(" "),t("h1",{attrs:{id:"load-assemble-voc-and-pairs"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#load-assemble-voc-and-pairs"}},[e._v("#")]),e._v(" Load/Assemble voc and pairs")]),e._v(" "),t("p",[e._v('save_dir = os.path.join("data", "save")\nvoc, pairs = loadPrepareData(corpus, corpus_name, datafile, save_dir)')]),e._v(" "),t("h1",{attrs:{id:"print-some-pairs-to-validate"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#print-some-pairs-to-validate"}},[e._v("#")]),e._v(" Print some pairs to validate")]),e._v(" "),t("p",[e._v('print("\\npairs:")\nfor pair in pairs[:10]:\nprint(pair)')]),e._v(" "),t("p",[e._v("Start preparing training data ...\nReading lines...\nRead 221282 sentence pairs\nTrimmed to 64313 sentence pairs\nCounting words...\nCounted words: 18082")]),e._v(" "),t("p",[e._v("pairs:\n['they do to !', 'they do not !']\n['she okay ?', 'i hope so .']\n['wow', 'let s go .']\n['what good stuff ?', 'the real you .']\n['the real you .', 'like my fear of wearing pastels ?']\n['do you listen to this crap ?', 'what crap ?']\n['well no . . .', 'then that s all you had to say .']\n['then that s all you had to say .', 'but']\n['but', 'you always been this selfish ?']\n['have fun tonight ?', 'tons']")]),e._v(" "),t("p",[e._v("Еще одна тактика, полезная для достижения более быстрой конвергенции во время обучения, — это удаление редко используемых слов из нашего словарного запаса. Уменьшение пространства признаков также смягчит сложность функции, которую модель должна научиться аппроксимировать. Мы сделаем это как двухэтапный процесс:")]),e._v(" "),t("ol",[t("li",[t("p",[e._v("Обрежьте слова, используемые под "),t("code",[e._v("MIN_COUNT")]),e._v("порогом, с помощью "),t("code",[e._v("voc.trim")]),e._v(" функции.")])]),e._v(" "),t("li",[t("p",[e._v("Отфильтруйте пары с обрезанными словами.")])])]),e._v(" "),t("p",[e._v("MIN_COUNT = 3    # Minimum word count threshold for trimming")]),e._v(" "),t("p",[e._v("def trimRareWords(voc, pairs, MIN_COUNT):\n# Trim words used under the MIN_COUNT from the voc\nvoc.trim(MIN_COUNT)\n# Filter out pairs with trimmed words\nkeep_pairs = []\nfor pair in pairs:\ninput_sentence = pair[0]\noutput_sentence = pair[1]\nkeep_input = True\nkeep_output = True\n# Check input sentence\nfor word in input_sentence.split(' '):\nif word not in voc.word2index:\nkeep_input = False\nbreak\n# Check output sentence\nfor word in output_sentence.split(' '):\nif word not in voc.word2index:\nkeep_output = False\nbreak")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",[t("code",[e._v('    # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n    if keep_input and keep_output:\n        keep_pairs.append(pair)\n\nprint("Trimmed from {} pairs to {}, {:.4f} of total".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\nreturn keep_pairs\n')])])]),t("h1",{attrs:{id:"trim-voc-and-pairs"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#trim-voc-and-pairs"}},[e._v("#")]),e._v(" Trim voc and pairs")]),e._v(" "),t("p",[e._v("pairs = trimRareWords(voc, pairs, MIN_COUNT)")]),e._v(" "),t("p",[e._v("keep_words 7833 / 18079 = 0.4333\nTrimmed from 64313 pairs to 53131, 0.8261 of total")]),e._v(" "),t("h2",{attrs:{id:"подготовьте-данные-для-моделеи"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#подготовьте-данные-для-моделеи"}},[e._v("#")]),e._v(" Подготовьте данные для моделей"),t("a",{attrs:{href:"https://pytorch.org/tutorials/beginner/chatbot_tutorial.html?highlight=chat%20bot#prepare-data-for-models",target:"_blank",rel:"noopener noreferrer"}},[t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("Несмотря на то, что мы приложили немало усилий для подготовки и преобразования наших данных в красивый объект словаря и список пар предложений, наши модели в конечном итоге будут ожидать числовых тензоров факелов в качестве входных данных. Один из способов подготовки обработанных данных для моделей можно найти в "),t("a",{attrs:{href:"https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("учебнике по переводу seq2seq"),t("OutboundLink")],1),e._v(" . В этом уроке мы используем размер пакета 1, что означает, что все, что нам нужно сделать, это преобразовать слова в наших парах предложений в соответствующие индексы из словаря и передать это моделям.")]),e._v(" "),t("p",[e._v("Однако, если вы заинтересованы в ускорении обучения и/или хотите использовать возможности распараллеливания графического процессора, вам потребуется тренироваться с помощью мини-пакетов.")]),e._v(" "),t("p",[e._v("Использование мини-пакетов также означает, что мы должны помнить об изменении длины предложений в наших пакетах. Чтобы разместить предложения разного размера в одном пакете, мы создадим наш тензор пакетного ввода формы "),t("em",[e._v("(max_length, batch_size)")]),e._v(" , где предложения короче "),t("em",[e._v("max_length")]),e._v(" дополняются нулями после "),t("em",[e._v("EOS_token")]),e._v(" .")]),e._v(" "),t("p",[e._v("Если бы мы просто преобразовали наши английские предложения в тензоры, преобразовав слова в их индексы ( "),t("code",[e._v("indexesFromSentence")]),e._v(") и нулевой знак, наш тензор имел бы форму "),t("em",[e._v("(размер_пакета, максимальная_длина)")]),e._v(" , и индексация первого измерения вернула бы полную последовательность во всех временных шагах. Однако нам нужно иметь возможность индексировать наш пакет во времени и по всем последовательностям в пакете. Поэтому мы транспонируем форму входного пакета в "),t("em",[e._v("(max_length, batch_size)")]),e._v(" , чтобы индексирование по первому измерению возвращало временной шаг для всех предложений в пакете. Мы обрабатываем это транспонирование неявно в "),t("code",[e._v("zeroPadding")]),e._v("функции.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://pytorch.org/tutorials/_images/seq2seq_batches.png",alt:"партии"}})]),e._v(" "),t("p",[e._v("Функция "),t("code",[e._v("inputVar")]),e._v("обрабатывает процесс преобразования предложений в тензор, в конечном итоге создавая тензор правильной формы с нулевым дополнением. Он также возвращает тензор для "),t("code",[e._v("lengths")]),e._v("каждой последовательности в пакете, который позже будет передан нашему декодеру.")]),e._v(" "),t("p",[e._v("Функция "),t("code",[e._v("outputVar")]),e._v("выполняет ту же функцию, что и "),t("code",[e._v("inputVar")]),e._v(", но вместо возврата "),t("code",[e._v("lengths")]),e._v("тензора она возвращает тензор бинарной маски и максимальную длину целевого предложения. Тензор бинарной маски имеет ту же форму, что и выходной целевой тензор, но каждый элемент, являющийся "),t("em",[e._v("PAD_token,")]),e._v(" равен 0, а все остальные — 1.")]),e._v(" "),t("p",[t("code",[e._v("batch2TrainData")]),e._v("просто берет кучу пар и возвращает входной и целевой тензоры, используя вышеупомянутые функции.")]),e._v(" "),t("p",[e._v("def indexesFromSentence(voc, sentence):\nreturn [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]")]),e._v(" "),t("p",[e._v("def zeroPadding(l, fillvalue=PAD_token):\nreturn list(itertools.zip_longest(*l, fillvalue=fillvalue))")]),e._v(" "),t("p",[e._v("def binaryMatrix(l, value=PAD_token):\nm = []\nfor i, seq in enumerate(l):\nm.append([])\nfor token in seq:\nif token == PAD_token:\nm[i].append(0)\nelse:\nm[i].append(1)\nreturn m")]),e._v(" "),t("h1",{attrs:{id:"returns-padded-input-sequence-tensor-and-lengths"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#returns-padded-input-sequence-tensor-and-lengths"}},[e._v("#")]),e._v(" Returns padded input sequence tensor and lengths")]),e._v(" "),t("p",[e._v("def inputVar(l, voc):\nindexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n"),t("a",{attrs:{href:"https://pytorch.org/docs/stable/tensors.html#torch.Tensor",title:"torch.Tensor",target:"_blank",rel:"noopener noreferrer"}},[e._v("lengths"),t("OutboundLink")],1),e._v(" = "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor",title:"torch.tensor",target:"_blank",rel:"noopener noreferrer"}},[e._v("torch.tensor"),t("OutboundLink")],1),e._v("([len(indexes) for indexes in indexes_batch])\npadList = zeroPadding(indexes_batch)\npadVar = torch.LongTensor(padList)\nreturn padVar, "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/tensors.html#torch.Tensor",title:"torch.Tensor",target:"_blank",rel:"noopener noreferrer"}},[e._v("lengths"),t("OutboundLink")],1)]),e._v(" "),t("h1",{attrs:{id:"returns-padded-target-sequence-tensor-padding-mask-and-max-target-length"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#returns-padded-target-sequence-tensor-padding-mask-and-max-target-length"}},[e._v("#")]),e._v(" Returns padded target sequence tensor, padding mask, and max target length")]),e._v(" "),t("p",[e._v("def outputVar(l, voc):\nindexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\nmax_target_len = max([len(indexes) for indexes in indexes_batch])\npadList = zeroPadding(indexes_batch)\n"),t("a",{attrs:{href:"https://pytorch.org/docs/stable/tensors.html#torch.Tensor",title:"torch.Tensor",target:"_blank",rel:"noopener noreferrer"}},[e._v("mask"),t("OutboundLink")],1),e._v(" = binaryMatrix(padList)\n"),t("a",{attrs:{href:"https://pytorch.org/docs/stable/tensors.html#torch.Tensor",title:"torch.Tensor",target:"_blank",rel:"noopener noreferrer"}},[e._v("mask"),t("OutboundLink")],1),e._v(" = torch.BoolTensor("),t("a",{attrs:{href:"https://pytorch.org/docs/stable/tensors.html#torch.Tensor",title:"torch.Tensor",target:"_blank",rel:"noopener noreferrer"}},[e._v("mask"),t("OutboundLink")],1),e._v(")\npadVar = torch.LongTensor(padList)\nreturn padVar, "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/tensors.html#torch.Tensor",title:"torch.Tensor",target:"_blank",rel:"noopener noreferrer"}},[e._v("mask"),t("OutboundLink")],1),e._v(", max_target_len")]),e._v(" "),t("h1",{attrs:{id:"returns-all-items-for-a-given-batch-of-pairs"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#returns-all-items-for-a-given-batch-of-pairs"}},[e._v("#")]),e._v(" Returns all items for a given batch of pairs")]),e._v(" "),t("p",[e._v('def batch2TrainData(voc, pair_batch):\npair_batch.sort(key=lambda x: len(x[0].split(" ")), reverse=True)\ninput_batch, output_batch = [], []\nfor pair in pair_batch:\ninput_batch.append(pair[0])\noutput_batch.append(pair[1])\ninp, '),t("a",{attrs:{href:"https://pytorch.org/docs/stable/tensors.html#torch.Tensor",title:"torch.Tensor",target:"_blank",rel:"noopener noreferrer"}},[e._v("lengths"),t("OutboundLink")],1),e._v(" = inputVar(input_batch, voc)\noutput, "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/tensors.html#torch.Tensor",title:"torch.Tensor",target:"_blank",rel:"noopener noreferrer"}},[e._v("mask"),t("OutboundLink")],1),e._v(", max_target_len = outputVar(output_batch, voc)\nreturn inp, "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/tensors.html#torch.Tensor",title:"torch.Tensor",target:"_blank",rel:"noopener noreferrer"}},[e._v("lengths"),t("OutboundLink")],1),e._v(", output, "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/tensors.html#torch.Tensor",title:"torch.Tensor",target:"_blank",rel:"noopener noreferrer"}},[e._v("mask"),t("OutboundLink")],1),e._v(", max_target_len")]),e._v(" "),t("h1",{attrs:{id:"example-for-validation"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#example-for-validation"}},[e._v("#")]),e._v(" Example for validation")]),e._v(" "),t("p",[e._v("small_batch_size = 5\nbatches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n"),t("a",{attrs:{href:"https://pytorch.org/docs/stable/tensors.html#torch.Tensor",title:"torch.Tensor",target:"_blank",rel:"noopener noreferrer"}},[e._v("input_variable"),t("OutboundLink")],1),e._v(", "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/tensors.html#torch.Tensor",title:"torch.Tensor",target:"_blank",rel:"noopener noreferrer"}},[e._v("lengths"),t("OutboundLink")],1),e._v(", "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/tensors.html#torch.Tensor",title:"torch.Tensor",target:"_blank",rel:"noopener noreferrer"}},[e._v("target_variable"),t("OutboundLink")],1),e._v(", "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/tensors.html#torch.Tensor",title:"torch.Tensor",target:"_blank",rel:"noopener noreferrer"}},[e._v("mask"),t("OutboundLink")],1),e._v(", max_target_len = batches")]),e._v(" "),t("p",[e._v('print("input_variable:", '),t("a",{attrs:{href:"https://pytorch.org/docs/stable/tensors.html#torch.Tensor",title:"torch.Tensor",target:"_blank",rel:"noopener noreferrer"}},[e._v("input_variable"),t("OutboundLink")],1),e._v(')\nprint("lengths:", '),t("a",{attrs:{href:"https://pytorch.org/docs/stable/tensors.html#torch.Tensor",title:"torch.Tensor",target:"_blank",rel:"noopener noreferrer"}},[e._v("lengths"),t("OutboundLink")],1),e._v(')\nprint("target_variable:", '),t("a",{attrs:{href:"https://pytorch.org/docs/stable/tensors.html#torch.Tensor",title:"torch.Tensor",target:"_blank",rel:"noopener noreferrer"}},[e._v("target_variable"),t("OutboundLink")],1),e._v(')\nprint("mask:", '),t("a",{attrs:{href:"https://pytorch.org/docs/stable/tensors.html#torch.Tensor",title:"torch.Tensor",target:"_blank",rel:"noopener noreferrer"}},[e._v("mask"),t("OutboundLink")],1),e._v(')\nprint("max_target_len:", max_target_len)')]),e._v(" "),t("p",[e._v("input_variable: tensor([[   8,   50, 2218, 3903,   11],\n[ 176,  109,   19,    4,  200],\n[ 386,   22,   22, 1197, 3203],\n[  84, 1913,  595,   14,   14],\n[2426,   50,   10,    2,    2],\n[ 112,  113,    2,    0,    0],\n[  14,  784,    0,    0,    0],\n[  14,  615,    0,    0,    0],\n[  14,   14,    0,    0,    0],\n[   2,    2,    0,    0,    0]])\nlengths: tensor([10, 10,  6,  5,  5])\ntarget_variable: tensor([[  50,    5,   50,  335,  828],\n[ 531,  641,   93,   22,   14],\n[ 185,  161,  136,  167,   75],\n[ 605,   24,    5,   44,   17],\n[  36,    6,  700,   62,   22],\n[ 688,    2,   79,  708, 2264],\n[  14,    0, 2214,   10,  167],\n[  14,    0,    2,    2,   10],\n[  14,    0,    0,    0,    2],\n[   2,    0,    0,    0,    0]])\nmask: tensor([[ True,  True,  True,  True,  True],\n[ True,  True,  True,  True,  True],\n[ True,  True,  True,  True,  True],\n[ True,  True,  True,  True,  True],\n[ True,  True,  True,  True,  True],\n[ True,  True,  True,  True,  True],\n[ True, False,  True,  True,  True],\n[ True, False,  True,  True,  True],\n[ True, False, False, False,  True],\n[ True, False, False, False, False]])\nmax_target_len: 10")]),e._v(" "),t("h2",{attrs:{id:"определить-модели"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#определить-модели"}},[e._v("#")]),e._v(" Определить модели"),t("a",{attrs:{href:"https://pytorch.org/tutorials/beginner/chatbot_tutorial.html?highlight=chat%20bot#define-models",target:"_blank",rel:"noopener noreferrer"}},[t("OutboundLink")],1)]),e._v(" "),t("h3",{attrs:{id:"модель-seq2seq"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#модель-seq2seq"}},[e._v("#")]),e._v(" Модель Seq2Seq"),t("a",{attrs:{href:"https://pytorch.org/tutorials/beginner/chatbot_tutorial.html?highlight=chat%20bot#seq2seq-model",target:"_blank",rel:"noopener noreferrer"}},[t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("Мозг нашего чат- бота представляет собой модель «последовательность за последовательностью» (seq2seq). Целью модели seq2seq является получение последовательности переменной длины в качестве входных данных и возврат последовательности переменной длины в качестве выходных данных с использованием модели фиксированного размера.")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://arxiv.org/abs/1409.3215",target:"_blank",rel:"noopener noreferrer"}},[e._v("Суцкевер и др."),t("OutboundLink")],1),e._v(" обнаружил, что, используя вместе две отдельные рекуррентные нейронные сети, мы можем выполнить эту задачу. Один RNN действует как "),t("strong",[e._v("кодировщик")]),e._v(" , который кодирует входную последовательность переменной длины в вектор контекста фиксированной длины. Теоретически этот вектор контекста (последний скрытый слой RNN) будет содержать семантическую информацию о предложении запроса, которое вводится боту . Второй RNN — это "),t("strong",[e._v("декодер")]),e._v(" , который принимает входное слово и вектор контекста и возвращает предположение для следующего слова в последовательности и скрытое состояние для использования в следующей итерации.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://pytorch.org/tutorials/_images/seq2seq_ts.png",alt:"модель"}})]),e._v(" "),t("p",[e._v("Источник изображения: "),t("a",{attrs:{href:"https://jeddy92.github.io/JEddy92.github.io/ts_seq2seq_intro/",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://jeddy92.github.io/JEddy92.github.io/ts_seq2seq_intro/"),t("OutboundLink")],1)]),e._v(" "),t("h3",{attrs:{id:"кодер"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#кодер"}},[e._v("#")]),e._v(" Кодер"),t("a",{attrs:{href:"https://pytorch.org/tutorials/beginner/chatbot_tutorial.html?highlight=chat%20bot#encoder",target:"_blank",rel:"noopener noreferrer"}},[t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("Кодер RNN перебирает входное предложение по одной лексеме (например, слову) за раз, на каждом временном шаге выводя «выходной» вектор и вектор «скрытого состояния». Затем скрытый вектор состояния передается на следующий временной шаг, а выходной вектор записывается. Кодер преобразует контекст, который он видел в каждой точке последовательности, в набор точек в многомерном пространстве, которые декодер будет использовать для создания значимого вывода для данной задачи.")]),e._v(" "),t("p",[e._v("В основе нашего кодировщика лежит многоуровневый рекуррентный блок Gated, изобретенный "),t("a",{attrs:{href:"https://arxiv.org/pdf/1406.1078v3.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Cho et al."),t("OutboundLink")],1),e._v(" в 2014 году. Мы будем использовать двунаправленный вариант ГРУ, а это означает, что по существу есть две независимые RNN: одна, на которую подается входная последовательность в обычном последовательном порядке, и другая, на которую подается входная последовательность в обратном порядке. Выходы каждой сети суммируются на каждом временном шаге. Использование двунаправленного GRU даст нам преимущество кодирования как прошлого, так и будущего контекста.")]),e._v(" "),t("p",[e._v("Двунаправленный РНН:")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://pytorch.org/tutorials/_images/RNN-bidirectional.png",target:"_blank",rel:"noopener noreferrer"}},[t("img",{attrs:{src:"https://pytorch.org/tutorials/_images/RNN-bidirectional.png",alt:"rnn_bidir"}}),t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("Источник изображения: "),t("a",{attrs:{href:"https://colah.github.io/posts/2015-09-NN-Types-FP/",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://colah.github.io/posts/2015-09-NN-Types-FP/"),t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("Обратите внимание, что "),t("code",[e._v("embedding")]),e._v("слой используется для кодирования наших индексов слов в пространстве признаков произвольного размера. Для наших моделей этот слой будет сопоставлять каждое слово с пространством признаков размером "),t("em",[e._v("hidden_size")]),e._v(" . При обучении эти значения должны кодировать семантическое сходство между словами с одинаковым значением.")]),e._v(" "),t("p",[e._v("Наконец, при передаче заполненного пакета последовательностей в модуль RNN мы должны упаковать и распаковать заполнение вокруг прохода RNN, используя "),t("code",[e._v("nn.utils.rnn.pack_padded_sequence")]),e._v("и "),t("code",[e._v("nn.utils.rnn.pad_packed_sequence")]),e._v("соответственно.")]),e._v(" "),t("p",[t("strong",[e._v("Расчетный график:")])]),e._v(" "),t("blockquote",[t("ol",[t("li",[t("p",[e._v("Преобразование индексов слов во вложения.")])]),e._v(" "),t("li",[t("p",[e._v("Упакуйте дополненный пакет последовательностей для модуля RNN.")])]),e._v(" "),t("li",[t("p",[e._v("Вперед пройти через ГРУ.")])]),e._v(" "),t("li",[t("p",[e._v("Распаковать прокладку.")])]),e._v(" "),t("li",[t("p",[e._v("Суммируйте двунаправленные выходы GRU.")])]),e._v(" "),t("li",[t("p",[e._v("Возвращает вывод и окончательное скрытое состояние.")])])])]),e._v(" "),t("p",[t("strong",[e._v("Входы:")])]),e._v(" "),t("ul",[t("li",[t("p",[t("code",[e._v("input_seq")]),e._v(": пакет входных предложений; форма = "),t("em",[e._v("(max_length, размер партии)")])])]),e._v(" "),t("li",[t("p",[t("code",[e._v("input_lengths")]),e._v(": список длин предложений, соответствующих каждому предложению в пакете; форма = "),t("em",[e._v("(размер_пакета)")])])]),e._v(" "),t("li",[t("p",[t("code",[e._v("hidden")]),e._v(": скрытое состояние; shape= "),t("em",[e._v("(n_layers x num_directions, batch_size, hidden_size)")])])])]),e._v(" "),t("p",[t("strong",[e._v("Выходы:")])]),e._v(" "),t("ul",[t("li",[t("p",[t("code",[e._v("outputs")]),e._v(": выходные признаки из последнего скрытого слоя ГРУ (сумма двунаправленных выходов); shape= "),t("em",[e._v("(max_length, batch_size, hidden_size)")])])]),e._v(" "),t("li",[t("p",[t("code",[e._v("hidden")]),e._v(": обновлено скрытое состояние от ГРУ; shape= "),t("em",[e._v("(n_layers x num_directions, batch_size, hidden_size)")])])])]),e._v(" "),t("p",[e._v("class EncoderRNN("),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module",title:"torch.nn.Module",target:"_blank",rel:"noopener noreferrer"}},[e._v("nn.Module"),t("OutboundLink")],1),e._v("):\ndef "),t("strong",[e._v("init")]),e._v("(self, hidden_size, "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding",title:"torch.nn.Embedding",target:"_blank",rel:"noopener noreferrer"}},[e._v("embedding"),t("OutboundLink")],1),e._v(", n_layers=1, dropout=0):\nsuper("),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module",title:"torch.nn.Module",target:"_blank",rel:"noopener noreferrer"}},[e._v("EncoderRNN"),t("OutboundLink")],1),e._v(", self)."),t("strong",[e._v("init")]),e._v("()\nself.n_layers = n_layers\nself.hidden_size = hidden_size\nself."),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding",title:"torch.nn.Embedding",target:"_blank",rel:"noopener noreferrer"}},[e._v("embedding"),t("OutboundLink")],1),e._v(" = "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding",title:"torch.nn.Embedding",target:"_blank",rel:"noopener noreferrer"}},[e._v("embedding"),t("OutboundLink")],1)]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",[t("code",[e._v('    # Initialize GRU; the input_size and hidden_size params are both set to \'hidden_size\'\n    #   because our input size is a word embedding with number of features == hidden_size\n    self.gru = [nn.GRU](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU "torch.nn.GRU")(hidden_size, hidden_size, n_layers,\n                      dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n\ndef forward(self, input_seq, input_lengths, hidden=None):\n    # Convert word indexes to embeddings\n    embedded = self.[embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding "torch.nn.Embedding")(input_seq)\n    # Pack padded batch of sequences for RNN module\n    packed = [nn.utils.rnn.pack_padded_sequence](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html#torch.nn.utils.rnn.pack_padded_sequence "torch.nn.utils.rnn.pack_padded_sequence")(embedded, input_lengths)\n    # Forward pass through GRU\n    outputs, hidden = self.gru(packed, hidden)\n    # Unpack padding\n    outputs, _ = [nn.utils.rnn.pad_packed_sequence](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_packed_sequence.html#torch.nn.utils.rnn.pad_packed_sequence "torch.nn.utils.rnn.pad_packed_sequence")(outputs)\n    # Sum bidirectional GRU outputs\n    outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n    # Return output and final hidden state\n    return outputs, hidden\n')])])]),t("h3",{attrs:{id:"декодер"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#декодер"}},[e._v("#")]),e._v(" Декодер"),t("a",{attrs:{href:"https://pytorch.org/tutorials/beginner/chatbot_tutorial.html?highlight=chat%20bot#decoder",target:"_blank",rel:"noopener noreferrer"}},[t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("Декодер RNN генерирует ответное предложение посимвольно. Он использует векторы контекста кодировщика и внутренние скрытые состояния для генерации следующего слова в последовательности. Он продолжает генерировать слова, пока не выведет "),t("em",[e._v("EOS_token")]),e._v(" , представляющий конец предложения. Общая проблема декодера vanilla seq2seq заключается в том, что если мы полагаемся исключительно на вектор контекста для кодирования значения всей входной последовательности, вполне вероятно, что у нас будет потеря информации. Это особенно актуально при работе с длинными входными последовательностями, что сильно ограничивает возможности нашего декодера.")]),e._v(" "),t("p",[e._v("Для борьбы с этим "),t("a",{attrs:{href:"https://arxiv.org/abs/1409.0473",target:"_blank",rel:"noopener noreferrer"}},[e._v("Bahdanau et al."),t("OutboundLink")],1),e._v(" создал «механизм внимания», который позволяет декодеру обращать внимание на определенные части входной последовательности, а не использовать весь фиксированный контекст на каждом шагу.")]),e._v(" "),t("p",[e._v("На высоком уровне внимание рассчитывается с использованием текущего скрытого состояния декодера и выходных данных кодировщика. Выходные веса внимания имеют ту же форму, что и входная последовательность, что позволяет нам умножать их на выходные данные кодировщика, давая нам взвешенную сумму, которая указывает части выходных данных кодировщика, на которые следует обратить внимание. "),t("a",{attrs:{href:"https://github.com/spro",target:"_blank",rel:"noopener noreferrer"}},[e._v("Рисунок Шона Робертсона"),t("OutboundLink")],1),e._v(" очень хорошо описывает это:")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://pytorch.org/tutorials/_images/attn2.png",alt:"attn2"}})]),e._v(" "),t("p",[t("a",{attrs:{href:"https://arxiv.org/abs/1508.04025",target:"_blank",rel:"noopener noreferrer"}},[e._v("Луонг и др."),t("OutboundLink")],1),e._v(" улучшили основу работы Багданау и др., создав «Всеобщее внимание». Ключевое отличие состоит в том, что при «Глобальном внимании» мы учитываем все скрытые состояния кодировщика, в отличие от «Локального внимания» Бахданау и др., которое рассматривает только скрытое состояние кодировщика с текущего временного шага. Еще одно отличие состоит в том, что с «Глобальным вниманием» мы вычисляем веса внимания или энергии, используя скрытое состояние декодера только из текущего временного шага. Расчет внимания Багданау и др. требует знания состояния декодера на предыдущем временном шаге. Кроме того, Луонг и др. предоставляет различные методы для расчета энергии внимания между выходом кодировщика и выходом декодера, которые называются «функциями оценки»:")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://pytorch.org/tutorials/_images/scores.png",target:"_blank",rel:"noopener noreferrer"}},[t("img",{attrs:{src:"https://pytorch.org/tutorials/_images/scores.png",alt:"баллы"}}),t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("гдечастчаст​= текущее состояние целевого декодера ичасˉсчасˉс​= все состояния энкодера.")]),e._v(" "),t("p",[e._v("В целом механизм глобального внимания можно представить на следующем рисунке. Обратите внимание, что мы будем реализовывать «уровень внимания» как отдельный "),t("code",[e._v("nn.Module")]),e._v("файл с именем "),t("code",[e._v("Attn")]),e._v(". Результатом работы этого модуля является нормализованный тензор весов softmax формы "),t("em",[e._v("(batch_size, 1, max_length)")]),e._v(" .")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://pytorch.org/tutorials/_images/global_attn.png",target:"_blank",rel:"noopener noreferrer"}},[t("img",{attrs:{src:"https://pytorch.org/tutorials/_images/global_attn.png",alt:"global_attn"}}),t("OutboundLink")],1)]),e._v(" "),t("h1",{attrs:{id:"luong-attention-layer"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#luong-attention-layer"}},[e._v("#")]),e._v(" Luong attention layer")]),e._v(" "),t("p",[e._v("class Attn("),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module",title:"torch.nn.Module",target:"_blank",rel:"noopener noreferrer"}},[e._v("nn.Module"),t("OutboundLink")],1),e._v("):\ndef "),t("strong",[e._v("init")]),e._v("(self, method, hidden_size):\nsuper("),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module",title:"torch.nn.Module",target:"_blank",rel:"noopener noreferrer"}},[e._v("Attn"),t("OutboundLink")],1),e._v(", self)."),t("strong",[e._v("init")]),e._v("()\nself.method = method\nif self.method not in ['dot', 'general', 'concat']:\nraise ValueError(self.method, \"is not an appropriate attention method.\")\nself.hidden_size = hidden_size\nif self.method == 'general':\nself.attn = "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear",title:"torch.nn.Linear",target:"_blank",rel:"noopener noreferrer"}},[e._v("nn.Linear"),t("OutboundLink")],1),e._v("(self.hidden_size, hidden_size)\nelif self.method == 'concat':\nself.attn = "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear",title:"torch.nn.Linear",target:"_blank",rel:"noopener noreferrer"}},[e._v("nn.Linear"),t("OutboundLink")],1),e._v("(self.hidden_size * 2, hidden_size)\nself.v = "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter",title:"torch.nn.parameter.Parameter",target:"_blank",rel:"noopener noreferrer"}},[e._v("nn.Parameter"),t("OutboundLink")],1),e._v("(torch.FloatTensor(hidden_size))")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",[t("code",[e._v('def dot_score(self, hidden, encoder_output):\n    return [torch.sum](https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum "torch.sum")(hidden * encoder_output, dim=2)\n\ndef general_score(self, hidden, encoder_output):\n    energy = self.attn(encoder_output)\n    return [torch.sum](https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum "torch.sum")(hidden * energy, dim=2)\n\ndef concat_score(self, hidden, encoder_output):\n    energy = self.attn([torch.cat](https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat "torch.cat")((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n    return [torch.sum](https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum "torch.sum")(self.v * energy, dim=2)\n\ndef forward(self, hidden, encoder_outputs):\n    # Calculate the attention weights (energies) based on the given method\n    if self.method == \'general\':\n        attn_energies = self.general_score(hidden, encoder_outputs)\n    elif self.method == \'concat\':\n        attn_energies = self.concat_score(hidden, encoder_outputs)\n    elif self.method == \'dot\':\n        attn_energies = self.dot_score(hidden, encoder_outputs)\n\n    # Transpose max_length and batch_size dimensions\n    attn_energies = attn_energies.t()\n\n    # Return the softmax normalized probability scores (with added dimension)\n    return [F.softmax](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html#torch.nn.functional.softmax "torch.nn.functional.softmax")(attn_energies, dim=1).unsqueeze(1)\n')])])]),t("p",[e._v("Теперь, когда мы определили наш подмодуль внимания, мы можем реализовать реальную модель декодера. Для декодера мы будем вручную загружать нашу партию по одному временному шагу за раз. Это означает, что наш встроенный тензор слов и выходные данные GRU будут иметь форму "),t("em",[e._v("(1, batch_size, hidden_size)")]),e._v(" .")]),e._v(" "),t("p",[t("strong",[e._v("Расчетный график:")])]),e._v(" "),t("blockquote",[t("ol",[t("li",[t("p",[e._v("Получить вложение текущего входного слова.")])]),e._v(" "),t("li",[t("p",[e._v("Вперед через однонаправленное ГРУ.")])]),e._v(" "),t("li",[t("p",[e._v("Рассчитайте веса внимания по текущим выходным данным ГРУ из (2).")])]),e._v(" "),t("li",[t("p",[e._v("Умножьте веса внимания на выходные данные кодировщика, чтобы получить новый вектор контекста «взвешенной суммы».")])]),e._v(" "),t("li",[t("p",[e._v("Объедините взвешенный вектор контекста и выходные данные GRU, используя уравнение Луонга. 5.")])]),e._v(" "),t("li",[t("p",[e._v("Предсказать следующее слово, используя уравнение Луонга. 6 (без софтмакс).")])]),e._v(" "),t("li",[t("p",[e._v("Возвращает вывод и окончательное скрытое состояние.")])])])]),e._v(" "),t("p",[t("strong",[e._v("Входы:")])]),e._v(" "),t("ul",[t("li",[t("p",[t("code",[e._v("input_step")]),e._v(": один временной шаг (одно слово) пакета входной последовательности; форма = "),t("em",[e._v("(1, размер_пакета)")])])]),e._v(" "),t("li",[t("p",[t("code",[e._v("last_hidden")]),e._v(": последний скрытый слой ГРУ; shape= "),t("em",[e._v("(n_layers x num_directions, batch_size, hidden_size)")])])]),e._v(" "),t("li",[t("p",[t("code",[e._v("encoder_outputs")]),e._v(": вывод модели энкодера; shape= "),t("em",[e._v("(max_length, batch_size, hidden_size)")])])])]),e._v(" "),t("p",[t("strong",[e._v("Выходы:")])]),e._v(" "),t("ul",[t("li",[t("p",[t("code",[e._v("output")]),e._v(": нормализованный тензор softmax, дающий вероятности того, что каждое слово является правильным следующим словом в декодированной последовательности; shape= "),t("em",[e._v("(batch_size, voc.num_words)")])])]),e._v(" "),t("li",[t("p",[t("code",[e._v("hidden")]),e._v(": окончательное скрытое состояние ГРУ; shape= "),t("em",[e._v("(n_layers x num_directions, batch_size, hidden_size)")])])])]),e._v(" "),t("p",[e._v("class LuongAttnDecoderRNN("),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module",title:"torch.nn.Module",target:"_blank",rel:"noopener noreferrer"}},[e._v("nn.Module"),t("OutboundLink")],1),e._v("):\ndef "),t("strong",[e._v("init")]),e._v("(self, attn_model, "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding",title:"torch.nn.Embedding",target:"_blank",rel:"noopener noreferrer"}},[e._v("embedding"),t("OutboundLink")],1),e._v(", hidden_size, output_size, n_layers=1, dropout=0.1):\nsuper("),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module",title:"torch.nn.Module",target:"_blank",rel:"noopener noreferrer"}},[e._v("LuongAttnDecoderRNN"),t("OutboundLink")],1),e._v(", self)."),t("strong",[e._v("init")]),e._v("()")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",[t("code",[e._v('    # Keep for reference\n    self.attn_model = attn_model\n    self.hidden_size = hidden_size\n    self.output_size = output_size\n    self.n_layers = n_layers\n    self.dropout = dropout\n\n    # Define layers\n    self.[embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding "torch.nn.Embedding") = [embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding "torch.nn.Embedding")\n    self.embedding_dropout = [nn.Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout "torch.nn.Dropout")(dropout)\n    self.gru = [nn.GRU](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU "torch.nn.GRU")(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n    self.concat = [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear "torch.nn.Linear")(hidden_size * 2, hidden_size)\n    self.out = [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear "torch.nn.Linear")(hidden_size, output_size)\n\n    self.attn = [Attn](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module "torch.nn.Module")(attn_model, hidden_size)\n\ndef forward(self, input_step, last_hidden, encoder_outputs):\n    # Note: we run this one step (word) at a time\n    # Get embedding of current input word\n    embedded = self.[embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding "torch.nn.Embedding")(input_step)\n    embedded = self.embedding_dropout(embedded)\n    # Forward through unidirectional GRU\n    rnn_output, hidden = self.gru(embedded, last_hidden)\n    # Calculate attention weights from the current GRU output\n    attn_weights = self.attn(rnn_output, encoder_outputs)\n    # Multiply attention weights to encoder outputs to get new "weighted sum" context vector\n    context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n    # Concatenate weighted context vector and GRU output using Luong eq. 5\n    rnn_output = rnn_output.squeeze(0)\n    context = context.squeeze(1)\n    concat_input = [torch.cat](https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat "torch.cat")((rnn_output, context), 1)\n    concat_output = [torch.tanh](https://pytorch.org/docs/stable/generated/torch.tanh.html#torch.tanh "torch.tanh")(self.concat(concat_input))\n    # Predict next word using Luong eq. 6\n    output = self.out(concat_output)\n    output = [F.softmax](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html#torch.nn.functional.softmax "torch.nn.functional.softmax")(output, dim=1)\n    # Return output and final hidden state\n    return output, hidden\n')])])]),t("h2",{attrs:{id:"определить-процедуру-обучения"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#определить-процедуру-обучения"}},[e._v("#")]),e._v(" Определить процедуру обучения"),t("a",{attrs:{href:"https://pytorch.org/tutorials/beginner/chatbot_tutorial.html?highlight=chat%20bot#define-training-procedure",target:"_blank",rel:"noopener noreferrer"}},[t("OutboundLink")],1)]),e._v(" "),t("h3",{attrs:{id:"замаскированная-потеря"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#замаскированная-потеря"}},[e._v("#")]),e._v(" Замаскированная потеря"),t("a",{attrs:{href:"https://pytorch.org/tutorials/beginner/chatbot_tutorial.html?highlight=chat%20bot#masked-loss",target:"_blank",rel:"noopener noreferrer"}},[t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("Поскольку мы имеем дело с пакетами дополненных последовательностей, мы не можем просто учитывать все элементы тензора при вычислении потерь. Мы определяем "),t("code",[e._v("maskNLLLoss")]),e._v("расчет наших потерь на основе выходного тензора нашего декодера, целевого тензора и тензора бинарной маски, описывающего заполнение целевого тензора. Эта функция потерь вычисляет среднее отрицательное логарифмическое правдоподобие элементов, которые соответствуют "),t("em",[e._v("1")]),e._v(" в тензоре маски.")]),e._v(" "),t("p",[e._v("def maskNLLLoss(inp, target, "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/tensors.html#torch.Tensor",title:"torch.Tensor",target:"_blank",rel:"noopener noreferrer"}},[e._v("mask"),t("OutboundLink")],1),e._v("):\nnTotal = "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/tensors.html#torch.Tensor",title:"torch.Tensor",target:"_blank",rel:"noopener noreferrer"}},[e._v("mask"),t("OutboundLink")],1),e._v(".sum()\ncrossEntropy = -"),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.log.html#torch.log",title:"torch.log",target:"_blank",rel:"noopener noreferrer"}},[e._v("torch.log"),t("OutboundLink")],1),e._v("("),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.gather.html#torch.gather",title:"torch.gather",target:"_blank",rel:"noopener noreferrer"}},[e._v("torch.gather"),t("OutboundLink")],1),e._v("(inp, 1, target.view(-1, 1)).squeeze(1))\nloss = crossEntropy.masked_select("),t("a",{attrs:{href:"https://pytorch.org/docs/stable/tensors.html#torch.Tensor",title:"torch.Tensor",target:"_blank",rel:"noopener noreferrer"}},[e._v("mask"),t("OutboundLink")],1),e._v(").mean()\nloss = loss.to("),t("a",{attrs:{href:"https://pytorch.org/docs/stable/tensor_attributes.html#torch.device",title:"torch.device",target:"_blank",rel:"noopener noreferrer"}},[e._v("device"),t("OutboundLink")],1),e._v(")\nreturn loss, nTotal.item()")]),e._v(" "),t("h3",{attrs:{id:"одна-итерация-обучения"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#одна-итерация-обучения"}},[e._v("#")]),e._v(" Одна итерация обучения"),t("a",{attrs:{href:"https://pytorch.org/tutorials/beginner/chatbot_tutorial.html?highlight=chat%20bot#single-training-iteration",target:"_blank",rel:"noopener noreferrer"}},[t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("Функция "),t("code",[e._v("train")]),e._v("содержит алгоритм для одной итерации обучения (один пакет входных данных).")]),e._v(" "),t("p",[e._v("Мы воспользуемся парой хитрых приемов, чтобы помочь в сходимости:")]),e._v(" "),t("ul",[t("li",[t("p",[e._v("Первый прием заключается в использовании "),t("strong",[e._v("принуждения учителя")]),e._v(" . Это означает, что с некоторой вероятностью, установленной "),t("code",[e._v("teacher_forcing_ratio")]),e._v(", мы используем текущее целевое слово в качестве следующего ввода декодера, а не используем текущее предположение декодера. Этот метод действует как тренировочные колеса для декодера, помогая более эффективному обучению. Однако принуждение учителя может привести к нестабильности модели во время вывода, поскольку у декодера может не быть достаточного шанса действительно создать свои собственные выходные последовательности во время обучения. Таким образом, мы должны помнить о том, как мы устанавливаем "),t("code",[e._v("teacher_forcing_ratio")]),e._v(", и не обманываться быстрой сходимостью.")])]),e._v(" "),t("li",[t("p",[e._v("Второй прием, который мы реализуем, — "),t("strong",[e._v("отсечение градиента")]),e._v(" . Это широко используемый метод для борьбы с проблемой «взрывающегося градиента». По сути, обрезав или установив пороговое значение градиентов до максимального значения, мы предотвращаем экспоненциальный рост градиентов и либо переполнение (NaN), либо превышение крутых обрывов в функции стоимости.")])])]),e._v(" "),t("p",[t("a",{attrs:{href:"https://pytorch.org/tutorials/_images/grad_clip.png",target:"_blank",rel:"noopener noreferrer"}},[t("img",{attrs:{src:"https://pytorch.org/tutorials/_images/grad_clip.png",alt:"grad_clip"}}),t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("Источник изображения: Goodfellow et al. "),t("em",[e._v("Глубокое обучение")]),e._v(" . 2016. "),t("a",{attrs:{href:"https://www.deeplearningbook.org/",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://www.deeplearningbook.org/"),t("OutboundLink")],1)]),e._v(" "),t("p",[t("strong",[e._v("Последовательность операций:")])]),e._v(" "),t("blockquote",[t("ol",[t("li",[t("p",[e._v("Пропустить всю входную партию через энкодер.")])]),e._v(" "),t("li",[t("p",[e._v("Инициализируйте входные данные декодера как SOS_token, а скрытое состояние — как окончательное скрытое состояние кодировщика.")])]),e._v(" "),t("li",[t("p",[e._v("Пересылать последовательность входных пакетов через декодер по одному временному шагу за раз.")])]),e._v(" "),t("li",[t("p",[e._v("Если учитель принудительно: установите следующий вход декодера в качестве текущей цели; иначе: установить следующий вход декодера как текущий выход декодера.")])]),e._v(" "),t("li",[t("p",[e._v("Подсчитывайте и накапливайте убытки.")])]),e._v(" "),t("li",[t("p",[e._v("Выполните обратное распространение.")])]),e._v(" "),t("li",[t("p",[e._v("Клип градиенты.")])]),e._v(" "),t("li",[t("p",[e._v("Обновите параметры модели кодировщика и декодера.")])])])]),e._v(" "),t("p",[e._v("ПРИМЕЧАНИЕ")]),e._v(" "),t("p",[e._v("Модули PyTorch RNN ( "),t("code",[e._v("RNN")]),e._v(", "),t("code",[e._v("LSTM")]),e._v(", "),t("code",[e._v("GRU")]),e._v(") можно использовать как любые другие неповторяющиеся слои, просто передав им всю входную последовательность (или пакет последовательностей). Мы используем "),t("code",[e._v("GRU")]),e._v("такой слой в файле "),t("code",[e._v("encoder")]),e._v(". Реальность такова, что под капотом существует итеративный процесс, зацикливающийся на каждом временном шаге и вычисляющий скрытые состояния. Кроме того, вы можете запускать эти модули по одному временному шагу за раз. В этом случае мы вручную перебираем последовательности в процессе обучения, как мы должны делать для модели "),t("code",[e._v("decoder")]),e._v(". Пока вы поддерживаете правильную концептуальную модель этих модулей, реализация последовательных моделей может быть очень простой.")]),e._v(" "),t("p",[e._v("def train("),t("a",{attrs:{href:"https://pytorch.org/docs/stable/tensors.html#torch.Tensor",title:"torch.Tensor",target:"_blank",rel:"noopener noreferrer"}},[e._v("input_variable"),t("OutboundLink")],1),e._v(", "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/tensors.html#torch.Tensor",title:"torch.Tensor",target:"_blank",rel:"noopener noreferrer"}},[e._v("lengths"),t("OutboundLink")],1),e._v(", "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/tensors.html#torch.Tensor",title:"torch.Tensor",target:"_blank",rel:"noopener noreferrer"}},[e._v("target_variable"),t("OutboundLink")],1),e._v(", "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/tensors.html#torch.Tensor",title:"torch.Tensor",target:"_blank",rel:"noopener noreferrer"}},[e._v("mask"),t("OutboundLink")],1),e._v(", max_target_len, encoder, decoder, "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding",title:"torch.nn.Embedding",target:"_blank",rel:"noopener noreferrer"}},[e._v("embedding"),t("OutboundLink")],1),e._v(",\n"),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam",title:"torch.optim.Adam",target:"_blank",rel:"noopener noreferrer"}},[e._v("encoder_optimizer"),t("OutboundLink")],1),e._v(", "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam",title:"torch.optim.Adam",target:"_blank",rel:"noopener noreferrer"}},[e._v("decoder_optimizer"),t("OutboundLink")],1),e._v(", batch_size, clip, max_length=MAX_LENGTH):")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",[t("code",[e._v('# Zero gradients\n[encoder_optimizer.zero_grad](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam.zero_grad "torch.optim.Adam.zero_grad")()\n[decoder_optimizer.zero_grad](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam.zero_grad "torch.optim.Adam.zero_grad")()\n\n# Set device options\n[input_variable](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [input_variable](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device "torch.device"))\n[target_variable](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [target_variable](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device "torch.device"))\n[mask](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [mask](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device "torch.device"))\n# Lengths for rnn packing should always be on the cpu\n[lengths](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor") = [lengths](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor").to("cpu")\n\n# Initialize variables\nloss = 0\nprint_losses = []\nn_totals = 0\n\n# Forward pass through encoder\nencoder_outputs, encoder_hidden = encoder([input_variable](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), [lengths](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"))\n\n# Create initial decoder input (start with SOS tokens for each sentence)\ndecoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\ndecoder_input = decoder_input.to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device "torch.device"))\n\n# Set initial decoder hidden state to the encoder\'s final hidden state\ndecoder_hidden = encoder_hidden[:decoder.n_layers]\n\n# Determine if we are using teacher forcing this iteration\nuse_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n\n# Forward batch of sequences through decoder one time step at a time\nif use_teacher_forcing:\n    for t in range(max_target_len):\n        decoder_output, decoder_hidden = decoder(\n            decoder_input, decoder_hidden, encoder_outputs\n        )\n        # Teacher forcing: next input is current target\n        decoder_input = [target_variable](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")[t].view(1, -1)\n        # Calculate and accumulate loss\n        mask_loss, nTotal = maskNLLLoss(decoder_output, [target_variable](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")[t], [mask](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")[t])\n        loss += mask_loss\n        print_losses.append(mask_loss.item() * nTotal)\n        n_totals += nTotal\nelse:\n    for t in range(max_target_len):\n        decoder_output, decoder_hidden = decoder(\n            decoder_input, decoder_hidden, encoder_outputs\n        )\n        # No teacher forcing: next input is decoder\'s own current output\n        _, topi = decoder_output.topk(1)\n        decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n        decoder_input = decoder_input.to([device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device "torch.device"))\n        # Calculate and accumulate loss\n        mask_loss, nTotal = maskNLLLoss(decoder_output, [target_variable](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")[t], [mask](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor")[t])\n        loss += mask_loss\n        print_losses.append(mask_loss.item() * nTotal)\n        n_totals += nTotal\n\n# Perform backpropatation\nloss.backward()\n\n# Clip gradients: gradients are modified in place\n_ = [nn.utils.clip_grad_norm_](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_ "torch.nn.utils.clip_grad_norm_")([encoder.parameters](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters "torch.nn.Module.parameters")(), clip)\n_ = [nn.utils.clip_grad_norm_](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_ "torch.nn.utils.clip_grad_norm_")([decoder.parameters](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters "torch.nn.Module.parameters")(), clip)\n\n# Adjust model weights\n[encoder_optimizer.step](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.step.html#torch.optim.Optimizer.step "torch.optim.Optimizer.step")()\n[decoder_optimizer.step](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.step.html#torch.optim.Optimizer.step "torch.optim.Optimizer.step")()\n\nreturn sum(print_losses) / n_totals\n')])])]),t("h3",{attrs:{id:"итерации-обучения"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#итерации-обучения"}},[e._v("#")]),e._v(" Итерации обучения"),t("a",{attrs:{href:"https://pytorch.org/tutorials/beginner/chatbot_tutorial.html?highlight=chat%20bot#training-iterations",target:"_blank",rel:"noopener noreferrer"}},[t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("Наконец пришло время связать полную процедуру обучения с данными. Функция "),t("code",[e._v("trainIters")]),e._v("отвечает за запуск "),t("code",[e._v("n_iterations")]),e._v("обучения с учетом переданных моделей, оптимизаторов, данных и т. д. Эта функция не требует пояснений, поскольку мы проделали тяжелую работу с помощью этой "),t("code",[e._v("train")]),e._v("функции.")]),e._v(" "),t("p",[e._v("Следует отметить, что когда мы сохраняем нашу модель, мы сохраняем tar-архив, содержащий state_dicts (параметры) кодировщика и декодера, state_dicts оптимизаторов, потери, итерации и т. д. Сохранение модели таким образом даст нам окончательный результат. гибкость с контрольно-пропускным пунктом. После загрузки контрольной точки мы сможем использовать параметры модели для выполнения логического вывода или продолжить обучение с того места, на котором остановились.")]),e._v(" "),t("p",[e._v("def trainIters(model_name, voc, pairs, encoder, decoder, "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam",title:"torch.optim.Adam",target:"_blank",rel:"noopener noreferrer"}},[e._v("encoder_optimizer"),t("OutboundLink")],1),e._v(", "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam",title:"torch.optim.Adam",target:"_blank",rel:"noopener noreferrer"}},[e._v("decoder_optimizer"),t("OutboundLink")],1),e._v(", "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding",title:"torch.nn.Embedding",target:"_blank",rel:"noopener noreferrer"}},[e._v("embedding"),t("OutboundLink")],1),e._v(", encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",[t("code",[e._v('# Load batches for each iteration\ntraining_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n                  for _ in range(n_iteration)]\n\n# Initializations\nprint(\'Initializing ...\')\nstart_iteration = 1\nprint_loss = 0\nif loadFilename:\n    start_iteration = checkpoint[\'iteration\'] + 1\n\n# Training loop\nprint("Training...")\nfor iteration in range(start_iteration, n_iteration + 1):\n    training_batch = training_batches[iteration - 1]\n    # Extract fields from batch\n    [input_variable](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), [lengths](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), [target_variable](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), [mask](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), max_target_len = training_batch\n\n    # Run a training iteration with batch\n    loss = train([input_variable](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), [lengths](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), [target_variable](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), [mask](https://pytorch.org/docs/stable/tensors.html#torch.Tensor "torch.Tensor"), max_target_len, encoder,\n                 decoder, [embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding "torch.nn.Embedding"), [encoder_optimizer](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam "torch.optim.Adam"), [decoder_optimizer](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam "torch.optim.Adam"), batch_size, clip)\n    print_loss += loss\n\n    # Print progress\n    if iteration % print_every == 0:\n        print_loss_avg = print_loss / print_every\n        print("Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n        print_loss = 0\n\n    # Save checkpoint\n    if (iteration % save_every == 0):\n        directory = os.path.join(save_dir, model_name, corpus_name, \'{}-{}_{}\'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n        if not os.path.exists(directory):\n            os.makedirs(directory)\n        [torch.save](https://pytorch.org/docs/stable/generated/torch.save.html#torch.save "torch.save")({\n            \'iteration\': iteration,\n            \'en\': [encoder.state_dict](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict "torch.nn.Module.state_dict")(),\n            \'de\': [decoder.state_dict](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict "torch.nn.Module.state_dict")(),\n            \'en_opt\': [encoder_optimizer.state_dict](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam.state_dict "torch.optim.Adam.state_dict")(),\n            \'de_opt\': [decoder_optimizer.state_dict](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam.state_dict "torch.optim.Adam.state_dict")(),\n            \'loss\': loss,\n            \'voc_dict\': voc.__dict__,\n            \'embedding\': [embedding.state_dict](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict "torch.nn.Module.state_dict")()\n        }, os.path.join(directory, \'{}_{}.tar\'.format(iteration, \'checkpoint\')))\n')])])]),t("h2",{attrs:{id:"определить-оценку"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#определить-оценку"}},[e._v("#")]),e._v(" Определить оценку"),t("a",{attrs:{href:"https://pytorch.org/tutorials/beginner/chatbot_tutorial.html?highlight=chat%20bot#define-evaluation",target:"_blank",rel:"noopener noreferrer"}},[t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("После обучения модели мы хотим сами разговаривать с ботом . Во-первых, мы должны определить, как мы хотим, чтобы модель декодировала закодированный ввод.")]),e._v(" "),t("h3",{attrs:{id:"жадное-декодирование"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#жадное-декодирование"}},[e._v("#")]),e._v(" Жадное декодирование"),t("a",{attrs:{href:"https://pytorch.org/tutorials/beginner/chatbot_tutorial.html?highlight=chat%20bot#greedy-decoding",target:"_blank",rel:"noopener noreferrer"}},[t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("Жадное декодирование — это метод декодирования, который мы используем во время обучения, когда мы "),t("strong",[e._v("НЕ")]),e._v(" используем принуждение учителя. Другими словами, для каждого временного шага мы просто выбираем слово из "),t("code",[e._v("decoder_output")]),e._v("с наибольшим значением softmax. Этот метод декодирования оптимален на уровне одного временного шага.")]),e._v(" "),t("p",[e._v("Чтобы облегчить операцию жадного декодирования, мы определяем "),t("code",[e._v("GreedySearchDecoder")]),e._v("класс. При запуске объект этого класса принимает входную последовательность ( "),t("code",[e._v("input_seq")]),e._v(") формы "),t("em",[e._v("(input_seq length, 1)")]),e._v(" , скалярный "),t("code",[e._v("input_length")]),e._v("тензор входной длины ( ) и a "),t("code",[e._v("max_length")]),e._v("для ограничения длины предложения ответа. Входное предложение оценивается с использованием следующего вычислительного графа:")]),e._v(" "),t("p",[t("strong",[e._v("Расчетный график:")])]),e._v(" "),t("blockquote",[t("ol",[t("li",[t("p",[e._v("Прямой ввод через модель кодировщика.")])]),e._v(" "),t("li",[t("p",[e._v("Подготовьте последний скрытый слой кодировщика, который будет первым скрытым входом в декодер.")])]),e._v(" "),t("li",[t("p",[e._v("Инициализируйте первый ввод декодера как SOS_token.")])]),e._v(" "),t("li",[t("p",[e._v("Инициализируйте тензоры, чтобы добавить к ним декодированные слова.")])]),e._v(" "),t("li",[t("p",[e._v("Итеративно декодировать токен по одному слову за раз:")]),e._v(" "),t("ol",[t("li",[t("p",[e._v("Прямой проход через декодер.")])]),e._v(" "),t("li",[t("p",[e._v("Получите токен наиболее вероятного слова и его оценку softmax.")])]),e._v(" "),t("li",[t("p",[e._v("Запишите жетон и счет.")])]),e._v(" "),t("li",[t("p",[e._v("Подготовьте текущий токен к следующему вводу декодера.")])])])]),e._v(" "),t("li",[t("p",[e._v("Возврат коллекций токенов слов и оценок.")])])])]),e._v(" "),t("p",[e._v("class GreedySearchDecoder("),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module",title:"torch.nn.Module",target:"_blank",rel:"noopener noreferrer"}},[e._v("nn.Module"),t("OutboundLink")],1),e._v("):\ndef "),t("strong",[e._v("init")]),e._v("(self, encoder, decoder):\nsuper("),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module",title:"torch.nn.Module",target:"_blank",rel:"noopener noreferrer"}},[e._v("GreedySearchDecoder"),t("OutboundLink")],1),e._v(", self)."),t("strong",[e._v("init")]),e._v("()\nself.encoder = encoder\nself.decoder = decoder")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",[t("code",[e._v('def forward(self, input_seq, input_length, max_length):\n    # Forward input through encoder model\n    encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n    # Prepare encoder\'s final hidden layer to be first hidden input to the decoder\n    decoder_hidden = encoder_hidden[:decoder.n_layers]\n    # Initialize decoder input with SOS_token\n    decoder_input = [torch.ones](https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones "torch.ones")(1, 1, [device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device "torch.device")=[device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device "torch.device"), dtype=[torch.long](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype "torch.dtype")) * SOS_token\n    # Initialize tensors to append decoded words to\n    all_tokens = [torch.zeros](https://pytorch.org/docs/stable/generated/torch.zeros.html#torch.zeros "torch.zeros")([0], [device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device "torch.device")=[device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device "torch.device"), dtype=[torch.long](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype "torch.dtype"))\n    all_scores = [torch.zeros](https://pytorch.org/docs/stable/generated/torch.zeros.html#torch.zeros "torch.zeros")([0], [device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device "torch.device")=[device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device "torch.device"))\n    # Iteratively decode one word token at a time\n    for _ in range(max_length):\n        # Forward pass through decoder\n        decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n        # Obtain most likely word token and its softmax score\n        decoder_scores, decoder_input = [torch.max](https://pytorch.org/docs/stable/generated/torch.max.html#torch.max "torch.max")(decoder_output, dim=1)\n        # Record token and score\n        all_tokens = [torch.cat](https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat "torch.cat")((all_tokens, decoder_input), dim=0)\n        all_scores = [torch.cat](https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat "torch.cat")((all_scores, decoder_scores), dim=0)\n        # Prepare current token to be next decoder input (add a dimension)\n        decoder_input = [torch.unsqueeze](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html#torch.unsqueeze "torch.unsqueeze")(decoder_input, 0)\n    # Return collections of word tokens and scores\n    return all_tokens, all_scores\n')])])]),t("h3",{attrs:{id:"оцените-мои-текст"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#оцените-мои-текст"}},[e._v("#")]),e._v(" Оцените мой текст"),t("a",{attrs:{href:"https://pytorch.org/tutorials/beginner/chatbot_tutorial.html?highlight=chat%20bot#evaluate-my-text",target:"_blank",rel:"noopener noreferrer"}},[t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("Теперь, когда мы определили наш метод декодирования, мы можем написать функции для оценки строкового входного предложения. Функция "),t("code",[e._v("evaluate")]),e._v("управляет низкоуровневым процессом обработки входного предложения. Сначала мы форматируем предложение как входной пакет индексов слов с "),t("em",[e._v("batch_size==1")]),e._v(" . Мы делаем это, преобразовывая слова предложения в их соответствующие индексы и транспонируя измерения, чтобы подготовить тензор для наших моделей. Мы также создаем "),t("code",[e._v("lengths")]),e._v("тензор, который содержит длину нашего входного предложения. В данном случае "),t("code",[e._v("lengths")]),e._v("это скаляр, потому что мы оцениваем только одно предложение за раз (batch_size==1). Затем мы получаем тензор декодированного ответного предложения, используя наш "),t("code",[e._v("GreedySearchDecoder")]),e._v(" объект ( "),t("code",[e._v("searcher")]),e._v("). Наконец, мы конвертируем индексы ответа в слова и возвращаем список декодированных слов.")]),e._v(" "),t("p",[t("code",[e._v("evaluateInput")]),e._v("действует как пользовательский интерфейс для нашего чат- бота . При вызове появится текстовое поле ввода, в котором мы можем ввести наше предложение запроса. После ввода нашего входного предложения и нажатия "),t("em",[e._v("Enter")]),e._v(" наш текст нормализуется так же, как и наши обучающие данные, и в конечном итоге передается функции "),t("code",[e._v("evaluate")]),e._v("для получения декодированного выходного предложения. Мы зацикливаем этот процесс, поэтому мы можем продолжать общаться с нашим ботом, пока не введем либо «q», либо «quit».")]),e._v(" "),t("p",[e._v("Наконец, если вводится предложение, содержащее слово, которого нет в словаре, мы изящно обрабатываем это, печатая сообщение об ошибке и предлагая пользователю ввести другое предложение.")]),e._v(" "),t("p",[e._v("def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n### Format input sentence as a batch\n# words -> indexes\nindexes_batch = [indexesFromSentence(voc, sentence)]\n# Create lengths tensor\n"),t("a",{attrs:{href:"https://pytorch.org/docs/stable/tensors.html#torch.Tensor",title:"torch.Tensor",target:"_blank",rel:"noopener noreferrer"}},[e._v("lengths"),t("OutboundLink")],1),e._v(" = "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor",title:"torch.tensor",target:"_blank",rel:"noopener noreferrer"}},[e._v("torch.tensor"),t("OutboundLink")],1),e._v("([len(indexes) for indexes in indexes_batch])\n# Transpose dimensions of batch to match models' expectations\ninput_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n# Use appropriate device\ninput_batch = input_batch.to("),t("a",{attrs:{href:"https://pytorch.org/docs/stable/tensor_attributes.html#torch.device",title:"torch.device",target:"_blank",rel:"noopener noreferrer"}},[e._v("device"),t("OutboundLink")],1),e._v(")\n"),t("a",{attrs:{href:"https://pytorch.org/docs/stable/tensors.html#torch.Tensor",title:"torch.Tensor",target:"_blank",rel:"noopener noreferrer"}},[e._v("lengths"),t("OutboundLink")],1),e._v(" = "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/tensors.html#torch.Tensor",title:"torch.Tensor",target:"_blank",rel:"noopener noreferrer"}},[e._v("lengths"),t("OutboundLink")],1),e._v('.to("cpu")\n# Decode sentence with searcher\ntokens, scores = searcher(input_batch, '),t("a",{attrs:{href:"https://pytorch.org/docs/stable/tensors.html#torch.Tensor",title:"torch.Tensor",target:"_blank",rel:"noopener noreferrer"}},[e._v("lengths"),t("OutboundLink")],1),e._v(", max_length)\n# indexes -> words\ndecoded_words = [voc.index2word[token.item()] for token in tokens]\nreturn decoded_words")]),e._v(" "),t("p",[e._v("def evaluateInput(encoder, decoder, searcher, voc):\ninput_sentence = ''\nwhile(1):\ntry:\n# Get input sentence\ninput_sentence = input('> ')\n# Check if it is quit case\nif input_sentence == 'q' or input_sentence == 'quit': break\n# Normalize sentence\ninput_sentence = normalizeString(input_sentence)\n# Evaluate sentence\noutput_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n# Format and print response sentence\noutput_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\nprint('Bot:', ' '.join(output_words))")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",[t("code",[e._v('    except KeyError:\n        print("Error: Encountered unknown word.")\n')])])]),t("h2",{attrs:{id:"запустить-модель"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#запустить-модель"}},[e._v("#")]),e._v(" Запустить модель"),t("a",{attrs:{href:"https://pytorch.org/tutorials/beginner/chatbot_tutorial.html?highlight=chat%20bot#run-model",target:"_blank",rel:"noopener noreferrer"}},[t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("Наконец, пришло время запустить нашу модель!")]),e._v(" "),t("p",[e._v("Независимо от того, хотим ли мы обучать или тестировать модель чат- бота , мы должны инициализировать отдельные модели кодировщика и декодера. В следующем блоке мы устанавливаем желаемые конфигурации, выбираем запуск с нуля или устанавливаем контрольную точку для загрузки, а также строим и инициализируем модели. Не стесняйтесь экспериментировать с различными конфигурациями моделей для оптимизации производительности.")]),e._v(" "),t("h1",{attrs:{id:"configure-models"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#configure-models"}},[e._v("#")]),e._v(" Configure models")]),e._v(" "),t("p",[e._v("model_name = 'cb_model'\nattn_model = 'dot'\n#attn_model = 'general'\n#attn_model = 'concat'\nhidden_size = 500\nencoder_n_layers = 2\ndecoder_n_layers = 2\ndropout = 0.1\nbatch_size = 64")]),e._v(" "),t("h1",{attrs:{id:"set-checkpoint-to-load-from-set-to-none-if-starting-from-scratch"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#set-checkpoint-to-load-from-set-to-none-if-starting-from-scratch"}},[e._v("#")]),e._v(" Set checkpoint to load from; set to None if starting from scratch")]),e._v(" "),t("p",[e._v("loadFilename = None\ncheckpoint_iter = 4000\n#loadFilename = os.path.join(save_dir, model_name, corpus_name,")]),e._v(" "),t("h1",{attrs:{id:"format-encoder-n-layers-decoder-n-layers-hidden-size"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#format-encoder-n-layers-decoder-n-layers-hidden-size"}},[e._v("#")]),e._v(" '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),")]),e._v(" "),t("h1",{attrs:{id:"checkpoint-tar-format-checkpoint-iter"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#checkpoint-tar-format-checkpoint-iter"}},[e._v("#")]),e._v(" '{}_checkpoint.tar'.format(checkpoint_iter))")]),e._v(" "),t("h1",{attrs:{id:"load-model-if-a-loadfilename-is-provided"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#load-model-if-a-loadfilename-is-provided"}},[e._v("#")]),e._v(" Load model if a loadFilename is provided")]),e._v(" "),t("p",[e._v("if loadFilename:\n# If loading on same machine the model was trained on\ncheckpoint = "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.load.html#torch.load",title:"torch.load",target:"_blank",rel:"noopener noreferrer"}},[e._v("torch.load"),t("OutboundLink")],1),e._v("(loadFilename)\n# If loading a model trained on GPU to CPU\n#checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\nencoder_sd = checkpoint['en']\ndecoder_sd = checkpoint['de']\nencoder_optimizer_sd = checkpoint['en_opt']\ndecoder_optimizer_sd = checkpoint['de_opt']\nembedding_sd = checkpoint['embedding']\nvoc."),t("strong",[e._v("dict")]),e._v(" = checkpoint['voc_dict']")]),e._v(" "),t("p",[e._v("print('Building encoder and decoder ...')")]),e._v(" "),t("h1",{attrs:{id:"initialize-word-embeddings"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#initialize-word-embeddings"}},[e._v("#")]),e._v(" Initialize word embeddings")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding",title:"torch.nn.Embedding",target:"_blank",rel:"noopener noreferrer"}},[e._v("embedding"),t("OutboundLink")],1),e._v(" = "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding",title:"torch.nn.Embedding",target:"_blank",rel:"noopener noreferrer"}},[e._v("nn.Embedding"),t("OutboundLink")],1),e._v("(voc.num_words, hidden_size)\nif loadFilename:\n"),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict",title:"torch.nn.Module.load_state_dict",target:"_blank",rel:"noopener noreferrer"}},[e._v("embedding.load_state_dict"),t("OutboundLink")],1),e._v("(embedding_sd)")]),e._v(" "),t("h1",{attrs:{id:"initialize-encoder-decoder-models"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#initialize-encoder-decoder-models"}},[e._v("#")]),e._v(" Initialize encoder & decoder models")]),e._v(" "),t("p",[e._v("encoder = "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module",title:"torch.nn.Module",target:"_blank",rel:"noopener noreferrer"}},[e._v("EncoderRNN"),t("OutboundLink")],1),e._v("(hidden_size, "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding",title:"torch.nn.Embedding",target:"_blank",rel:"noopener noreferrer"}},[e._v("embedding"),t("OutboundLink")],1),e._v(", encoder_n_layers, dropout)\ndecoder = "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module",title:"torch.nn.Module",target:"_blank",rel:"noopener noreferrer"}},[e._v("LuongAttnDecoderRNN"),t("OutboundLink")],1),e._v("(attn_model, "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding",title:"torch.nn.Embedding",target:"_blank",rel:"noopener noreferrer"}},[e._v("embedding"),t("OutboundLink")],1),e._v(", hidden_size, voc.num_words, decoder_n_layers, dropout)\nif loadFilename:\n"),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict",title:"torch.nn.Module.load_state_dict",target:"_blank",rel:"noopener noreferrer"}},[e._v("encoder.load_state_dict"),t("OutboundLink")],1),e._v("(encoder_sd)\n"),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict",title:"torch.nn.Module.load_state_dict",target:"_blank",rel:"noopener noreferrer"}},[e._v("decoder.load_state_dict"),t("OutboundLink")],1),e._v("(decoder_sd)")]),e._v(" "),t("h1",{attrs:{id:"use-appropriate-device"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#use-appropriate-device"}},[e._v("#")]),e._v(" Use appropriate device")]),e._v(" "),t("p",[e._v("encoder = "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.to",title:"torch.nn.Module.to",target:"_blank",rel:"noopener noreferrer"}},[e._v("encoder.to"),t("OutboundLink")],1),e._v("("),t("a",{attrs:{href:"https://pytorch.org/docs/stable/tensor_attributes.html#torch.device",title:"torch.device",target:"_blank",rel:"noopener noreferrer"}},[e._v("device"),t("OutboundLink")],1),e._v(")\ndecoder = "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.to",title:"torch.nn.Module.to",target:"_blank",rel:"noopener noreferrer"}},[e._v("decoder.to"),t("OutboundLink")],1),e._v("("),t("a",{attrs:{href:"https://pytorch.org/docs/stable/tensor_attributes.html#torch.device",title:"torch.device",target:"_blank",rel:"noopener noreferrer"}},[e._v("device"),t("OutboundLink")],1),e._v(")\nprint('Models built and ready to go!')")]),e._v(" "),t("p",[e._v("Building encoder and decoder ...\nModels built and ready to go!")]),e._v(" "),t("h3",{attrs:{id:"выполнить-обучение"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#выполнить-обучение"}},[e._v("#")]),e._v(" Выполнить обучение"),t("a",{attrs:{href:"https://pytorch.org/tutorials/beginner/chatbot_tutorial.html?highlight=chat%20bot#run-training",target:"_blank",rel:"noopener noreferrer"}},[t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("Запустите следующий блок, если вы хотите обучить модель.")]),e._v(" "),t("p",[e._v("Сначала мы устанавливаем параметры обучения, затем инициализируем наши оптимизаторы и, наконец, вызываем функцию "),t("code",[e._v("trainIters")]),e._v("для запуска наших итераций обучения.")]),e._v(" "),t("h1",{attrs:{id:"configure-training-optimization"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#configure-training-optimization"}},[e._v("#")]),e._v(" Configure training/optimization")]),e._v(" "),t("p",[e._v("clip = 50.0\nteacher_forcing_ratio = 1.0\nlearning_rate = 0.0001\ndecoder_learning_ratio = 5.0\nn_iteration = 4000\nprint_every = 1\nsave_every = 500")]),e._v(" "),t("h1",{attrs:{id:"ensure-dropout-layers-are-in-train-mode"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#ensure-dropout-layers-are-in-train-mode"}},[e._v("#")]),e._v(" Ensure dropout layers are in train mode")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train",title:"torch.nn.Module.train",target:"_blank",rel:"noopener noreferrer"}},[e._v("encoder.train"),t("OutboundLink")],1),e._v("()\n"),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train",title:"torch.nn.Module.train",target:"_blank",rel:"noopener noreferrer"}},[e._v("decoder.train"),t("OutboundLink")],1),e._v("()")]),e._v(" "),t("h1",{attrs:{id:"initialize-optimizers"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#initialize-optimizers"}},[e._v("#")]),e._v(" Initialize optimizers")]),e._v(" "),t("p",[e._v("print('Building optimizers ...')\n"),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam",title:"torch.optim.Adam",target:"_blank",rel:"noopener noreferrer"}},[e._v("encoder_optimizer"),t("OutboundLink")],1),e._v(" = "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam",title:"torch.optim.Adam",target:"_blank",rel:"noopener noreferrer"}},[e._v("optim.Adam"),t("OutboundLink")],1),e._v("("),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters",title:"torch.nn.Module.parameters",target:"_blank",rel:"noopener noreferrer"}},[e._v("encoder.parameters"),t("OutboundLink")],1),e._v("(), lr=learning_rate)\n"),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam",title:"torch.optim.Adam",target:"_blank",rel:"noopener noreferrer"}},[e._v("decoder_optimizer"),t("OutboundLink")],1),e._v(" = "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam",title:"torch.optim.Adam",target:"_blank",rel:"noopener noreferrer"}},[e._v("optim.Adam"),t("OutboundLink")],1),e._v("("),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters",title:"torch.nn.Module.parameters",target:"_blank",rel:"noopener noreferrer"}},[e._v("decoder.parameters"),t("OutboundLink")],1),e._v("(), lr=learning_rate * decoder_learning_ratio)\nif loadFilename:\n"),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam.load_state_dict",title:"torch.optim.Adam.load_state_dict",target:"_blank",rel:"noopener noreferrer"}},[e._v("encoder_optimizer.load_state_dict"),t("OutboundLink")],1),e._v("(encoder_optimizer_sd)\n"),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam.load_state_dict",title:"torch.optim.Adam.load_state_dict",target:"_blank",rel:"noopener noreferrer"}},[e._v("decoder_optimizer.load_state_dict"),t("OutboundLink")],1),e._v("(decoder_optimizer_sd)")]),e._v(" "),t("h1",{attrs:{id:"if-you-have-cuda-configure-cuda-to-call"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#if-you-have-cuda-configure-cuda-to-call"}},[e._v("#")]),e._v(" If you have cuda, configure cuda to call")]),e._v(" "),t("p",[e._v("for state in "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam",title:"torch.optim.Adam",target:"_blank",rel:"noopener noreferrer"}},[e._v("encoder_optimizer"),t("OutboundLink")],1),e._v(".state.values():\nfor k, v in state.items():\nif isinstance(v, "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/tensors.html#torch.Tensor",title:"torch.Tensor",target:"_blank",rel:"noopener noreferrer"}},[e._v("torch.Tensor"),t("OutboundLink")],1),e._v("):\nstate[k] = v.cuda()")]),e._v(" "),t("p",[e._v("for state in "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam",title:"torch.optim.Adam",target:"_blank",rel:"noopener noreferrer"}},[e._v("decoder_optimizer"),t("OutboundLink")],1),e._v(".state.values():\nfor k, v in state.items():\nif isinstance(v, "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/tensors.html#torch.Tensor",title:"torch.Tensor",target:"_blank",rel:"noopener noreferrer"}},[e._v("torch.Tensor"),t("OutboundLink")],1),e._v("):\nstate[k] = v.cuda()")]),e._v(" "),t("h1",{attrs:{id:"run-training-iterations"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#run-training-iterations"}},[e._v("#")]),e._v(" Run training iterations")]),e._v(" "),t("p",[e._v('print("Starting Training!")\ntrainIters(model_name, voc, pairs, encoder, decoder, '),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam",title:"torch.optim.Adam",target:"_blank",rel:"noopener noreferrer"}},[e._v("encoder_optimizer"),t("OutboundLink")],1),e._v(", "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam",title:"torch.optim.Adam",target:"_blank",rel:"noopener noreferrer"}},[e._v("decoder_optimizer"),t("OutboundLink")],1),e._v(",\n"),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding",title:"torch.nn.Embedding",target:"_blank",rel:"noopener noreferrer"}},[e._v("embedding"),t("OutboundLink")],1),e._v(", encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\nprint_every, save_every, clip, corpus_name, loadFilename)")]),e._v(" "),t("p",[e._v("Building optimizers ...\nStarting Training!\nInitializing ...\nTraining...\nIteration: 1; Percent complete: 0.0%; Average loss: 8.9700\nIteration: 2; Percent complete: 0.1%; Average loss: 8.8470\nIteration: 3; Percent complete: 0.1%; Average loss: 8.6492\nIteration: 4; Percent complete: 0.1%; Average loss: 8.3822\nIteration: 5; Percent complete: 0.1%; Average loss: 7.9444\nIteration: 6; Percent complete: 0.1%; Average loss: 7.4082\nIteration: 7; Percent complete: 0.2%; Average loss: 6.9694\nIteration: 8; Percent complete: 0.2%; Average loss: 7.0588\nIteration: 9; Percent complete: 0.2%; Average loss: 7.0053\nIteration: 10; Percent complete: 0.2%; Average loss: 6.8384\nIteration: 11; Percent complete: 0.3%; Average loss: 6.3953\nIteration: 12; Percent complete: 0.3%; Average loss: 6.3882\nIteration: 13; Percent complete: 0.3%; Average loss: 5.7536\nIteration: 14; Percent complete: 0.4%; Average loss: 5.8333\nIteration: 15; Percent complete: 0.4%; Average loss: 5.3831\nIteration: 16; Percent complete: 0.4%; Average loss: 5.5356\nIteration: 17; Percent complete: 0.4%; Average loss: 5.5085\nIteration: 18; Percent complete: 0.4%; Average loss: 5.2067\nIteration: 19; Percent complete: 0.5%; Average loss: 5.1449\nIteration: 20; Percent complete: 0.5%; Average loss: 4.9224\nIteration: 21; Percent complete: 0.5%; Average loss: 4.9559\nIteration: 22; Percent complete: 0.5%; Average loss: 5.2067\nIteration: 23; Percent complete: 0.6%; Average loss: 4.9907\nIteration: 24; Percent complete: 0.6%; Average loss: 4.9597\nIteration: 25; Percent complete: 0.6%; Average loss: 4.7518\nIteration: 26; Percent complete: 0.7%; Average loss: 4.8905\nIteration: 27; Percent complete: 0.7%; Average loss: 4.8915\nIteration: 28; Percent complete: 0.7%; Average loss: 4.9458\nIteration: 29; Percent complete: 0.7%; Average loss: 4.8195\nIteration: 30; Percent complete: 0.8%; Average loss: 4.6957\nIteration: 31; Percent complete: 0.8%; Average loss: 4.7685\nIteration: 32; Percent complete: 0.8%; Average loss: 4.6801\nIteration: 33; Percent complete: 0.8%; Average loss: 4.6699\nIteration: 34; Percent complete: 0.9%; Average loss: 4.8237\nIteration: 35; Percent complete: 0.9%; Average loss: 4.7081\nIteration: 36; Percent complete: 0.9%; Average loss: 4.8263\nIteration: 37; Percent complete: 0.9%; Average loss: 4.6833\nIteration: 38; Percent complete: 0.9%; Average loss: 4.7279\nIteration: 39; Percent complete: 1.0%; Average loss: 4.8308\nIteration: 40; Percent complete: 1.0%; Average loss: 4.7558\nIteration: 41; Percent complete: 1.0%; Average loss: 4.5463\nIteration: 42; Percent complete: 1.1%; Average loss: 4.6118\nIteration: 43; Percent complete: 1.1%; Average loss: 4.7541\nIteration: 44; Percent complete: 1.1%; Average loss: 4.9019\nIteration: 45; Percent complete: 1.1%; Average loss: 4.6478\nIteration: 46; Percent complete: 1.1%; Average loss: 4.8721\nIteration: 47; Percent complete: 1.2%; Average loss: 4.5145\nIteration: 48; Percent complete: 1.2%; Average loss: 4.4521\nIteration: 49; Percent complete: 1.2%; Average loss: 4.8045\nIteration: 50; Percent complete: 1.2%; Average loss: 4.8926\nIteration: 51; Percent complete: 1.3%; Average loss: 4.7199\nIteration: 52; Percent complete: 1.3%; Average loss: 4.6343\nIteration: 53; Percent complete: 1.3%; Average loss: 4.6322\nIteration: 54; Percent complete: 1.4%; Average loss: 4.7976\nIteration: 55; Percent complete: 1.4%; Average loss: 4.6095\nIteration: 56; Percent complete: 1.4%; Average loss: 4.5252\nIteration: 57; Percent complete: 1.4%; Average loss: 4.4518\nIteration: 58; Percent complete: 1.5%; Average loss: 4.5704\nIteration: 59; Percent complete: 1.5%; Average loss: 4.5866\nIteration: 60; Percent complete: 1.5%; Average loss: 4.2498\nIteration: 61; Percent complete: 1.5%; Average loss: 4.6244\nIteration: 62; Percent complete: 1.6%; Average loss: 4.5608\nIteration: 63; Percent complete: 1.6%; Average loss: 4.7279\nIteration: 64; Percent complete: 1.6%; Average loss: 4.7355\nIteration: 65; Percent complete: 1.6%; Average loss: 4.3942\nIteration: 66; Percent complete: 1.7%; Average loss: 4.6423\nIteration: 67; Percent complete: 1.7%; Average loss: 4.6154\nIteration: 68; Percent complete: 1.7%; Average loss: 4.4548\nIteration: 69; Percent complete: 1.7%; Average loss: 4.7868\nIteration: 70; Percent complete: 1.8%; Average loss: 4.7671\nIteration: 71; Percent complete: 1.8%; Average loss: 4.4860\nIteration: 72; Percent complete: 1.8%; Average loss: 4.4452\nIteration: 73; Percent complete: 1.8%; Average loss: 4.5898\nIteration: 74; Percent complete: 1.8%; Average loss: 4.5361\nIteration: 75; Percent complete: 1.9%; Average loss: 4.5354\nIteration: 76; Percent complete: 1.9%; Average loss: 4.4998\nIteration: 77; Percent complete: 1.9%; Average loss: 4.5722\nIteration: 78; Percent complete: 1.9%; Average loss: 4.4933\nIteration: 79; Percent complete: 2.0%; Average loss: 4.4895\nIteration: 80; Percent complete: 2.0%; Average loss: 4.3753\nIteration: 81; Percent complete: 2.0%; Average loss: 4.6085\nIteration: 82; Percent complete: 2.1%; Average loss: 4.4414\nIteration: 83; Percent complete: 2.1%; Average loss: 4.5456\nIteration: 84; Percent complete: 2.1%; Average loss: 4.5438\nIteration: 85; Percent complete: 2.1%; Average loss: 4.3721\nIteration: 86; Percent complete: 2.1%; Average loss: 4.6570\nIteration: 87; Percent complete: 2.2%; Average loss: 4.3506\nIteration: 88; Percent complete: 2.2%; Average loss: 4.5794\nIteration: 89; Percent complete: 2.2%; Average loss: 4.3529\nIteration: 90; Percent complete: 2.2%; Average loss: 4.5047\nIteration: 91; Percent complete: 2.3%; Average loss: 4.3786\nIteration: 92; Percent complete: 2.3%; Average loss: 4.5125\nIteration: 93; Percent complete: 2.3%; Average loss: 4.3071\nIteration: 94; Percent complete: 2.4%; Average loss: 4.4469\nIteration: 95; Percent complete: 2.4%; Average loss: 4.4316\nIteration: 96; Percent complete: 2.4%; Average loss: 4.7124\nIteration: 97; Percent complete: 2.4%; Average loss: 4.6710\nIteration: 98; Percent complete: 2.5%; Average loss: 4.6075\nIteration: 99; Percent complete: 2.5%; Average loss: 4.4941\nIteration: 100; Percent complete: 2.5%; Average loss: 4.5788\nIteration: 101; Percent complete: 2.5%; Average loss: 4.3714\nIteration: 102; Percent complete: 2.5%; Average loss: 4.3298\nIteration: 103; Percent complete: 2.6%; Average loss: 4.8722\nIteration: 104; Percent complete: 2.6%; Average loss: 4.1694\nIteration: 105; Percent complete: 2.6%; Average loss: 4.4343\nIteration: 106; Percent complete: 2.6%; Average loss: 4.5241\nIteration: 107; Percent complete: 2.7%; Average loss: 4.4016\nIteration: 108; Percent complete: 2.7%; Average loss: 4.5618\nIteration: 109; Percent complete: 2.7%; Average loss: 4.4316\nIteration: 110; Percent complete: 2.8%; Average loss: 4.6101\nIteration: 111; Percent complete: 2.8%; Average loss: 4.5759\nIteration: 112; Percent complete: 2.8%; Average loss: 4.4571\nIteration: 113; Percent complete: 2.8%; Average loss: 4.6995\nIteration: 114; Percent complete: 2.9%; Average loss: 4.3772\nIteration: 115; Percent complete: 2.9%; Average loss: 4.3443\nIteration: 116; Percent complete: 2.9%; Average loss: 4.5216\nIteration: 117; Percent complete: 2.9%; Average loss: 4.2190\nIteration: 118; Percent complete: 2.9%; Average loss: 4.5704\nIteration: 119; Percent complete: 3.0%; Average loss: 4.3013\nIteration: 120; Percent complete: 3.0%; Average loss: 4.6425\nIteration: 121; Percent complete: 3.0%; Average loss: 4.2048\nIteration: 122; Percent complete: 3.0%; Average loss: 4.1455\nIteration: 123; Percent complete: 3.1%; Average loss: 4.3540\nIteration: 124; Percent complete: 3.1%; Average loss: 4.3485\nIteration: 125; Percent complete: 3.1%; Average loss: 4.0981\nIteration: 126; Percent complete: 3.1%; Average loss: 4.3665\nIteration: 127; Percent complete: 3.2%; Average loss: 4.1973\nIteration: 128; Percent complete: 3.2%; Average loss: 4.2205\nIteration: 129; Percent complete: 3.2%; Average loss: 4.1992\nIteration: 130; Percent complete: 3.2%; Average loss: 4.4132\nIteration: 131; Percent complete: 3.3%; Average loss: 4.0240\nIteration: 132; Percent complete: 3.3%; Average loss: 4.2990\nIteration: 133; Percent complete: 3.3%; Average loss: 4.5170\nIteration: 134; Percent complete: 3.4%; Average loss: 3.8894\nIteration: 135; Percent complete: 3.4%; Average loss: 4.3818\nIteration: 136; Percent complete: 3.4%; Average loss: 4.4036\nIteration: 137; Percent complete: 3.4%; Average loss: 4.1783\nIteration: 138; Percent complete: 3.5%; Average loss: 4.3174\nIteration: 139; Percent complete: 3.5%; Average loss: 4.4096\nIteration: 140; Percent complete: 3.5%; Average loss: 4.0719\nIteration: 141; Percent complete: 3.5%; Average loss: 4.3096\nIteration: 142; Percent complete: 3.5%; Average loss: 4.5662\nIteration: 143; Percent complete: 3.6%; Average loss: 4.3051\nIteration: 144; Percent complete: 3.6%; Average loss: 4.3809\nIteration: 145; Percent complete: 3.6%; Average loss: 4.3486\nIteration: 146; Percent complete: 3.6%; Average loss: 4.3289\nIteration: 147; Percent complete: 3.7%; Average loss: 4.1118\nIteration: 148; Percent complete: 3.7%; Average loss: 4.3161\nIteration: 149; Percent complete: 3.7%; Average loss: 3.8698\nIteration: 150; Percent complete: 3.8%; Average loss: 4.5008\nIteration: 151; Percent complete: 3.8%; Average loss: 4.2832\nIteration: 152; Percent complete: 3.8%; Average loss: 4.2264\nIteration: 153; Percent complete: 3.8%; Average loss: 4.2891\nIteration: 154; Percent complete: 3.9%; Average loss: 4.1414\nIteration: 155; Percent complete: 3.9%; Average loss: 4.3741\nIteration: 156; Percent complete: 3.9%; Average loss: 4.0703\nIteration: 157; Percent complete: 3.9%; Average loss: 4.3230\nIteration: 158; Percent complete: 4.0%; Average loss: 3.9987\nIteration: 159; Percent complete: 4.0%; Average loss: 4.3711\nIteration: 160; Percent complete: 4.0%; Average loss: 4.1353\nIteration: 161; Percent complete: 4.0%; Average loss: 3.9694\nIteration: 162; Percent complete: 4.0%; Average loss: 4.1109\nIteration: 163; Percent complete: 4.1%; Average loss: 4.2590\nIteration: 164; Percent complete: 4.1%; Average loss: 4.1788\nIteration: 165; Percent complete: 4.1%; Average loss: 4.3308\nIteration: 166; Percent complete: 4.2%; Average loss: 4.2642\nIteration: 167; Percent complete: 4.2%; Average loss: 4.3659\nIteration: 168; Percent complete: 4.2%; Average loss: 4.4066\nIteration: 169; Percent complete: 4.2%; Average loss: 4.1221\nIteration: 170; Percent complete: 4.2%; Average loss: 4.3547\nIteration: 171; Percent complete: 4.3%; Average loss: 3.9961\nIteration: 172; Percent complete: 4.3%; Average loss: 4.1953\nIteration: 173; Percent complete: 4.3%; Average loss: 4.1440\nIteration: 174; Percent complete: 4.3%; Average loss: 4.2400\nIteration: 175; Percent complete: 4.4%; Average loss: 4.1635\nIteration: 176; Percent complete: 4.4%; Average loss: 4.1198\nIteration: 177; Percent complete: 4.4%; Average loss: 4.2807\nIteration: 178; Percent complete: 4.5%; Average loss: 3.9931\nIteration: 179; Percent complete: 4.5%; Average loss: 4.2450\nIteration: 180; Percent complete: 4.5%; Average loss: 4.2311\nIteration: 181; Percent complete: 4.5%; Average loss: 4.1536\nIteration: 182; Percent complete: 4.5%; Average loss: 4.1184\nIteration: 183; Percent complete: 4.6%; Average loss: 4.2194\nIteration: 184; Percent complete: 4.6%; Average loss: 4.3290\nIteration: 185; Percent complete: 4.6%; Average loss: 4.2879\nIteration: 186; Percent complete: 4.7%; Average loss: 4.3699\nIteration: 187; Percent complete: 4.7%; Average loss: 4.2234\nIteration: 188; Percent complete: 4.7%; Average loss: 3.8990\nIteration: 189; Percent complete: 4.7%; Average loss: 4.1708\nIteration: 190; Percent complete: 4.8%; Average loss: 4.2739\nIteration: 191; Percent complete: 4.8%; Average loss: 4.2301\nIteration: 192; Percent complete: 4.8%; Average loss: 4.3405\nIteration: 193; Percent complete: 4.8%; Average loss: 3.9069\nIteration: 194; Percent complete: 4.9%; Average loss: 4.3545\nIteration: 195; Percent complete: 4.9%; Average loss: 3.7989\nIteration: 196; Percent complete: 4.9%; Average loss: 3.8028\nIteration: 197; Percent complete: 4.9%; Average loss: 4.0973\nIteration: 198; Percent complete: 5.0%; Average loss: 4.0655\nIteration: 199; Percent complete: 5.0%; Average loss: 3.9145\nIteration: 200; Percent complete: 5.0%; Average loss: 4.0161\nIteration: 201; Percent complete: 5.0%; Average loss: 4.3891\nIteration: 202; Percent complete: 5.1%; Average loss: 3.9099\nIteration: 203; Percent complete: 5.1%; Average loss: 4.0327\nIteration: 204; Percent complete: 5.1%; Average loss: 3.8482\nIteration: 205; Percent complete: 5.1%; Average loss: 4.4438\nIteration: 206; Percent complete: 5.1%; Average loss: 3.8727\nIteration: 207; Percent complete: 5.2%; Average loss: 4.1948\nIteration: 208; Percent complete: 5.2%; Average loss: 3.9674\nIteration: 209; Percent complete: 5.2%; Average loss: 4.1582\nIteration: 210; Percent complete: 5.2%; Average loss: 4.3650\nIteration: 211; Percent complete: 5.3%; Average loss: 3.9919\nIteration: 212; Percent complete: 5.3%; Average loss: 4.2016\nIteration: 213; Percent complete: 5.3%; Average loss: 4.1472\nIteration: 214; Percent complete: 5.3%; Average loss: 4.1354\nIteration: 215; Percent complete: 5.4%; Average loss: 4.1148\nIteration: 216; Percent complete: 5.4%; Average loss: 4.0283\nIteration: 217; Percent complete: 5.4%; Average loss: 3.8195\nIteration: 218; Percent complete: 5.5%; Average loss: 4.3116\nIteration: 219; Percent complete: 5.5%; Average loss: 3.9345\nIteration: 220; Percent complete: 5.5%; Average loss: 4.0701\nIteration: 221; Percent complete: 5.5%; Average loss: 3.9311\nIteration: 222; Percent complete: 5.5%; Average loss: 4.0128\nIteration: 223; Percent complete: 5.6%; Average loss: 3.7172\nIteration: 224; Percent complete: 5.6%; Average loss: 3.7222\nIteration: 225; Percent complete: 5.6%; Average loss: 3.9537\nIteration: 226; Percent complete: 5.7%; Average loss: 4.1152\nIteration: 227; Percent complete: 5.7%; Average loss: 4.0149\nIteration: 228; Percent complete: 5.7%; Average loss: 3.8137\nIteration: 229; Percent complete: 5.7%; Average loss: 4.0339\nIteration: 230; Percent complete: 5.8%; Average loss: 3.8447\nIteration: 231; Percent complete: 5.8%; Average loss: 3.9381\nIteration: 232; Percent complete: 5.8%; Average loss: 3.8171\nIteration: 233; Percent complete: 5.8%; Average loss: 4.4217\nIteration: 234; Percent complete: 5.9%; Average loss: 3.9165\nIteration: 235; Percent complete: 5.9%; Average loss: 4.2247\nIteration: 236; Percent complete: 5.9%; Average loss: 4.0249\nIteration: 237; Percent complete: 5.9%; Average loss: 3.9571\nIteration: 238; Percent complete: 5.9%; Average loss: 3.7278\nIteration: 239; Percent complete: 6.0%; Average loss: 4.0054\nIteration: 240; Percent complete: 6.0%; Average loss: 4.1672\nIteration: 241; Percent complete: 6.0%; Average loss: 3.7650\nIteration: 242; Percent complete: 6.0%; Average loss: 4.1035\nIteration: 243; Percent complete: 6.1%; Average loss: 3.9667\nIteration: 244; Percent complete: 6.1%; Average loss: 3.8894\nIteration: 245; Percent complete: 6.1%; Average loss: 4.2070\nIteration: 246; Percent complete: 6.2%; Average loss: 4.0519\nIteration: 247; Percent complete: 6.2%; Average loss: 4.1808\nIteration: 248; Percent complete: 6.2%; Average loss: 3.9132\nIteration: 249; Percent complete: 6.2%; Average loss: 4.1259\nIteration: 250; Percent complete: 6.2%; Average loss: 3.9206\nIteration: 251; Percent complete: 6.3%; Average loss: 4.1631\nIteration: 252; Percent complete: 6.3%; Average loss: 4.0771\nIteration: 253; Percent complete: 6.3%; Average loss: 3.8434\nIteration: 254; Percent complete: 6.3%; Average loss: 3.8822\nIteration: 255; Percent complete: 6.4%; Average loss: 3.9802\nIteration: 256; Percent complete: 6.4%; Average loss: 3.7456\nIteration: 257; Percent complete: 6.4%; Average loss: 4.0142\nIteration: 258; Percent complete: 6.5%; Average loss: 4.2198\nIteration: 259; Percent complete: 6.5%; Average loss: 3.8980\nIteration: 260; Percent complete: 6.5%; Average loss: 3.8626\nIteration: 261; Percent complete: 6.5%; Average loss: 3.8949\nIteration: 262; Percent complete: 6.6%; Average loss: 3.8173\nIteration: 263; Percent complete: 6.6%; Average loss: 3.7264\nIteration: 264; Percent complete: 6.6%; Average loss: 3.8598\nIteration: 265; Percent complete: 6.6%; Average loss: 4.1563\nIteration: 266; Percent complete: 6.7%; Average loss: 4.2438\nIteration: 267; Percent complete: 6.7%; Average loss: 3.9603\nIteration: 268; Percent complete: 6.7%; Average loss: 3.8456\nIteration: 269; Percent complete: 6.7%; Average loss: 4.0321\nIteration: 270; Percent complete: 6.8%; Average loss: 3.7480\nIteration: 271; Percent complete: 6.8%; Average loss: 3.9264\nIteration: 272; Percent complete: 6.8%; Average loss: 4.0873\nIteration: 273; Percent complete: 6.8%; Average loss: 3.6059\nIteration: 274; Percent complete: 6.9%; Average loss: 4.1603\nIteration: 275; Percent complete: 6.9%; Average loss: 3.8314\nIteration: 276; Percent complete: 6.9%; Average loss: 4.0169\nIteration: 277; Percent complete: 6.9%; Average loss: 4.1214\nIteration: 278; Percent complete: 7.0%; Average loss: 4.0171\nIteration: 279; Percent complete: 7.0%; Average loss: 3.9871\nIteration: 280; Percent complete: 7.0%; Average loss: 4.1332\nIteration: 281; Percent complete: 7.0%; Average loss: 3.8271\nIteration: 282; Percent complete: 7.0%; Average loss: 3.8667\nIteration: 283; Percent complete: 7.1%; Average loss: 3.9273\nIteration: 284; Percent complete: 7.1%; Average loss: 4.0817\nIteration: 285; Percent complete: 7.1%; Average loss: 4.0523\nIteration: 286; Percent complete: 7.1%; Average loss: 3.5757\nIteration: 287; Percent complete: 7.2%; Average loss: 3.9357\nIteration: 288; Percent complete: 7.2%; Average loss: 4.1580\nIteration: 289; Percent complete: 7.2%; Average loss: 3.9479\nIteration: 290; Percent complete: 7.2%; Average loss: 3.9766\nIteration: 291; Percent complete: 7.3%; Average loss: 3.7055\nIteration: 292; Percent complete: 7.3%; Average loss: 4.1444\nIteration: 293; Percent complete: 7.3%; Average loss: 4.0636\nIteration: 294; Percent complete: 7.3%; Average loss: 4.0749\nIteration: 295; Percent complete: 7.4%; Average loss: 4.0238\nIteration: 296; Percent complete: 7.4%; Average loss: 4.0357\nIteration: 297; Percent complete: 7.4%; Average loss: 3.9924\nIteration: 298; Percent complete: 7.4%; Average loss: 4.0683\nIteration: 299; Percent complete: 7.5%; Average loss: 4.1717\nIteration: 300; Percent complete: 7.5%; Average loss: 4.0412\nIteration: 301; Percent complete: 7.5%; Average loss: 3.9588\nIteration: 302; Percent complete: 7.5%; Average loss: 3.8434\nIteration: 303; Percent complete: 7.6%; Average loss: 3.9712\nIteration: 304; Percent complete: 7.6%; Average loss: 3.8805\nIteration: 305; Percent complete: 7.6%; Average loss: 3.9547\nIteration: 306; Percent complete: 7.6%; Average loss: 3.9086\nIteration: 307; Percent complete: 7.7%; Average loss: 3.6818\nIteration: 308; Percent complete: 7.7%; Average loss: 4.3812\nIteration: 309; Percent complete: 7.7%; Average loss: 3.6564\nIteration: 310; Percent complete: 7.8%; Average loss: 3.9988\nIteration: 311; Percent complete: 7.8%; Average loss: 4.0346\nIteration: 312; Percent complete: 7.8%; Average loss: 3.7475\nIteration: 313; Percent complete: 7.8%; Average loss: 3.5815\nIteration: 314; Percent complete: 7.8%; Average loss: 3.9625\nIteration: 315; Percent complete: 7.9%; Average loss: 3.8401\nIteration: 316; Percent complete: 7.9%; Average loss: 4.0395\nIteration: 317; Percent complete: 7.9%; Average loss: 4.0545\nIteration: 318; Percent complete: 8.0%; Average loss: 4.0083\nIteration: 319; Percent complete: 8.0%; Average loss: 4.0594\nIteration: 320; Percent complete: 8.0%; Average loss: 3.9105\nIteration: 321; Percent complete: 8.0%; Average loss: 3.7769\nIteration: 322; Percent complete: 8.1%; Average loss: 3.9270\nIteration: 323; Percent complete: 8.1%; Average loss: 3.7890\nIteration: 324; Percent complete: 8.1%; Average loss: 3.9031\nIteration: 325; Percent complete: 8.1%; Average loss: 3.8335\nIteration: 326; Percent complete: 8.2%; Average loss: 3.7360\nIteration: 327; Percent complete: 8.2%; Average loss: 3.8597\nIteration: 328; Percent complete: 8.2%; Average loss: 3.7423\nIteration: 329; Percent complete: 8.2%; Average loss: 3.9417\nIteration: 330; Percent complete: 8.2%; Average loss: 3.9134\nIteration: 331; Percent complete: 8.3%; Average loss: 3.8883\nIteration: 332; Percent complete: 8.3%; Average loss: 3.7918\nIteration: 333; Percent complete: 8.3%; Average loss: 3.8654\nIteration: 334; Percent complete: 8.3%; Average loss: 3.7036\nIteration: 335; Percent complete: 8.4%; Average loss: 3.9694\nIteration: 336; Percent complete: 8.4%; Average loss: 4.0825\nIteration: 337; Percent complete: 8.4%; Average loss: 3.9222\nIteration: 338; Percent complete: 8.5%; Average loss: 3.9358\nIteration: 339; Percent complete: 8.5%; Average loss: 3.8769\nIteration: 340; Percent complete: 8.5%; Average loss: 3.8878\nIteration: 341; Percent complete: 8.5%; Average loss: 4.2316\nIteration: 342; Percent complete: 8.6%; Average loss: 3.7874\nIteration: 343; Percent complete: 8.6%; Average loss: 3.8920\nIteration: 344; Percent complete: 8.6%; Average loss: 3.8393\nIteration: 345; Percent complete: 8.6%; Average loss: 4.0307\nIteration: 346; Percent complete: 8.6%; Average loss: 3.5886\nIteration: 347; Percent complete: 8.7%; Average loss: 3.7844\nIteration: 348; Percent complete: 8.7%; Average loss: 3.9334\nIteration: 349; Percent complete: 8.7%; Average loss: 3.7787\nIteration: 350; Percent complete: 8.8%; Average loss: 3.7018\nIteration: 351; Percent complete: 8.8%; Average loss: 4.0295\nIteration: 352; Percent complete: 8.8%; Average loss: 3.9926\nIteration: 353; Percent complete: 8.8%; Average loss: 3.8099\nIteration: 354; Percent complete: 8.8%; Average loss: 3.8230\nIteration: 355; Percent complete: 8.9%; Average loss: 3.8671\nIteration: 356; Percent complete: 8.9%; Average loss: 3.8722\nIteration: 357; Percent complete: 8.9%; Average loss: 3.6677\nIteration: 358; Percent complete: 8.9%; Average loss: 3.8607\nIteration: 359; Percent complete: 9.0%; Average loss: 4.0137\nIteration: 360; Percent complete: 9.0%; Average loss: 3.9628\nIteration: 361; Percent complete: 9.0%; Average loss: 3.8623\nIteration: 362; Percent complete: 9.0%; Average loss: 3.8694\nIteration: 363; Percent complete: 9.1%; Average loss: 3.6514\nIteration: 364; Percent complete: 9.1%; Average loss: 3.5634\nIteration: 365; Percent complete: 9.1%; Average loss: 3.7078\nIteration: 366; Percent complete: 9.2%; Average loss: 3.7598\nIteration: 367; Percent complete: 9.2%; Average loss: 3.7980\nIteration: 368; Percent complete: 9.2%; Average loss: 3.8088\nIteration: 369; Percent complete: 9.2%; Average loss: 4.1682\nIteration: 370; Percent complete: 9.2%; Average loss: 3.7819\nIteration: 371; Percent complete: 9.3%; Average loss: 3.8395\nIteration: 372; Percent complete: 9.3%; Average loss: 3.5782\nIteration: 373; Percent complete: 9.3%; Average loss: 3.7364\nIteration: 374; Percent complete: 9.3%; Average loss: 4.2207\nIteration: 375; Percent complete: 9.4%; Average loss: 3.9206\nIteration: 376; Percent complete: 9.4%; Average loss: 3.8898\nIteration: 377; Percent complete: 9.4%; Average loss: 3.9735\nIteration: 378; Percent complete: 9.4%; Average loss: 3.6952\nIteration: 379; Percent complete: 9.5%; Average loss: 3.7033\nIteration: 380; Percent complete: 9.5%; Average loss: 3.5934\nIteration: 381; Percent complete: 9.5%; Average loss: 3.8539\nIteration: 382; Percent complete: 9.6%; Average loss: 3.6227\nIteration: 383; Percent complete: 9.6%; Average loss: 3.7810\nIteration: 384; Percent complete: 9.6%; Average loss: 3.9347\nIteration: 385; Percent complete: 9.6%; Average loss: 3.6653\nIteration: 386; Percent complete: 9.7%; Average loss: 3.9774\nIteration: 387; Percent complete: 9.7%; Average loss: 3.8540\nIteration: 388; Percent complete: 9.7%; Average loss: 3.8881\nIteration: 389; Percent complete: 9.7%; Average loss: 3.8511\nIteration: 390; Percent complete: 9.8%; Average loss: 3.7250\nIteration: 391; Percent complete: 9.8%; Average loss: 3.8074\nIteration: 392; Percent complete: 9.8%; Average loss: 3.9215\nIteration: 393; Percent complete: 9.8%; Average loss: 4.2799\nIteration: 394; Percent complete: 9.8%; Average loss: 3.5309\nIteration: 395; Percent complete: 9.9%; Average loss: 3.7129\nIteration: 396; Percent complete: 9.9%; Average loss: 3.9738\nIteration: 397; Percent complete: 9.9%; Average loss: 4.1357\nIteration: 398; Percent complete: 10.0%; Average loss: 3.8166\nIteration: 399; Percent complete: 10.0%; Average loss: 3.6079\nIteration: 400; Percent complete: 10.0%; Average loss: 3.7370\nIteration: 401; Percent complete: 10.0%; Average loss: 3.8032\nIteration: 402; Percent complete: 10.1%; Average loss: 3.7411\nIteration: 403; Percent complete: 10.1%; Average loss: 3.8968\nIteration: 404; Percent complete: 10.1%; Average loss: 3.8351\nIteration: 405; Percent complete: 10.1%; Average loss: 3.9866\nIteration: 406; Percent complete: 10.2%; Average loss: 3.7723\nIteration: 407; Percent complete: 10.2%; Average loss: 3.9873\nIteration: 408; Percent complete: 10.2%; Average loss: 3.7800\nIteration: 409; Percent complete: 10.2%; Average loss: 3.9413\nIteration: 410; Percent complete: 10.2%; Average loss: 3.3991\nIteration: 411; Percent complete: 10.3%; Average loss: 3.7671\nIteration: 412; Percent complete: 10.3%; Average loss: 3.9957\nIteration: 413; Percent complete: 10.3%; Average loss: 3.7736\nIteration: 414; Percent complete: 10.3%; Average loss: 3.8679\nIteration: 415; Percent complete: 10.4%; Average loss: 3.7264\nIteration: 416; Percent complete: 10.4%; Average loss: 3.8155\nIteration: 417; Percent complete: 10.4%; Average loss: 3.6551\nIteration: 418; Percent complete: 10.4%; Average loss: 3.6520\nIteration: 419; Percent complete: 10.5%; Average loss: 4.1690\nIteration: 420; Percent complete: 10.5%; Average loss: 3.7811\nIteration: 421; Percent complete: 10.5%; Average loss: 3.9485\nIteration: 422; Percent complete: 10.5%; Average loss: 3.9121\nIteration: 423; Percent complete: 10.6%; Average loss: 3.7341\nIteration: 424; Percent complete: 10.6%; Average loss: 3.5346\nIteration: 425; Percent complete: 10.6%; Average loss: 3.9463\nIteration: 426; Percent complete: 10.7%; Average loss: 3.9926\nIteration: 427; Percent complete: 10.7%; Average loss: 3.6988\nIteration: 428; Percent complete: 10.7%; Average loss: 3.6252\nIteration: 429; Percent complete: 10.7%; Average loss: 3.8291\nIteration: 430; Percent complete: 10.8%; Average loss: 3.7843\nIteration: 431; Percent complete: 10.8%; Average loss: 4.0337\nIteration: 432; Percent complete: 10.8%; Average loss: 3.8813\nIteration: 433; Percent complete: 10.8%; Average loss: 3.6705\nIteration: 434; Percent complete: 10.8%; Average loss: 3.6109\nIteration: 435; Percent complete: 10.9%; Average loss: 3.8988\nIteration: 436; Percent complete: 10.9%; Average loss: 4.0428\nIteration: 437; Percent complete: 10.9%; Average loss: 3.9240\nIteration: 438; Percent complete: 10.9%; Average loss: 3.7478\nIteration: 439; Percent complete: 11.0%; Average loss: 3.6666\nIteration: 440; Percent complete: 11.0%; Average loss: 3.9939\nIteration: 441; Percent complete: 11.0%; Average loss: 3.7099\nIteration: 442; Percent complete: 11.1%; Average loss: 3.7962\nIteration: 443; Percent complete: 11.1%; Average loss: 4.2017\nIteration: 444; Percent complete: 11.1%; Average loss: 3.8396\nIteration: 445; Percent complete: 11.1%; Average loss: 4.0658\nIteration: 446; Percent complete: 11.2%; Average loss: 3.8802\nIteration: 447; Percent complete: 11.2%; Average loss: 3.6820\nIteration: 448; Percent complete: 11.2%; Average loss: 3.7557\nIteration: 449; Percent complete: 11.2%; Average loss: 3.7933\nIteration: 450; Percent complete: 11.2%; Average loss: 3.7714\nIteration: 451; Percent complete: 11.3%; Average loss: 3.6828\nIteration: 452; Percent complete: 11.3%; Average loss: 3.6148\nIteration: 453; Percent complete: 11.3%; Average loss: 3.7757\nIteration: 454; Percent complete: 11.3%; Average loss: 3.6143\nIteration: 455; Percent complete: 11.4%; Average loss: 3.8915\nIteration: 456; Percent complete: 11.4%; Average loss: 3.5861\nIteration: 457; Percent complete: 11.4%; Average loss: 3.8854\nIteration: 458; Percent complete: 11.5%; Average loss: 3.5180\nIteration: 459; Percent complete: 11.5%; Average loss: 3.6331\nIteration: 460; Percent complete: 11.5%; Average loss: 3.9545\nIteration: 461; Percent complete: 11.5%; Average loss: 3.9139\nIteration: 462; Percent complete: 11.6%; Average loss: 3.6444\nIteration: 463; Percent complete: 11.6%; Average loss: 3.8843\nIteration: 464; Percent complete: 11.6%; Average loss: 4.0560\nIteration: 465; Percent complete: 11.6%; Average loss: 4.0323\nIteration: 466; Percent complete: 11.7%; Average loss: 3.8524\nIteration: 467; Percent complete: 11.7%; Average loss: 4.0396\nIteration: 468; Percent complete: 11.7%; Average loss: 4.0068\nIteration: 469; Percent complete: 11.7%; Average loss: 3.7276\nIteration: 470; Percent complete: 11.8%; Average loss: 3.5703\nIteration: 471; Percent complete: 11.8%; Average loss: 3.7493\nIteration: 472; Percent complete: 11.8%; Average loss: 3.8255\nIteration: 473; Percent complete: 11.8%; Average loss: 3.8108\nIteration: 474; Percent complete: 11.8%; Average loss: 3.6415\nIteration: 475; Percent complete: 11.9%; Average loss: 3.8152\nIteration: 476; Percent complete: 11.9%; Average loss: 4.0190\nIteration: 477; Percent complete: 11.9%; Average loss: 3.7632\nIteration: 478; Percent complete: 11.9%; Average loss: 3.7051\nIteration: 479; Percent complete: 12.0%; Average loss: 3.7163\nIteration: 480; Percent complete: 12.0%; Average loss: 4.0750\nIteration: 481; Percent complete: 12.0%; Average loss: 3.6420\nIteration: 482; Percent complete: 12.0%; Average loss: 3.7057\nIteration: 483; Percent complete: 12.1%; Average loss: 3.7550\nIteration: 484; Percent complete: 12.1%; Average loss: 3.8002\nIteration: 485; Percent complete: 12.1%; Average loss: 3.7096\nIteration: 486; Percent complete: 12.2%; Average loss: 3.7390\nIteration: 487; Percent complete: 12.2%; Average loss: 3.9057\nIteration: 488; Percent complete: 12.2%; Average loss: 3.5208\nIteration: 489; Percent complete: 12.2%; Average loss: 3.7497\nIteration: 490; Percent complete: 12.2%; Average loss: 3.8399\nIteration: 491; Percent complete: 12.3%; Average loss: 3.9124\nIteration: 492; Percent complete: 12.3%; Average loss: 3.7688\nIteration: 493; Percent complete: 12.3%; Average loss: 3.5702\nIteration: 494; Percent complete: 12.3%; Average loss: 3.5491\nIteration: 495; Percent complete: 12.4%; Average loss: 3.7786\nIteration: 496; Percent complete: 12.4%; Average loss: 3.8979\nIteration: 497; Percent complete: 12.4%; Average loss: 3.6756\nIteration: 498; Percent complete: 12.4%; Average loss: 3.5799\nIteration: 499; Percent complete: 12.5%; Average loss: 3.5498\nIteration: 500; Percent complete: 12.5%; Average loss: 3.5317\nIteration: 501; Percent complete: 12.5%; Average loss: 3.5262\nIteration: 502; Percent complete: 12.6%; Average loss: 3.7196\nIteration: 503; Percent complete: 12.6%; Average loss: 3.3185\nIteration: 504; Percent complete: 12.6%; Average loss: 3.6548\nIteration: 505; Percent complete: 12.6%; Average loss: 3.7067\nIteration: 506; Percent complete: 12.7%; Average loss: 3.7489\nIteration: 507; Percent complete: 12.7%; Average loss: 4.0386\nIteration: 508; Percent complete: 12.7%; Average loss: 3.6530\nIteration: 509; Percent complete: 12.7%; Average loss: 3.6912\nIteration: 510; Percent complete: 12.8%; Average loss: 3.5430\nIteration: 511; Percent complete: 12.8%; Average loss: 3.6518\nIteration: 512; Percent complete: 12.8%; Average loss: 3.8413\nIteration: 513; Percent complete: 12.8%; Average loss: 3.6018\nIteration: 514; Percent complete: 12.8%; Average loss: 3.7885\nIteration: 515; Percent complete: 12.9%; Average loss: 3.9306\nIteration: 516; Percent complete: 12.9%; Average loss: 3.8261\nIteration: 517; Percent complete: 12.9%; Average loss: 4.1956\nIteration: 518; Percent complete: 13.0%; Average loss: 3.6474\nIteration: 519; Percent complete: 13.0%; Average loss: 3.6326\nIteration: 520; Percent complete: 13.0%; Average loss: 3.7512\nIteration: 521; Percent complete: 13.0%; Average loss: 3.5746\nIteration: 522; Percent complete: 13.1%; Average loss: 3.5617\nIteration: 523; Percent complete: 13.1%; Average loss: 3.6272\nIteration: 524; Percent complete: 13.1%; Average loss: 3.6374\nIteration: 525; Percent complete: 13.1%; Average loss: 3.6955\nIteration: 526; Percent complete: 13.2%; Average loss: 3.8945\nIteration: 527; Percent complete: 13.2%; Average loss: 4.0776\nIteration: 528; Percent complete: 13.2%; Average loss: 3.4162\nIteration: 529; Percent complete: 13.2%; Average loss: 3.7893\nIteration: 530; Percent complete: 13.2%; Average loss: 3.9246\nIteration: 531; Percent complete: 13.3%; Average loss: 3.6437\nIteration: 532; Percent complete: 13.3%; Average loss: 3.6396\nIteration: 533; Percent complete: 13.3%; Average loss: 3.7159\nIteration: 534; Percent complete: 13.4%; Average loss: 3.6660\nIteration: 535; Percent complete: 13.4%; Average loss: 3.5326\nIteration: 536; Percent complete: 13.4%; Average loss: 3.8390\nIteration: 537; Percent complete: 13.4%; Average loss: 3.7951\nIteration: 538; Percent complete: 13.5%; Average loss: 3.5712\nIteration: 539; Percent complete: 13.5%; Average loss: 4.0451\nIteration: 540; Percent complete: 13.5%; Average loss: 3.7214\nIteration: 541; Percent complete: 13.5%; Average loss: 3.8779\nIteration: 542; Percent complete: 13.6%; Average loss: 3.6397\nIteration: 543; Percent complete: 13.6%; Average loss: 3.8552\nIteration: 544; Percent complete: 13.6%; Average loss: 3.5136\nIteration: 545; Percent complete: 13.6%; Average loss: 3.7236\nIteration: 546; Percent complete: 13.7%; Average loss: 3.7471\nIteration: 547; Percent complete: 13.7%; Average loss: 3.5616\nIteration: 548; Percent complete: 13.7%; Average loss: 3.5848\nIteration: 549; Percent complete: 13.7%; Average loss: 3.7393\nIteration: 550; Percent complete: 13.8%; Average loss: 3.6228\nIteration: 551; Percent complete: 13.8%; Average loss: 3.8768\nIteration: 552; Percent complete: 13.8%; Average loss: 3.7422\nIteration: 553; Percent complete: 13.8%; Average loss: 3.6378\nIteration: 554; Percent complete: 13.9%; Average loss: 3.6924\nIteration: 555; Percent complete: 13.9%; Average loss: 3.8086\nIteration: 556; Percent complete: 13.9%; Average loss: 3.9153\nIteration: 557; Percent complete: 13.9%; Average loss: 3.7073\nIteration: 558; Percent complete: 14.0%; Average loss: 3.7361\nIteration: 559; Percent complete: 14.0%; Average loss: 3.5886\nIteration: 560; Percent complete: 14.0%; Average loss: 3.6520\nIteration: 561; Percent complete: 14.0%; Average loss: 3.7989\nIteration: 562; Percent complete: 14.1%; Average loss: 3.7032\nIteration: 563; Percent complete: 14.1%; Average loss: 3.5118\nIteration: 564; Percent complete: 14.1%; Average loss: 3.7663\nIteration: 565; Percent complete: 14.1%; Average loss: 3.6700\nIteration: 566; Percent complete: 14.1%; Average loss: 3.7845\nIteration: 567; Percent complete: 14.2%; Average loss: 3.4899\nIteration: 568; Percent complete: 14.2%; Average loss: 3.6253\nIteration: 569; Percent complete: 14.2%; Average loss: 3.7678\nIteration: 570; Percent complete: 14.2%; Average loss: 3.7171\nIteration: 571; Percent complete: 14.3%; Average loss: 3.7303\nIteration: 572; Percent complete: 14.3%; Average loss: 3.7554\nIteration: 573; Percent complete: 14.3%; Average loss: 3.8293\nIteration: 574; Percent complete: 14.3%; Average loss: 3.7566\nIteration: 575; Percent complete: 14.4%; Average loss: 3.5114\nIteration: 576; Percent complete: 14.4%; Average loss: 3.4420\nIteration: 577; Percent complete: 14.4%; Average loss: 3.4667\nIteration: 578; Percent complete: 14.4%; Average loss: 3.8175\nIteration: 579; Percent complete: 14.5%; Average loss: 3.7573\nIteration: 580; Percent complete: 14.5%; Average loss: 3.7346\nIteration: 581; Percent complete: 14.5%; Average loss: 3.7904\nIteration: 582; Percent complete: 14.5%; Average loss: 3.7799\nIteration: 583; Percent complete: 14.6%; Average loss: 3.7719\nIteration: 584; Percent complete: 14.6%; Average loss: 3.9636\nIteration: 585; Percent complete: 14.6%; Average loss: 3.9109\nIteration: 586; Percent complete: 14.6%; Average loss: 3.7162\nIteration: 587; Percent complete: 14.7%; Average loss: 3.7142\nIteration: 588; Percent complete: 14.7%; Average loss: 3.7941\nIteration: 589; Percent complete: 14.7%; Average loss: 3.7184\nIteration: 590; Percent complete: 14.8%; Average loss: 3.8140\nIteration: 591; Percent complete: 14.8%; Average loss: 3.7455\nIteration: 592; Percent complete: 14.8%; Average loss: 3.7415\nIteration: 593; Percent complete: 14.8%; Average loss: 3.6457\nIteration: 594; Percent complete: 14.8%; Average loss: 3.7642\nIteration: 595; Percent complete: 14.9%; Average loss: 3.5029\nIteration: 596; Percent complete: 14.9%; Average loss: 3.5364\nIteration: 597; Percent complete: 14.9%; Average loss: 3.4501\nIteration: 598; Percent complete: 14.9%; Average loss: 3.7058\nIteration: 599; Percent complete: 15.0%; Average loss: 3.5473\nIteration: 600; Percent complete: 15.0%; Average loss: 3.8271\nIteration: 601; Percent complete: 15.0%; Average loss: 3.8130\nIteration: 602; Percent complete: 15.0%; Average loss: 3.6043\nIteration: 603; Percent complete: 15.1%; Average loss: 3.4010\nIteration: 604; Percent complete: 15.1%; Average loss: 3.3608\nIteration: 605; Percent complete: 15.1%; Average loss: 3.7891\nIteration: 606; Percent complete: 15.2%; Average loss: 3.8294\nIteration: 607; Percent complete: 15.2%; Average loss: 3.5027\nIteration: 608; Percent complete: 15.2%; Average loss: 3.3865\nIteration: 609; Percent complete: 15.2%; Average loss: 3.5546\nIteration: 610; Percent complete: 15.2%; Average loss: 3.3992\nIteration: 611; Percent complete: 15.3%; Average loss: 3.4181\nIteration: 612; Percent complete: 15.3%; Average loss: 3.3687\nIteration: 613; Percent complete: 15.3%; Average loss: 3.4191\nIteration: 614; Percent complete: 15.3%; Average loss: 3.9346\nIteration: 615; Percent complete: 15.4%; Average loss: 3.6844\nIteration: 616; Percent complete: 15.4%; Average loss: 3.6599\nIteration: 617; Percent complete: 15.4%; Average loss: 3.6211\nIteration: 618; Percent complete: 15.4%; Average loss: 3.7372\nIteration: 619; Percent complete: 15.5%; Average loss: 3.6436\nIteration: 620; Percent complete: 15.5%; Average loss: 3.6031\nIteration: 621; Percent complete: 15.5%; Average loss: 3.5176\nIteration: 622; Percent complete: 15.6%; Average loss: 3.8381\nIteration: 623; Percent complete: 15.6%; Average loss: 3.5217\nIteration: 624; Percent complete: 15.6%; Average loss: 3.7862\nIteration: 625; Percent complete: 15.6%; Average loss: 3.6198\nIteration: 626; Percent complete: 15.7%; Average loss: 3.5482\nIteration: 627; Percent complete: 15.7%; Average loss: 3.6320\nIteration: 628; Percent complete: 15.7%; Average loss: 3.5803\nIteration: 629; Percent complete: 15.7%; Average loss: 3.5226\nIteration: 630; Percent complete: 15.8%; Average loss: 3.6963\nIteration: 631; Percent complete: 15.8%; Average loss: 3.9198\nIteration: 632; Percent complete: 15.8%; Average loss: 3.8749\nIteration: 633; Percent complete: 15.8%; Average loss: 3.5033\nIteration: 634; Percent complete: 15.8%; Average loss: 3.5430\nIteration: 635; Percent complete: 15.9%; Average loss: 3.5698\nIteration: 636; Percent complete: 15.9%; Average loss: 3.6722\nIteration: 637; Percent complete: 15.9%; Average loss: 3.5954\nIteration: 638; Percent complete: 16.0%; Average loss: 3.8314\nIteration: 639; Percent complete: 16.0%; Average loss: 3.7219\nIteration: 640; Percent complete: 16.0%; Average loss: 3.5729\nIteration: 641; Percent complete: 16.0%; Average loss: 3.6733\nIteration: 642; Percent complete: 16.1%; Average loss: 3.6377\nIteration: 643; Percent complete: 16.1%; Average loss: 3.7156\nIteration: 644; Percent complete: 16.1%; Average loss: 3.7067\nIteration: 645; Percent complete: 16.1%; Average loss: 3.8789\nIteration: 646; Percent complete: 16.2%; Average loss: 3.4068\nIteration: 647; Percent complete: 16.2%; Average loss: 3.6328\nIteration: 648; Percent complete: 16.2%; Average loss: 3.9147\nIteration: 649; Percent complete: 16.2%; Average loss: 3.4640\nIteration: 650; Percent complete: 16.2%; Average loss: 3.6289\nIteration: 651; Percent complete: 16.3%; Average loss: 3.4086\nIteration: 652; Percent complete: 16.3%; Average loss: 3.8568\nIteration: 653; Percent complete: 16.3%; Average loss: 3.5808\nIteration: 654; Percent complete: 16.4%; Average loss: 3.5658\nIteration: 655; Percent complete: 16.4%; Average loss: 3.5415\nIteration: 656; Percent complete: 16.4%; Average loss: 3.6627\nIteration: 657; Percent complete: 16.4%; Average loss: 3.6882\nIteration: 658; Percent complete: 16.4%; Average loss: 3.3507\nIteration: 659; Percent complete: 16.5%; Average loss: 3.6585\nIteration: 660; Percent complete: 16.5%; Average loss: 3.7087\nIteration: 661; Percent complete: 16.5%; Average loss: 3.5132\nIteration: 662; Percent complete: 16.6%; Average loss: 3.7195\nIteration: 663; Percent complete: 16.6%; Average loss: 3.7398\nIteration: 664; Percent complete: 16.6%; Average loss: 3.7965\nIteration: 665; Percent complete: 16.6%; Average loss: 3.6739\nIteration: 666; Percent complete: 16.7%; Average loss: 3.5253\nIteration: 667; Percent complete: 16.7%; Average loss: 3.6737\nIteration: 668; Percent complete: 16.7%; Average loss: 3.4974\nIteration: 669; Percent complete: 16.7%; Average loss: 3.9853\nIteration: 670; Percent complete: 16.8%; Average loss: 3.5467\nIteration: 671; Percent complete: 16.8%; Average loss: 3.8145\nIteration: 672; Percent complete: 16.8%; Average loss: 3.8515\nIteration: 673; Percent complete: 16.8%; Average loss: 3.3988\nIteration: 674; Percent complete: 16.9%; Average loss: 3.5211\nIteration: 675; Percent complete: 16.9%; Average loss: 3.7520\nIteration: 676; Percent complete: 16.9%; Average loss: 3.6912\nIteration: 677; Percent complete: 16.9%; Average loss: 3.4697\nIteration: 678; Percent complete: 17.0%; Average loss: 3.5212\nIteration: 679; Percent complete: 17.0%; Average loss: 3.7024\nIteration: 680; Percent complete: 17.0%; Average loss: 3.5967\nIteration: 681; Percent complete: 17.0%; Average loss: 3.7367\nIteration: 682; Percent complete: 17.1%; Average loss: 3.6471\nIteration: 683; Percent complete: 17.1%; Average loss: 3.5966\nIteration: 684; Percent complete: 17.1%; Average loss: 3.5273\nIteration: 685; Percent complete: 17.1%; Average loss: 3.7374\nIteration: 686; Percent complete: 17.2%; Average loss: 3.2905\nIteration: 687; Percent complete: 17.2%; Average loss: 3.5888\nIteration: 688; Percent complete: 17.2%; Average loss: 3.4674\nIteration: 689; Percent complete: 17.2%; Average loss: 3.3037\nIteration: 690; Percent complete: 17.2%; Average loss: 3.4470\nIteration: 691; Percent complete: 17.3%; Average loss: 3.6282\nIteration: 692; Percent complete: 17.3%; Average loss: 3.7601\nIteration: 693; Percent complete: 17.3%; Average loss: 3.3442\nIteration: 694; Percent complete: 17.3%; Average loss: 3.7033\nIteration: 695; Percent complete: 17.4%; Average loss: 3.5625\nIteration: 696; Percent complete: 17.4%; Average loss: 3.6657\nIteration: 697; Percent complete: 17.4%; Average loss: 3.4642\nIteration: 698; Percent complete: 17.4%; Average loss: 3.6097\nIteration: 699; Percent complete: 17.5%; Average loss: 3.6015\nIteration: 700; Percent complete: 17.5%; Average loss: 3.9117\nIteration: 701; Percent complete: 17.5%; Average loss: 3.4923\nIteration: 702; Percent complete: 17.5%; Average loss: 3.6074\nIteration: 703; Percent complete: 17.6%; Average loss: 3.6025\nIteration: 704; Percent complete: 17.6%; Average loss: 3.7339\nIteration: 705; Percent complete: 17.6%; Average loss: 3.7463\nIteration: 706; Percent complete: 17.6%; Average loss: 3.4058\nIteration: 707; Percent complete: 17.7%; Average loss: 3.7292\nIteration: 708; Percent complete: 17.7%; Average loss: 3.8080\nIteration: 709; Percent complete: 17.7%; Average loss: 3.4308\nIteration: 710; Percent complete: 17.8%; Average loss: 3.6952\nIteration: 711; Percent complete: 17.8%; Average loss: 3.6691\nIteration: 712; Percent complete: 17.8%; Average loss: 3.5117\nIteration: 713; Percent complete: 17.8%; Average loss: 3.5149\nIteration: 714; Percent complete: 17.8%; Average loss: 3.4909\nIteration: 715; Percent complete: 17.9%; Average loss: 3.3959\nIteration: 716; Percent complete: 17.9%; Average loss: 3.5248\nIteration: 717; Percent complete: 17.9%; Average loss: 3.7998\nIteration: 718; Percent complete: 17.9%; Average loss: 3.6354\nIteration: 719; Percent complete: 18.0%; Average loss: 3.7021\nIteration: 720; Percent complete: 18.0%; Average loss: 3.8058\nIteration: 721; Percent complete: 18.0%; Average loss: 3.3621\nIteration: 722; Percent complete: 18.1%; Average loss: 3.6679\nIteration: 723; Percent complete: 18.1%; Average loss: 3.7191\nIteration: 724; Percent complete: 18.1%; Average loss: 3.9297\nIteration: 725; Percent complete: 18.1%; Average loss: 3.6430\nIteration: 726; Percent complete: 18.1%; Average loss: 3.6120\nIteration: 727; Percent complete: 18.2%; Average loss: 3.9022\nIteration: 728; Percent complete: 18.2%; Average loss: 3.7454\nIteration: 729; Percent complete: 18.2%; Average loss: 3.5114\nIteration: 730; Percent complete: 18.2%; Average loss: 3.6949\nIteration: 731; Percent complete: 18.3%; Average loss: 3.5118\nIteration: 732; Percent complete: 18.3%; Average loss: 3.8177\nIteration: 733; Percent complete: 18.3%; Average loss: 3.7560\nIteration: 734; Percent complete: 18.4%; Average loss: 3.6772\nIteration: 735; Percent complete: 18.4%; Average loss: 3.3688\nIteration: 736; Percent complete: 18.4%; Average loss: 3.2745\nIteration: 737; Percent complete: 18.4%; Average loss: 3.6677\nIteration: 738; Percent complete: 18.4%; Average loss: 3.3109\nIteration: 739; Percent complete: 18.5%; Average loss: 3.4633\nIteration: 740; Percent complete: 18.5%; Average loss: 3.7874\nIteration: 741; Percent complete: 18.5%; Average loss: 3.2527\nIteration: 742; Percent complete: 18.6%; Average loss: 3.4407\nIteration: 743; Percent complete: 18.6%; Average loss: 3.6300\nIteration: 744; Percent complete: 18.6%; Average loss: 3.6061\nIteration: 745; Percent complete: 18.6%; Average loss: 3.4885\nIteration: 746; Percent complete: 18.6%; Average loss: 3.6302\nIteration: 747; Percent complete: 18.7%; Average loss: 3.5536\nIteration: 748; Percent complete: 18.7%; Average loss: 3.9225\nIteration: 749; Percent complete: 18.7%; Average loss: 3.3878\nIteration: 750; Percent complete: 18.8%; Average loss: 3.6142\nIteration: 751; Percent complete: 18.8%; Average loss: 3.3902\nIteration: 752; Percent complete: 18.8%; Average loss: 3.6572\nIteration: 753; Percent complete: 18.8%; Average loss: 3.4188\nIteration: 754; Percent complete: 18.9%; Average loss: 3.3910\nIteration: 755; Percent complete: 18.9%; Average loss: 3.5148\nIteration: 756; Percent complete: 18.9%; Average loss: 3.4673\nIteration: 757; Percent complete: 18.9%; Average loss: 3.6029\nIteration: 758; Percent complete: 18.9%; Average loss: 3.6335\nIteration: 759; Percent complete: 19.0%; Average loss: 3.4890\nIteration: 760; Percent complete: 19.0%; Average loss: 3.5215\nIteration: 761; Percent complete: 19.0%; Average loss: 3.5544\nIteration: 762; Percent complete: 19.1%; Average loss: 3.6318\nIteration: 763; Percent complete: 19.1%; Average loss: 3.5274\nIteration: 764; Percent complete: 19.1%; Average loss: 3.7429\nIteration: 765; Percent complete: 19.1%; Average loss: 3.7142\nIteration: 766; Percent complete: 19.1%; Average loss: 3.6719\nIteration: 767; Percent complete: 19.2%; Average loss: 3.6773\nIteration: 768; Percent complete: 19.2%; Average loss: 3.5151\nIteration: 769; Percent complete: 19.2%; Average loss: 3.6346\nIteration: 770; Percent complete: 19.2%; Average loss: 3.5922\nIteration: 771; Percent complete: 19.3%; Average loss: 3.8767\nIteration: 772; Percent complete: 19.3%; Average loss: 3.4024\nIteration: 773; Percent complete: 19.3%; Average loss: 3.6909\nIteration: 774; Percent complete: 19.4%; Average loss: 3.6540\nIteration: 775; Percent complete: 19.4%; Average loss: 3.7633\nIteration: 776; Percent complete: 19.4%; Average loss: 3.2732\nIteration: 777; Percent complete: 19.4%; Average loss: 3.7553\nIteration: 778; Percent complete: 19.4%; Average loss: 3.4036\nIteration: 779; Percent complete: 19.5%; Average loss: 3.4843\nIteration: 780; Percent complete: 19.5%; Average loss: 3.8564\nIteration: 781; Percent complete: 19.5%; Average loss: 3.5513\nIteration: 782; Percent complete: 19.6%; Average loss: 3.4089\nIteration: 783; Percent complete: 19.6%; Average loss: 3.5428\nIteration: 784; Percent complete: 19.6%; Average loss: 3.7658\nIteration: 785; Percent complete: 19.6%; Average loss: 3.5616\nIteration: 786; Percent complete: 19.7%; Average loss: 3.3015\nIteration: 787; Percent complete: 19.7%; Average loss: 3.5720\nIteration: 788; Percent complete: 19.7%; Average loss: 3.7423\nIteration: 789; Percent complete: 19.7%; Average loss: 3.8865\nIteration: 790; Percent complete: 19.8%; Average loss: 3.2603\nIteration: 791; Percent complete: 19.8%; Average loss: 3.2038\nIteration: 792; Percent complete: 19.8%; Average loss: 3.6855\nIteration: 793; Percent complete: 19.8%; Average loss: 3.7102\nIteration: 794; Percent complete: 19.9%; Average loss: 3.5375\nIteration: 795; Percent complete: 19.9%; Average loss: 3.4268\nIteration: 796; Percent complete: 19.9%; Average loss: 3.7297\nIteration: 797; Percent complete: 19.9%; Average loss: 3.5688\nIteration: 798; Percent complete: 20.0%; Average loss: 3.7186\nIteration: 799; Percent complete: 20.0%; Average loss: 3.6203\nIteration: 800; Percent complete: 20.0%; Average loss: 3.5660\nIteration: 801; Percent complete: 20.0%; Average loss: 3.5149\nIteration: 802; Percent complete: 20.1%; Average loss: 3.3750\nIteration: 803; Percent complete: 20.1%; Average loss: 3.6642\nIteration: 804; Percent complete: 20.1%; Average loss: 3.4272\nIteration: 805; Percent complete: 20.1%; Average loss: 3.4996\nIteration: 806; Percent complete: 20.2%; Average loss: 3.5687\nIteration: 807; Percent complete: 20.2%; Average loss: 3.5860\nIteration: 808; Percent complete: 20.2%; Average loss: 3.4764\nIteration: 809; Percent complete: 20.2%; Average loss: 3.5318\nIteration: 810; Percent complete: 20.2%; Average loss: 3.6166\nIteration: 811; Percent complete: 20.3%; Average loss: 3.5038\nIteration: 812; Percent complete: 20.3%; Average loss: 3.5363\nIteration: 813; Percent complete: 20.3%; Average loss: 3.6060\nIteration: 814; Percent complete: 20.3%; Average loss: 3.4456\nIteration: 815; Percent complete: 20.4%; Average loss: 3.6058\nIteration: 816; Percent complete: 20.4%; Average loss: 3.6269\nIteration: 817; Percent complete: 20.4%; Average loss: 3.4771\nIteration: 818; Percent complete: 20.4%; Average loss: 3.6748\nIteration: 819; Percent complete: 20.5%; Average loss: 3.5563\nIteration: 820; Percent complete: 20.5%; Average loss: 3.5266\nIteration: 821; Percent complete: 20.5%; Average loss: 3.3597\nIteration: 822; Percent complete: 20.5%; Average loss: 3.6572\nIteration: 823; Percent complete: 20.6%; Average loss: 3.6824\nIteration: 824; Percent complete: 20.6%; Average loss: 3.5254\nIteration: 825; Percent complete: 20.6%; Average loss: 3.5064\nIteration: 826; Percent complete: 20.6%; Average loss: 3.5548\nIteration: 827; Percent complete: 20.7%; Average loss: 3.6747\nIteration: 828; Percent complete: 20.7%; Average loss: 3.6234\nIteration: 829; Percent complete: 20.7%; Average loss: 3.5850\nIteration: 830; Percent complete: 20.8%; Average loss: 3.5147\nIteration: 831; Percent complete: 20.8%; Average loss: 3.4610\nIteration: 832; Percent complete: 20.8%; Average loss: 3.5693\nIteration: 833; Percent complete: 20.8%; Average loss: 3.8091\nIteration: 834; Percent complete: 20.8%; Average loss: 3.3069\nIteration: 835; Percent complete: 20.9%; Average loss: 3.5130\nIteration: 836; Percent complete: 20.9%; Average loss: 3.4756\nIteration: 837; Percent complete: 20.9%; Average loss: 3.8728\nIteration: 838; Percent complete: 20.9%; Average loss: 3.7524\nIteration: 839; Percent complete: 21.0%; Average loss: 3.7950\nIteration: 840; Percent complete: 21.0%; Average loss: 3.5625\nIteration: 841; Percent complete: 21.0%; Average loss: 3.5366\nIteration: 842; Percent complete: 21.1%; Average loss: 3.5566\nIteration: 843; Percent complete: 21.1%; Average loss: 3.6144\nIteration: 844; Percent complete: 21.1%; Average loss: 3.5758\nIteration: 845; Percent complete: 21.1%; Average loss: 3.4494\nIteration: 846; Percent complete: 21.1%; Average loss: 3.6203\nIteration: 847; Percent complete: 21.2%; Average loss: 3.5477\nIteration: 848; Percent complete: 21.2%; Average loss: 3.5711\nIteration: 849; Percent complete: 21.2%; Average loss: 3.4897\nIteration: 850; Percent complete: 21.2%; Average loss: 3.3131\nIteration: 851; Percent complete: 21.3%; Average loss: 3.3074\nIteration: 852; Percent complete: 21.3%; Average loss: 3.6017\nIteration: 853; Percent complete: 21.3%; Average loss: 3.5917\nIteration: 854; Percent complete: 21.3%; Average loss: 3.6324\nIteration: 855; Percent complete: 21.4%; Average loss: 3.7363\nIteration: 856; Percent complete: 21.4%; Average loss: 3.5813\nIteration: 857; Percent complete: 21.4%; Average loss: 3.4551\nIteration: 858; Percent complete: 21.4%; Average loss: 3.4782\nIteration: 859; Percent complete: 21.5%; Average loss: 3.6305\nIteration: 860; Percent complete: 21.5%; Average loss: 3.4623\nIteration: 861; Percent complete: 21.5%; Average loss: 3.2745\nIteration: 862; Percent complete: 21.6%; Average loss: 3.5245\nIteration: 863; Percent complete: 21.6%; Average loss: 3.5209\nIteration: 864; Percent complete: 21.6%; Average loss: 3.8349\nIteration: 865; Percent complete: 21.6%; Average loss: 3.5156\nIteration: 866; Percent complete: 21.6%; Average loss: 3.6910\nIteration: 867; Percent complete: 21.7%; Average loss: 3.4819\nIteration: 868; Percent complete: 21.7%; Average loss: 3.3324\nIteration: 869; Percent complete: 21.7%; Average loss: 3.6479\nIteration: 870; Percent complete: 21.8%; Average loss: 3.3454\nIteration: 871; Percent complete: 21.8%; Average loss: 3.3105\nIteration: 872; Percent complete: 21.8%; Average loss: 4.0391\nIteration: 873; Percent complete: 21.8%; Average loss: 3.6339\nIteration: 874; Percent complete: 21.9%; Average loss: 3.4885\nIteration: 875; Percent complete: 21.9%; Average loss: 3.4326\nIteration: 876; Percent complete: 21.9%; Average loss: 3.5411\nIteration: 877; Percent complete: 21.9%; Average loss: 3.4910\nIteration: 878; Percent complete: 21.9%; Average loss: 3.4183\nIteration: 879; Percent complete: 22.0%; Average loss: 3.5346\nIteration: 880; Percent complete: 22.0%; Average loss: 3.5197\nIteration: 881; Percent complete: 22.0%; Average loss: 3.4588\nIteration: 882; Percent complete: 22.1%; Average loss: 3.4408\nIteration: 883; Percent complete: 22.1%; Average loss: 3.4083\nIteration: 884; Percent complete: 22.1%; Average loss: 3.4932\nIteration: 885; Percent complete: 22.1%; Average loss: 3.4478\nIteration: 886; Percent complete: 22.1%; Average loss: 3.3997\nIteration: 887; Percent complete: 22.2%; Average loss: 3.4716\nIteration: 888; Percent complete: 22.2%; Average loss: 3.3864\nIteration: 889; Percent complete: 22.2%; Average loss: 3.2526\nIteration: 890; Percent complete: 22.2%; Average loss: 3.5135\nIteration: 891; Percent complete: 22.3%; Average loss: 3.4946\nIteration: 892; Percent complete: 22.3%; Average loss: 3.6342\nIteration: 893; Percent complete: 22.3%; Average loss: 3.4611\nIteration: 894; Percent complete: 22.4%; Average loss: 3.2956\nIteration: 895; Percent complete: 22.4%; Average loss: 3.7140\nIteration: 896; Percent complete: 22.4%; Average loss: 3.5006\nIteration: 897; Percent complete: 22.4%; Average loss: 3.3218\nIteration: 898; Percent complete: 22.4%; Average loss: 3.5431\nIteration: 899; Percent complete: 22.5%; Average loss: 3.5111\nIteration: 900; Percent complete: 22.5%; Average loss: 3.3644\nIteration: 901; Percent complete: 22.5%; Average loss: 3.5641\nIteration: 902; Percent complete: 22.6%; Average loss: 3.5347\nIteration: 903; Percent complete: 22.6%; Average loss: 3.4106\nIteration: 904; Percent complete: 22.6%; Average loss: 3.2395\nIteration: 905; Percent complete: 22.6%; Average loss: 3.5516\nIteration: 906; Percent complete: 22.7%; Average loss: 3.4027\nIteration: 907; Percent complete: 22.7%; Average loss: 3.4771\nIteration: 908; Percent complete: 22.7%; Average loss: 3.5469\nIteration: 909; Percent complete: 22.7%; Average loss: 3.6105\nIteration: 910; Percent complete: 22.8%; Average loss: 3.3355\nIteration: 911; Percent complete: 22.8%; Average loss: 3.7571\nIteration: 912; Percent complete: 22.8%; Average loss: 3.2348\nIteration: 913; Percent complete: 22.8%; Average loss: 3.3009\nIteration: 914; Percent complete: 22.9%; Average loss: 3.5246\nIteration: 915; Percent complete: 22.9%; Average loss: 3.7903\nIteration: 916; Percent complete: 22.9%; Average loss: 3.6667\nIteration: 917; Percent complete: 22.9%; Average loss: 3.6969\nIteration: 918; Percent complete: 22.9%; Average loss: 3.5682\nIteration: 919; Percent complete: 23.0%; Average loss: 3.6881\nIteration: 920; Percent complete: 23.0%; Average loss: 3.4023\nIteration: 921; Percent complete: 23.0%; Average loss: 3.3637\nIteration: 922; Percent complete: 23.1%; Average loss: 3.5422\nIteration: 923; Percent complete: 23.1%; Average loss: 3.5170\nIteration: 924; Percent complete: 23.1%; Average loss: 3.3203\nIteration: 925; Percent complete: 23.1%; Average loss: 3.7206\nIteration: 926; Percent complete: 23.2%; Average loss: 3.3674\nIteration: 927; Percent complete: 23.2%; Average loss: 3.6073\nIteration: 928; Percent complete: 23.2%; Average loss: 3.3314\nIteration: 929; Percent complete: 23.2%; Average loss: 3.5095\nIteration: 930; Percent complete: 23.2%; Average loss: 3.3633\nIteration: 931; Percent complete: 23.3%; Average loss: 3.5484\nIteration: 932; Percent complete: 23.3%; Average loss: 3.6314\nIteration: 933; Percent complete: 23.3%; Average loss: 3.5357\nIteration: 934; Percent complete: 23.4%; Average loss: 3.1989\nIteration: 935; Percent complete: 23.4%; Average loss: 3.6343\nIteration: 936; Percent complete: 23.4%; Average loss: 3.6223\nIteration: 937; Percent complete: 23.4%; Average loss: 3.4807\nIteration: 938; Percent complete: 23.4%; Average loss: 3.1557\nIteration: 939; Percent complete: 23.5%; Average loss: 3.6629\nIteration: 940; Percent complete: 23.5%; Average loss: 3.4249\nIteration: 941; Percent complete: 23.5%; Average loss: 3.2733\nIteration: 942; Percent complete: 23.5%; Average loss: 3.5663\nIteration: 943; Percent complete: 23.6%; Average loss: 3.5368\nIteration: 944; Percent complete: 23.6%; Average loss: 3.5358\nIteration: 945; Percent complete: 23.6%; Average loss: 3.7935\nIteration: 946; Percent complete: 23.6%; Average loss: 3.5126\nIteration: 947; Percent complete: 23.7%; Average loss: 3.5462\nIteration: 948; Percent complete: 23.7%; Average loss: 3.6159\nIteration: 949; Percent complete: 23.7%; Average loss: 3.7364\nIteration: 950; Percent complete: 23.8%; Average loss: 3.4376\nIteration: 951; Percent complete: 23.8%; Average loss: 3.4882\nIteration: 952; Percent complete: 23.8%; Average loss: 3.5758\nIteration: 953; Percent complete: 23.8%; Average loss: 3.4459\nIteration: 954; Percent complete: 23.8%; Average loss: 3.4093\nIteration: 955; Percent complete: 23.9%; Average loss: 3.4188\nIteration: 956; Percent complete: 23.9%; Average loss: 3.5076\nIteration: 957; Percent complete: 23.9%; Average loss: 3.5953\nIteration: 958; Percent complete: 23.9%; Average loss: 3.4097\nIteration: 959; Percent complete: 24.0%; Average loss: 3.3243\nIteration: 960; Percent complete: 24.0%; Average loss: 3.5854\nIteration: 961; Percent complete: 24.0%; Average loss: 3.4546\nIteration: 962; Percent complete: 24.1%; Average loss: 3.4168\nIteration: 963; Percent complete: 24.1%; Average loss: 3.6032\nIteration: 964; Percent complete: 24.1%; Average loss: 3.7606\nIteration: 965; Percent complete: 24.1%; Average loss: 3.3680\nIteration: 966; Percent complete: 24.1%; Average loss: 3.8552\nIteration: 967; Percent complete: 24.2%; Average loss: 3.2908\nIteration: 968; Percent complete: 24.2%; Average loss: 3.3041\nIteration: 969; Percent complete: 24.2%; Average loss: 3.5080\nIteration: 970; Percent complete: 24.2%; Average loss: 3.7195\nIteration: 971; Percent complete: 24.3%; Average loss: 3.4690\nIteration: 972; Percent complete: 24.3%; Average loss: 3.5404\nIteration: 973; Percent complete: 24.3%; Average loss: 3.4085\nIteration: 974; Percent complete: 24.3%; Average loss: 3.5561\nIteration: 975; Percent complete: 24.4%; Average loss: 3.2768\nIteration: 976; Percent complete: 24.4%; Average loss: 3.3640\nIteration: 977; Percent complete: 24.4%; Average loss: 3.5625\nIteration: 978; Percent complete: 24.4%; Average loss: 3.5869\nIteration: 979; Percent complete: 24.5%; Average loss: 3.4326\nIteration: 980; Percent complete: 24.5%; Average loss: 3.6266\nIteration: 981; Percent complete: 24.5%; Average loss: 3.7757\nIteration: 982; Percent complete: 24.6%; Average loss: 3.5794\nIteration: 983; Percent complete: 24.6%; Average loss: 3.1479\nIteration: 984; Percent complete: 24.6%; Average loss: 3.3369\nIteration: 985; Percent complete: 24.6%; Average loss: 3.5658\nIteration: 986; Percent complete: 24.6%; Average loss: 3.4820\nIteration: 987; Percent complete: 24.7%; Average loss: 3.4292\nIteration: 988; Percent complete: 24.7%; Average loss: 3.3320\nIteration: 989; Percent complete: 24.7%; Average loss: 3.6520\nIteration: 990; Percent complete: 24.8%; Average loss: 3.6536\nIteration: 991; Percent complete: 24.8%; Average loss: 3.2920\nIteration: 992; Percent complete: 24.8%; Average loss: 3.6517\nIteration: 993; Percent complete: 24.8%; Average loss: 3.3644\nIteration: 994; Percent complete: 24.9%; Average loss: 3.1623\nIteration: 995; Percent complete: 24.9%; Average loss: 3.3335\nIteration: 996; Percent complete: 24.9%; Average loss: 3.3717\nIteration: 997; Percent complete: 24.9%; Average loss: 3.5632\nIteration: 998; Percent complete: 24.9%; Average loss: 3.5291\nIteration: 999; Percent complete: 25.0%; Average loss: 3.3928\nIteration: 1000; Percent complete: 25.0%; Average loss: 3.6348\nIteration: 1001; Percent complete: 25.0%; Average loss: 3.6771\nIteration: 1002; Percent complete: 25.1%; Average loss: 3.5686\nIteration: 1003; Percent complete: 25.1%; Average loss: 3.5131\nIteration: 1004; Percent complete: 25.1%; Average loss: 3.2768\nIteration: 1005; Percent complete: 25.1%; Average loss: 3.3036\nIteration: 1006; Percent complete: 25.1%; Average loss: 3.1914\nIteration: 1007; Percent complete: 25.2%; Average loss: 3.5853\nIteration: 1008; Percent complete: 25.2%; Average loss: 3.4067\nIteration: 1009; Percent complete: 25.2%; Average loss: 3.4465\nIteration: 1010; Percent complete: 25.2%; Average loss: 3.3915\nIteration: 1011; Percent complete: 25.3%; Average loss: 3.6704\nIteration: 1012; Percent complete: 25.3%; Average loss: 3.6072\nIteration: 1013; Percent complete: 25.3%; Average loss: 3.7784\nIteration: 1014; Percent complete: 25.4%; Average loss: 3.5209\nIteration: 1015; Percent complete: 25.4%; Average loss: 3.6793\nIteration: 1016; Percent complete: 25.4%; Average loss: 3.5642\nIteration: 1017; Percent complete: 25.4%; Average loss: 3.4621\nIteration: 1018; Percent complete: 25.4%; Average loss: 3.6521\nIteration: 1019; Percent complete: 25.5%; Average loss: 3.2577\nIteration: 1020; Percent complete: 25.5%; Average loss: 3.2888\nIteration: 1021; Percent complete: 25.5%; Average loss: 3.7111\nIteration: 1022; Percent complete: 25.6%; Average loss: 3.2402\nIteration: 1023; Percent complete: 25.6%; Average loss: 3.4774\nIteration: 1024; Percent complete: 25.6%; Average loss: 3.2906\nIteration: 1025; Percent complete: 25.6%; Average loss: 3.5339\nIteration: 1026; Percent complete: 25.7%; Average loss: 3.5501\nIteration: 1027; Percent complete: 25.7%; Average loss: 3.5857\nIteration: 1028; Percent complete: 25.7%; Average loss: 3.1367\nIteration: 1029; Percent complete: 25.7%; Average loss: 3.4622\nIteration: 1030; Percent complete: 25.8%; Average loss: 3.6280\nIteration: 1031; Percent complete: 25.8%; Average loss: 3.4496\nIteration: 1032; Percent complete: 25.8%; Average loss: 3.7014\nIteration: 1033; Percent complete: 25.8%; Average loss: 3.4135\nIteration: 1034; Percent complete: 25.9%; Average loss: 3.3398\nIteration: 1035; Percent complete: 25.9%; Average loss: 3.3736\nIteration: 1036; Percent complete: 25.9%; Average loss: 3.2549\nIteration: 1037; Percent complete: 25.9%; Average loss: 3.7597\nIteration: 1038; Percent complete: 25.9%; Average loss: 3.5945\nIteration: 1039; Percent complete: 26.0%; Average loss: 3.2450\nIteration: 1040; Percent complete: 26.0%; Average loss: 3.5769\nIteration: 1041; Percent complete: 26.0%; Average loss: 3.4946\nIteration: 1042; Percent complete: 26.1%; Average loss: 3.5578\nIteration: 1043; Percent complete: 26.1%; Average loss: 3.4989\nIteration: 1044; Percent complete: 26.1%; Average loss: 3.2846\nIteration: 1045; Percent complete: 26.1%; Average loss: 3.6153\nIteration: 1046; Percent complete: 26.2%; Average loss: 3.3283\nIteration: 1047; Percent complete: 26.2%; Average loss: 3.5926\nIteration: 1048; Percent complete: 26.2%; Average loss: 3.7879\nIteration: 1049; Percent complete: 26.2%; Average loss: 3.4623\nIteration: 1050; Percent complete: 26.2%; Average loss: 3.7192\nIteration: 1051; Percent complete: 26.3%; Average loss: 3.3022\nIteration: 1052; Percent complete: 26.3%; Average loss: 3.5218\nIteration: 1053; Percent complete: 26.3%; Average loss: 3.5046\nIteration: 1054; Percent complete: 26.4%; Average loss: 3.3760\nIteration: 1055; Percent complete: 26.4%; Average loss: 3.1356\nIteration: 1056; Percent complete: 26.4%; Average loss: 3.2884\nIteration: 1057; Percent complete: 26.4%; Average loss: 3.5108\nIteration: 1058; Percent complete: 26.5%; Average loss: 3.4557\nIteration: 1059; Percent complete: 26.5%; Average loss: 3.5436\nIteration: 1060; Percent complete: 26.5%; Average loss: 3.5693\nIteration: 1061; Percent complete: 26.5%; Average loss: 3.5064\nIteration: 1062; Percent complete: 26.6%; Average loss: 3.3611\nIteration: 1063; Percent complete: 26.6%; Average loss: 3.4544\nIteration: 1064; Percent complete: 26.6%; Average loss: 3.3088\nIteration: 1065; Percent complete: 26.6%; Average loss: 3.2557\nIteration: 1066; Percent complete: 26.7%; Average loss: 3.3062\nIteration: 1067; Percent complete: 26.7%; Average loss: 3.4839\nIteration: 1068; Percent complete: 26.7%; Average loss: 3.5416\nIteration: 1069; Percent complete: 26.7%; Average loss: 3.1785\nIteration: 1070; Percent complete: 26.8%; Average loss: 3.4883\nIteration: 1071; Percent complete: 26.8%; Average loss: 3.5369\nIteration: 1072; Percent complete: 26.8%; Average loss: 3.4972\nIteration: 1073; Percent complete: 26.8%; Average loss: 3.3897\nIteration: 1074; Percent complete: 26.9%; Average loss: 3.6285\nIteration: 1075; Percent complete: 26.9%; Average loss: 3.5913\nIteration: 1076; Percent complete: 26.9%; Average loss: 3.4550\nIteration: 1077; Percent complete: 26.9%; Average loss: 3.3749\nIteration: 1078; Percent complete: 27.0%; Average loss: 3.3398\nIteration: 1079; Percent complete: 27.0%; Average loss: 3.2432\nIteration: 1080; Percent complete: 27.0%; Average loss: 3.4005\nIteration: 1081; Percent complete: 27.0%; Average loss: 3.5253\nIteration: 1082; Percent complete: 27.1%; Average loss: 3.3796\nIteration: 1083; Percent complete: 27.1%; Average loss: 3.4913\nIteration: 1084; Percent complete: 27.1%; Average loss: 3.6120\nIteration: 1085; Percent complete: 27.1%; Average loss: 3.3657\nIteration: 1086; Percent complete: 27.2%; Average loss: 3.3530\nIteration: 1087; Percent complete: 27.2%; Average loss: 3.5913\nIteration: 1088; Percent complete: 27.2%; Average loss: 3.5565\nIteration: 1089; Percent complete: 27.2%; Average loss: 3.3776\nIteration: 1090; Percent complete: 27.3%; Average loss: 3.5044\nIteration: 1091; Percent complete: 27.3%; Average loss: 3.7840\nIteration: 1092; Percent complete: 27.3%; Average loss: 3.5225\nIteration: 1093; Percent complete: 27.3%; Average loss: 3.4997\nIteration: 1094; Percent complete: 27.4%; Average loss: 3.5361\nIteration: 1095; Percent complete: 27.4%; Average loss: 3.5226\nIteration: 1096; Percent complete: 27.4%; Average loss: 3.0887\nIteration: 1097; Percent complete: 27.4%; Average loss: 3.7046\nIteration: 1098; Percent complete: 27.5%; Average loss: 3.4677\nIteration: 1099; Percent complete: 27.5%; Average loss: 3.2531\nIteration: 1100; Percent complete: 27.5%; Average loss: 3.1113\nIteration: 1101; Percent complete: 27.5%; Average loss: 3.5559\nIteration: 1102; Percent complete: 27.6%; Average loss: 3.4395\nIteration: 1103; Percent complete: 27.6%; Average loss: 3.2510\nIteration: 1104; Percent complete: 27.6%; Average loss: 3.1961\nIteration: 1105; Percent complete: 27.6%; Average loss: 3.3525\nIteration: 1106; Percent complete: 27.7%; Average loss: 3.7204\nIteration: 1107; Percent complete: 27.7%; Average loss: 3.4105\nIteration: 1108; Percent complete: 27.7%; Average loss: 3.3252\nIteration: 1109; Percent complete: 27.7%; Average loss: 3.3215\nIteration: 1110; Percent complete: 27.8%; Average loss: 3.4719\nIteration: 1111; Percent complete: 27.8%; Average loss: 3.6134\nIteration: 1112; Percent complete: 27.8%; Average loss: 3.4097\nIteration: 1113; Percent complete: 27.8%; Average loss: 3.4987\nIteration: 1114; Percent complete: 27.9%; Average loss: 3.3987\nIteration: 1115; Percent complete: 27.9%; Average loss: 3.3264\nIteration: 1116; Percent complete: 27.9%; Average loss: 3.5037\nIteration: 1117; Percent complete: 27.9%; Average loss: 3.4904\nIteration: 1118; Percent complete: 28.0%; Average loss: 3.6494\nIteration: 1119; Percent complete: 28.0%; Average loss: 3.5491\nIteration: 1120; Percent complete: 28.0%; Average loss: 3.3923\nIteration: 1121; Percent complete: 28.0%; Average loss: 3.4796\nIteration: 1122; Percent complete: 28.1%; Average loss: 3.3595\nIteration: 1123; Percent complete: 28.1%; Average loss: 3.4527\nIteration: 1124; Percent complete: 28.1%; Average loss: 3.5657\nIteration: 1125; Percent complete: 28.1%; Average loss: 3.3165\nIteration: 1126; Percent complete: 28.1%; Average loss: 3.4331\nIteration: 1127; Percent complete: 28.2%; Average loss: 3.4293\nIteration: 1128; Percent complete: 28.2%; Average loss: 3.5934\nIteration: 1129; Percent complete: 28.2%; Average loss: 3.3074\nIteration: 1130; Percent complete: 28.2%; Average loss: 3.3814\nIteration: 1131; Percent complete: 28.3%; Average loss: 3.3458\nIteration: 1132; Percent complete: 28.3%; Average loss: 3.2927\nIteration: 1133; Percent complete: 28.3%; Average loss: 3.3229\nIteration: 1134; Percent complete: 28.3%; Average loss: 3.4012\nIteration: 1135; Percent complete: 28.4%; Average loss: 3.3034\nIteration: 1136; Percent complete: 28.4%; Average loss: 3.6059\nIteration: 1137; Percent complete: 28.4%; Average loss: 3.3133\nIteration: 1138; Percent complete: 28.4%; Average loss: 3.5329\nIteration: 1139; Percent complete: 28.5%; Average loss: 3.2265\nIteration: 1140; Percent complete: 28.5%; Average loss: 3.4709\nIteration: 1141; Percent complete: 28.5%; Average loss: 3.4533\nIteration: 1142; Percent complete: 28.5%; Average loss: 3.3982\nIteration: 1143; Percent complete: 28.6%; Average loss: 3.3627\nIteration: 1144; Percent complete: 28.6%; Average loss: 3.5414\nIteration: 1145; Percent complete: 28.6%; Average loss: 3.5365\nIteration: 1146; Percent complete: 28.6%; Average loss: 3.7256\nIteration: 1147; Percent complete: 28.7%; Average loss: 3.3759\nIteration: 1148; Percent complete: 28.7%; Average loss: 3.2884\nIteration: 1149; Percent complete: 28.7%; Average loss: 3.2142\nIteration: 1150; Percent complete: 28.7%; Average loss: 3.3013\nIteration: 1151; Percent complete: 28.8%; Average loss: 3.2436\nIteration: 1152; Percent complete: 28.8%; Average loss: 3.3351\nIteration: 1153; Percent complete: 28.8%; Average loss: 3.2382\nIteration: 1154; Percent complete: 28.8%; Average loss: 3.4789\nIteration: 1155; Percent complete: 28.9%; Average loss: 3.2752\nIteration: 1156; Percent complete: 28.9%; Average loss: 3.3046\nIteration: 1157; Percent complete: 28.9%; Average loss: 3.5290\nIteration: 1158; Percent complete: 28.9%; Average loss: 3.3779\nIteration: 1159; Percent complete: 29.0%; Average loss: 3.3821\nIteration: 1160; Percent complete: 29.0%; Average loss: 3.4451\nIteration: 1161; Percent complete: 29.0%; Average loss: 3.2126\nIteration: 1162; Percent complete: 29.0%; Average loss: 3.2153\nIteration: 1163; Percent complete: 29.1%; Average loss: 3.5606\nIteration: 1164; Percent complete: 29.1%; Average loss: 3.3499\nIteration: 1165; Percent complete: 29.1%; Average loss: 3.5494\nIteration: 1166; Percent complete: 29.1%; Average loss: 3.4619\nIteration: 1167; Percent complete: 29.2%; Average loss: 3.3598\nIteration: 1168; Percent complete: 29.2%; Average loss: 3.3699\nIteration: 1169; Percent complete: 29.2%; Average loss: 3.3044\nIteration: 1170; Percent complete: 29.2%; Average loss: 3.2971\nIteration: 1171; Percent complete: 29.3%; Average loss: 3.3468\nIteration: 1172; Percent complete: 29.3%; Average loss: 3.4862\nIteration: 1173; Percent complete: 29.3%; Average loss: 3.6647\nIteration: 1174; Percent complete: 29.3%; Average loss: 3.3876\nIteration: 1175; Percent complete: 29.4%; Average loss: 3.4120\nIteration: 1176; Percent complete: 29.4%; Average loss: 3.4756\nIteration: 1177; Percent complete: 29.4%; Average loss: 3.5676\nIteration: 1178; Percent complete: 29.4%; Average loss: 3.4226\nIteration: 1179; Percent complete: 29.5%; Average loss: 3.4037\nIteration: 1180; Percent complete: 29.5%; Average loss: 3.4179\nIteration: 1181; Percent complete: 29.5%; Average loss: 3.3284\nIteration: 1182; Percent complete: 29.5%; Average loss: 3.2469\nIteration: 1183; Percent complete: 29.6%; Average loss: 3.4174\nIteration: 1184; Percent complete: 29.6%; Average loss: 3.4705\nIteration: 1185; Percent complete: 29.6%; Average loss: 3.3908\nIteration: 1186; Percent complete: 29.6%; Average loss: 3.4354\nIteration: 1187; Percent complete: 29.7%; Average loss: 3.4555\nIteration: 1188; Percent complete: 29.7%; Average loss: 3.4880\nIteration: 1189; Percent complete: 29.7%; Average loss: 3.4935\nIteration: 1190; Percent complete: 29.8%; Average loss: 3.4268\nIteration: 1191; Percent complete: 29.8%; Average loss: 3.4228\nIteration: 1192; Percent complete: 29.8%; Average loss: 3.7274\nIteration: 1193; Percent complete: 29.8%; Average loss: 3.1567\nIteration: 1194; Percent complete: 29.8%; Average loss: 3.2896\nIteration: 1195; Percent complete: 29.9%; Average loss: 3.3528\nIteration: 1196; Percent complete: 29.9%; Average loss: 3.1829\nIteration: 1197; Percent complete: 29.9%; Average loss: 3.1600\nIteration: 1198; Percent complete: 29.9%; Average loss: 3.4541\nIteration: 1199; Percent complete: 30.0%; Average loss: 3.6209\nIteration: 1200; Percent complete: 30.0%; Average loss: 3.5588\nIteration: 1201; Percent complete: 30.0%; Average loss: 3.4702\nIteration: 1202; Percent complete: 30.0%; Average loss: 3.4632\nIteration: 1203; Percent complete: 30.1%; Average loss: 3.3329\nIteration: 1204; Percent complete: 30.1%; Average loss: 3.3912\nIteration: 1205; Percent complete: 30.1%; Average loss: 3.5972\nIteration: 1206; Percent complete: 30.1%; Average loss: 3.6292\nIteration: 1207; Percent complete: 30.2%; Average loss: 3.4215\nIteration: 1208; Percent complete: 30.2%; Average loss: 3.6018\nIteration: 1209; Percent complete: 30.2%; Average loss: 3.4539\nIteration: 1210; Percent complete: 30.2%; Average loss: 3.2798\nIteration: 1211; Percent complete: 30.3%; Average loss: 3.4327\nIteration: 1212; Percent complete: 30.3%; Average loss: 3.4619\nIteration: 1213; Percent complete: 30.3%; Average loss: 3.4347\nIteration: 1214; Percent complete: 30.3%; Average loss: 3.6474\nIteration: 1215; Percent complete: 30.4%; Average loss: 3.3551\nIteration: 1216; Percent complete: 30.4%; Average loss: 3.6804\nIteration: 1217; Percent complete: 30.4%; Average loss: 3.2575\nIteration: 1218; Percent complete: 30.4%; Average loss: 3.6221\nIteration: 1219; Percent complete: 30.5%; Average loss: 3.4467\nIteration: 1220; Percent complete: 30.5%; Average loss: 3.5135\nIteration: 1221; Percent complete: 30.5%; Average loss: 3.6501\nIteration: 1222; Percent complete: 30.6%; Average loss: 3.6081\nIteration: 1223; Percent complete: 30.6%; Average loss: 3.1003\nIteration: 1224; Percent complete: 30.6%; Average loss: 3.3327\nIteration: 1225; Percent complete: 30.6%; Average loss: 3.3940\nIteration: 1226; Percent complete: 30.6%; Average loss: 3.1230\nIteration: 1227; Percent complete: 30.7%; Average loss: 3.4929\nIteration: 1228; Percent complete: 30.7%; Average loss: 3.4772\nIteration: 1229; Percent complete: 30.7%; Average loss: 3.3340\nIteration: 1230; Percent complete: 30.8%; Average loss: 3.2902\nIteration: 1231; Percent complete: 30.8%; Average loss: 3.5490\nIteration: 1232; Percent complete: 30.8%; Average loss: 3.5659\nIteration: 1233; Percent complete: 30.8%; Average loss: 3.5821\nIteration: 1234; Percent complete: 30.9%; Average loss: 3.3568\nIteration: 1235; Percent complete: 30.9%; Average loss: 3.3234\nIteration: 1236; Percent complete: 30.9%; Average loss: 3.2549\nIteration: 1237; Percent complete: 30.9%; Average loss: 3.1468\nIteration: 1238; Percent complete: 30.9%; Average loss: 3.4269\nIteration: 1239; Percent complete: 31.0%; Average loss: 3.3952\nIteration: 1240; Percent complete: 31.0%; Average loss: 3.5227\nIteration: 1241; Percent complete: 31.0%; Average loss: 3.4252\nIteration: 1242; Percent complete: 31.1%; Average loss: 3.3754\nIteration: 1243; Percent complete: 31.1%; Average loss: 3.0098\nIteration: 1244; Percent complete: 31.1%; Average loss: 3.5573\nIteration: 1245; Percent complete: 31.1%; Average loss: 3.5448\nIteration: 1246; Percent complete: 31.1%; Average loss: 3.4143\nIteration: 1247; Percent complete: 31.2%; Average loss: 3.3661\nIteration: 1248; Percent complete: 31.2%; Average loss: 3.2777\nIteration: 1249; Percent complete: 31.2%; Average loss: 3.5549\nIteration: 1250; Percent complete: 31.2%; Average loss: 3.4145\nIteration: 1251; Percent complete: 31.3%; Average loss: 3.1826\nIteration: 1252; Percent complete: 31.3%; Average loss: 3.2324\nIteration: 1253; Percent complete: 31.3%; Average loss: 3.6124\nIteration: 1254; Percent complete: 31.4%; Average loss: 3.1607\nIteration: 1255; Percent complete: 31.4%; Average loss: 3.3589\nIteration: 1256; Percent complete: 31.4%; Average loss: 3.5868\nIteration: 1257; Percent complete: 31.4%; Average loss: 3.6750\nIteration: 1258; Percent complete: 31.4%; Average loss: 3.3460\nIteration: 1259; Percent complete: 31.5%; Average loss: 3.3926\nIteration: 1260; Percent complete: 31.5%; Average loss: 3.3572\nIteration: 1261; Percent complete: 31.5%; Average loss: 3.3089\nIteration: 1262; Percent complete: 31.6%; Average loss: 3.3830\nIteration: 1263; Percent complete: 31.6%; Average loss: 3.4232\nIteration: 1264; Percent complete: 31.6%; Average loss: 3.5726\nIteration: 1265; Percent complete: 31.6%; Average loss: 3.3449\nIteration: 1266; Percent complete: 31.6%; Average loss: 3.1479\nIteration: 1267; Percent complete: 31.7%; Average loss: 3.0682\nIteration: 1268; Percent complete: 31.7%; Average loss: 3.3077\nIteration: 1269; Percent complete: 31.7%; Average loss: 3.2655\nIteration: 1270; Percent complete: 31.8%; Average loss: 3.3214\nIteration: 1271; Percent complete: 31.8%; Average loss: 3.2663\nIteration: 1272; Percent complete: 31.8%; Average loss: 3.4016\nIteration: 1273; Percent complete: 31.8%; Average loss: 3.2863\nIteration: 1274; Percent complete: 31.9%; Average loss: 3.4534\nIteration: 1275; Percent complete: 31.9%; Average loss: 3.2815\nIteration: 1276; Percent complete: 31.9%; Average loss: 3.3410\nIteration: 1277; Percent complete: 31.9%; Average loss: 3.2584\nIteration: 1278; Percent complete: 31.9%; Average loss: 3.4380\nIteration: 1279; Percent complete: 32.0%; Average loss: 3.2113\nIteration: 1280; Percent complete: 32.0%; Average loss: 3.6378\nIteration: 1281; Percent complete: 32.0%; Average loss: 3.5360\nIteration: 1282; Percent complete: 32.0%; Average loss: 3.5982\nIteration: 1283; Percent complete: 32.1%; Average loss: 3.4689\nIteration: 1284; Percent complete: 32.1%; Average loss: 3.4454\nIteration: 1285; Percent complete: 32.1%; Average loss: 3.6346\nIteration: 1286; Percent complete: 32.1%; Average loss: 3.5114\nIteration: 1287; Percent complete: 32.2%; Average loss: 3.3094\nIteration: 1288; Percent complete: 32.2%; Average loss: 3.3620\nIteration: 1289; Percent complete: 32.2%; Average loss: 3.4526\nIteration: 1290; Percent complete: 32.2%; Average loss: 3.4174\nIteration: 1291; Percent complete: 32.3%; Average loss: 3.4755\nIteration: 1292; Percent complete: 32.3%; Average loss: 3.7072\nIteration: 1293; Percent complete: 32.3%; Average loss: 3.2736\nIteration: 1294; Percent complete: 32.4%; Average loss: 3.2059\nIteration: 1295; Percent complete: 32.4%; Average loss: 3.1776\nIteration: 1296; Percent complete: 32.4%; Average loss: 3.4062\nIteration: 1297; Percent complete: 32.4%; Average loss: 3.3500\nIteration: 1298; Percent complete: 32.5%; Average loss: 3.2075\nIteration: 1299; Percent complete: 32.5%; Average loss: 3.2946\nIteration: 1300; Percent complete: 32.5%; Average loss: 3.5753\nIteration: 1301; Percent complete: 32.5%; Average loss: 3.4039\nIteration: 1302; Percent complete: 32.6%; Average loss: 3.4058\nIteration: 1303; Percent complete: 32.6%; Average loss: 3.2776\nIteration: 1304; Percent complete: 32.6%; Average loss: 3.2138\nIteration: 1305; Percent complete: 32.6%; Average loss: 3.4916\nIteration: 1306; Percent complete: 32.6%; Average loss: 3.4286\nIteration: 1307; Percent complete: 32.7%; Average loss: 3.3787\nIteration: 1308; Percent complete: 32.7%; Average loss: 3.2695\nIteration: 1309; Percent complete: 32.7%; Average loss: 3.6281\nIteration: 1310; Percent complete: 32.8%; Average loss: 3.1362\nIteration: 1311; Percent complete: 32.8%; Average loss: 3.1524\nIteration: 1312; Percent complete: 32.8%; Average loss: 3.2623\nIteration: 1313; Percent complete: 32.8%; Average loss: 3.3925\nIteration: 1314; Percent complete: 32.9%; Average loss: 3.2296\nIteration: 1315; Percent complete: 32.9%; Average loss: 3.2815\nIteration: 1316; Percent complete: 32.9%; Average loss: 3.4823\nIteration: 1317; Percent complete: 32.9%; Average loss: 3.3303\nIteration: 1318; Percent complete: 33.0%; Average loss: 3.2823\nIteration: 1319; Percent complete: 33.0%; Average loss: 3.2352\nIteration: 1320; Percent complete: 33.0%; Average loss: 3.4060\nIteration: 1321; Percent complete: 33.0%; Average loss: 3.5570\nIteration: 1322; Percent complete: 33.1%; Average loss: 3.5336\nIteration: 1323; Percent complete: 33.1%; Average loss: 3.5010\nIteration: 1324; Percent complete: 33.1%; Average loss: 3.4801\nIteration: 1325; Percent complete: 33.1%; Average loss: 3.3461\nIteration: 1326; Percent complete: 33.1%; Average loss: 3.3054\nIteration: 1327; Percent complete: 33.2%; Average loss: 3.4474\nIteration: 1328; Percent complete: 33.2%; Average loss: 3.2099\nIteration: 1329; Percent complete: 33.2%; Average loss: 3.6360\nIteration: 1330; Percent complete: 33.2%; Average loss: 3.4797\nIteration: 1331; Percent complete: 33.3%; Average loss: 3.4388\nIteration: 1332; Percent complete: 33.3%; Average loss: 3.2646\nIteration: 1333; Percent complete: 33.3%; Average loss: 3.2457\nIteration: 1334; Percent complete: 33.4%; Average loss: 3.3683\nIteration: 1335; Percent complete: 33.4%; Average loss: 3.6668\nIteration: 1336; Percent complete: 33.4%; Average loss: 3.5264\nIteration: 1337; Percent complete: 33.4%; Average loss: 3.2493\nIteration: 1338; Percent complete: 33.5%; Average loss: 3.3856\nIteration: 1339; Percent complete: 33.5%; Average loss: 3.3277\nIteration: 1340; Percent complete: 33.5%; Average loss: 3.2536\nIteration: 1341; Percent complete: 33.5%; Average loss: 3.4442\nIteration: 1342; Percent complete: 33.6%; Average loss: 3.3707\nIteration: 1343; Percent complete: 33.6%; Average loss: 3.4906\nIteration: 1344; Percent complete: 33.6%; Average loss: 3.1717\nIteration: 1345; Percent complete: 33.6%; Average loss: 3.4644\nIteration: 1346; Percent complete: 33.7%; Average loss: 3.3651\nIteration: 1347; Percent complete: 33.7%; Average loss: 3.4759\nIteration: 1348; Percent complete: 33.7%; Average loss: 3.1417\nIteration: 1349; Percent complete: 33.7%; Average loss: 3.4498\nIteration: 1350; Percent complete: 33.8%; Average loss: 3.1310\nIteration: 1351; Percent complete: 33.8%; Average loss: 3.3874\nIteration: 1352; Percent complete: 33.8%; Average loss: 3.2350\nIteration: 1353; Percent complete: 33.8%; Average loss: 3.6097\nIteration: 1354; Percent complete: 33.9%; Average loss: 3.0718\nIteration: 1355; Percent complete: 33.9%; Average loss: 3.5914\nIteration: 1356; Percent complete: 33.9%; Average loss: 3.4075\nIteration: 1357; Percent complete: 33.9%; Average loss: 3.5065\nIteration: 1358; Percent complete: 34.0%; Average loss: 3.4397\nIteration: 1359; Percent complete: 34.0%; Average loss: 3.0722\nIteration: 1360; Percent complete: 34.0%; Average loss: 3.2159\nIteration: 1361; Percent complete: 34.0%; Average loss: 3.1804\nIteration: 1362; Percent complete: 34.1%; Average loss: 3.5011\nIteration: 1363; Percent complete: 34.1%; Average loss: 3.5369\nIteration: 1364; Percent complete: 34.1%; Average loss: 3.4586\nIteration: 1365; Percent complete: 34.1%; Average loss: 3.2577\nIteration: 1366; Percent complete: 34.2%; Average loss: 3.1546\nIteration: 1367; Percent complete: 34.2%; Average loss: 2.8832\nIteration: 1368; Percent complete: 34.2%; Average loss: 3.1846\nIteration: 1369; Percent complete: 34.2%; Average loss: 3.3756\nIteration: 1370; Percent complete: 34.2%; Average loss: 3.4114\nIteration: 1371; Percent complete: 34.3%; Average loss: 3.6374\nIteration: 1372; Percent complete: 34.3%; Average loss: 3.4259\nIteration: 1373; Percent complete: 34.3%; Average loss: 3.2152\nIteration: 1374; Percent complete: 34.4%; Average loss: 3.3367\nIteration: 1375; Percent complete: 34.4%; Average loss: 3.3071\nIteration: 1376; Percent complete: 34.4%; Average loss: 3.1448\nIteration: 1377; Percent complete: 34.4%; Average loss: 3.3181\nIteration: 1378; Percent complete: 34.4%; Average loss: 3.5397\nIteration: 1379; Percent complete: 34.5%; Average loss: 3.4002\nIteration: 1380; Percent complete: 34.5%; Average loss: 3.3638\nIteration: 1381; Percent complete: 34.5%; Average loss: 3.2679\nIteration: 1382; Percent complete: 34.5%; Average loss: 3.4648\nIteration: 1383; Percent complete: 34.6%; Average loss: 3.3090\nIteration: 1384; Percent complete: 34.6%; Average loss: 3.4131\nIteration: 1385; Percent complete: 34.6%; Average loss: 3.2466\nIteration: 1386; Percent complete: 34.6%; Average loss: 3.3302\nIteration: 1387; Percent complete: 34.7%; Average loss: 3.3405\nIteration: 1388; Percent complete: 34.7%; Average loss: 3.2009\nIteration: 1389; Percent complete: 34.7%; Average loss: 3.2451\nIteration: 1390; Percent complete: 34.8%; Average loss: 3.1908\nIteration: 1391; Percent complete: 34.8%; Average loss: 3.8399\nIteration: 1392; Percent complete: 34.8%; Average loss: 3.2889\nIteration: 1393; Percent complete: 34.8%; Average loss: 3.2498\nIteration: 1394; Percent complete: 34.8%; Average loss: 3.4763\nIteration: 1395; Percent complete: 34.9%; Average loss: 3.5217\nIteration: 1396; Percent complete: 34.9%; Average loss: 3.2808\nIteration: 1397; Percent complete: 34.9%; Average loss: 3.4464\nIteration: 1398; Percent complete: 34.9%; Average loss: 3.3188\nIteration: 1399; Percent complete: 35.0%; Average loss: 3.3163\nIteration: 1400; Percent complete: 35.0%; Average loss: 3.3658\nIteration: 1401; Percent complete: 35.0%; Average loss: 3.2799\nIteration: 1402; Percent complete: 35.0%; Average loss: 3.2800\nIteration: 1403; Percent complete: 35.1%; Average loss: 3.3205\nIteration: 1404; Percent complete: 35.1%; Average loss: 3.2261\nIteration: 1405; Percent complete: 35.1%; Average loss: 3.4320\nIteration: 1406; Percent complete: 35.1%; Average loss: 3.5405\nIteration: 1407; Percent complete: 35.2%; Average loss: 3.2351\nIteration: 1408; Percent complete: 35.2%; Average loss: 3.0752\nIteration: 1409; Percent complete: 35.2%; Average loss: 3.5368\nIteration: 1410; Percent complete: 35.2%; Average loss: 3.1923\nIteration: 1411; Percent complete: 35.3%; Average loss: 3.5836\nIteration: 1412; Percent complete: 35.3%; Average loss: 3.4298\nIteration: 1413; Percent complete: 35.3%; Average loss: 3.2915\nIteration: 1414; Percent complete: 35.4%; Average loss: 3.4378\nIteration: 1415; Percent complete: 35.4%; Average loss: 3.3322\nIteration: 1416; Percent complete: 35.4%; Average loss: 3.4186\nIteration: 1417; Percent complete: 35.4%; Average loss: 3.3884\nIteration: 1418; Percent complete: 35.4%; Average loss: 3.2909\nIteration: 1419; Percent complete: 35.5%; Average loss: 3.1109\nIteration: 1420; Percent complete: 35.5%; Average loss: 3.4333\nIteration: 1421; Percent complete: 35.5%; Average loss: 3.4672\nIteration: 1422; Percent complete: 35.5%; Average loss: 3.4790\nIteration: 1423; Percent complete: 35.6%; Average loss: 3.4689\nIteration: 1424; Percent complete: 35.6%; Average loss: 3.3264\nIteration: 1425; Percent complete: 35.6%; Average loss: 3.4299\nIteration: 1426; Percent complete: 35.6%; Average loss: 3.2961\nIteration: 1427; Percent complete: 35.7%; Average loss: 3.2719\nIteration: 1428; Percent complete: 35.7%; Average loss: 3.0689\nIteration: 1429; Percent complete: 35.7%; Average loss: 3.3420\nIteration: 1430; Percent complete: 35.8%; Average loss: 3.1799\nIteration: 1431; Percent complete: 35.8%; Average loss: 3.2689\nIteration: 1432; Percent complete: 35.8%; Average loss: 3.3857\nIteration: 1433; Percent complete: 35.8%; Average loss: 3.3272\nIteration: 1434; Percent complete: 35.9%; Average loss: 3.4777\nIteration: 1435; Percent complete: 35.9%; Average loss: 3.2052\nIteration: 1436; Percent complete: 35.9%; Average loss: 3.0931\nIteration: 1437; Percent complete: 35.9%; Average loss: 3.3477\nIteration: 1438; Percent complete: 35.9%; Average loss: 3.3405\nIteration: 1439; Percent complete: 36.0%; Average loss: 2.9625\nIteration: 1440; Percent complete: 36.0%; Average loss: 3.3911\nIteration: 1441; Percent complete: 36.0%; Average loss: 3.3985\nIteration: 1442; Percent complete: 36.0%; Average loss: 3.3393\nIteration: 1443; Percent complete: 36.1%; Average loss: 3.2975\nIteration: 1444; Percent complete: 36.1%; Average loss: 3.5299\nIteration: 1445; Percent complete: 36.1%; Average loss: 3.1285\nIteration: 1446; Percent complete: 36.1%; Average loss: 3.3325\nIteration: 1447; Percent complete: 36.2%; Average loss: 3.0779\nIteration: 1448; Percent complete: 36.2%; Average loss: 3.4414\nIteration: 1449; Percent complete: 36.2%; Average loss: 3.4564\nIteration: 1450; Percent complete: 36.2%; Average loss: 3.3412\nIteration: 1451; Percent complete: 36.3%; Average loss: 3.1260\nIteration: 1452; Percent complete: 36.3%; Average loss: 3.2171\nIteration: 1453; Percent complete: 36.3%; Average loss: 3.4805\nIteration: 1454; Percent complete: 36.4%; Average loss: 3.3036\nIteration: 1455; Percent complete: 36.4%; Average loss: 3.4416\nIteration: 1456; Percent complete: 36.4%; Average loss: 3.2486\nIteration: 1457; Percent complete: 36.4%; Average loss: 3.2477\nIteration: 1458; Percent complete: 36.4%; Average loss: 3.3859\nIteration: 1459; Percent complete: 36.5%; Average loss: 3.2678\nIteration: 1460; Percent complete: 36.5%; Average loss: 3.4226\nIteration: 1461; Percent complete: 36.5%; Average loss: 3.2277\nIteration: 1462; Percent complete: 36.5%; Average loss: 3.4146\nIteration: 1463; Percent complete: 36.6%; Average loss: 3.3722\nIteration: 1464; Percent complete: 36.6%; Average loss: 3.3234\nIteration: 1465; Percent complete: 36.6%; Average loss: 3.2484\nIteration: 1466; Percent complete: 36.6%; Average loss: 3.0866\nIteration: 1467; Percent complete: 36.7%; Average loss: 3.2420\nIteration: 1468; Percent complete: 36.7%; Average loss: 3.3717\nIteration: 1469; Percent complete: 36.7%; Average loss: 3.1251\nIteration: 1470; Percent complete: 36.8%; Average loss: 3.2721\nIteration: 1471; Percent complete: 36.8%; Average loss: 3.4208\nIteration: 1472; Percent complete: 36.8%; Average loss: 3.2644\nIteration: 1473; Percent complete: 36.8%; Average loss: 3.2783\nIteration: 1474; Percent complete: 36.9%; Average loss: 3.5039\nIteration: 1475; Percent complete: 36.9%; Average loss: 3.5206\nIteration: 1476; Percent complete: 36.9%; Average loss: 3.3750\nIteration: 1477; Percent complete: 36.9%; Average loss: 3.2576\nIteration: 1478; Percent complete: 37.0%; Average loss: 3.1356\nIteration: 1479; Percent complete: 37.0%; Average loss: 3.2304\nIteration: 1480; Percent complete: 37.0%; Average loss: 3.1512\nIteration: 1481; Percent complete: 37.0%; Average loss: 3.1496\nIteration: 1482; Percent complete: 37.0%; Average loss: 3.2870\nIteration: 1483; Percent complete: 37.1%; Average loss: 3.1519\nIteration: 1484; Percent complete: 37.1%; Average loss: 3.1866\nIteration: 1485; Percent complete: 37.1%; Average loss: 3.2960\nIteration: 1486; Percent complete: 37.1%; Average loss: 3.2875\nIteration: 1487; Percent complete: 37.2%; Average loss: 3.2572\nIteration: 1488; Percent complete: 37.2%; Average loss: 3.3864\nIteration: 1489; Percent complete: 37.2%; Average loss: 3.4481\nIteration: 1490; Percent complete: 37.2%; Average loss: 3.2902\nIteration: 1491; Percent complete: 37.3%; Average loss: 3.1421\nIteration: 1492; Percent complete: 37.3%; Average loss: 3.4686\nIteration: 1493; Percent complete: 37.3%; Average loss: 3.2914\nIteration: 1494; Percent complete: 37.4%; Average loss: 3.2767\nIteration: 1495; Percent complete: 37.4%; Average loss: 3.5083\nIteration: 1496; Percent complete: 37.4%; Average loss: 2.8587\nIteration: 1497; Percent complete: 37.4%; Average loss: 3.4568\nIteration: 1498; Percent complete: 37.5%; Average loss: 3.3293\nIteration: 1499; Percent complete: 37.5%; Average loss: 3.3830\nIteration: 1500; Percent complete: 37.5%; Average loss: 3.5454\nIteration: 1501; Percent complete: 37.5%; Average loss: 3.3404\nIteration: 1502; Percent complete: 37.5%; Average loss: 3.2801\nIteration: 1503; Percent complete: 37.6%; Average loss: 3.2445\nIteration: 1504; Percent complete: 37.6%; Average loss: 3.1177\nIteration: 1505; Percent complete: 37.6%; Average loss: 3.3300\nIteration: 1506; Percent complete: 37.6%; Average loss: 3.2950\nIteration: 1507; Percent complete: 37.7%; Average loss: 3.2538\nIteration: 1508; Percent complete: 37.7%; Average loss: 3.2791\nIteration: 1509; Percent complete: 37.7%; Average loss: 3.1494\nIteration: 1510; Percent complete: 37.8%; Average loss: 3.2380\nIteration: 1511; Percent complete: 37.8%; Average loss: 3.2233\nIteration: 1512; Percent complete: 37.8%; Average loss: 3.3395\nIteration: 1513; Percent complete: 37.8%; Average loss: 3.2851\nIteration: 1514; Percent complete: 37.9%; Average loss: 3.2371\nIteration: 1515; Percent complete: 37.9%; Average loss: 3.1596\nIteration: 1516; Percent complete: 37.9%; Average loss: 3.1666\nIteration: 1517; Percent complete: 37.9%; Average loss: 3.3287\nIteration: 1518; Percent complete: 38.0%; Average loss: 3.3172\nIteration: 1519; Percent complete: 38.0%; Average loss: 3.2097\nIteration: 1520; Percent complete: 38.0%; Average loss: 3.3236\nIteration: 1521; Percent complete: 38.0%; Average loss: 3.2804\nIteration: 1522; Percent complete: 38.0%; Average loss: 3.0890\nIteration: 1523; Percent complete: 38.1%; Average loss: 3.3548\nIteration: 1524; Percent complete: 38.1%; Average loss: 3.2163\nIteration: 1525; Percent complete: 38.1%; Average loss: 3.5044\nIteration: 1526; Percent complete: 38.1%; Average loss: 3.4395\nIteration: 1527; Percent complete: 38.2%; Average loss: 3.1290\nIteration: 1528; Percent complete: 38.2%; Average loss: 3.2341\nIteration: 1529; Percent complete: 38.2%; Average loss: 3.5858\nIteration: 1530; Percent complete: 38.2%; Average loss: 3.1608\nIteration: 1531; Percent complete: 38.3%; Average loss: 3.4159\nIteration: 1532; Percent complete: 38.3%; Average loss: 3.4340\nIteration: 1533; Percent complete: 38.3%; Average loss: 3.3895\nIteration: 1534; Percent complete: 38.4%; Average loss: 3.4777\nIteration: 1535; Percent complete: 38.4%; Average loss: 3.2485\nIteration: 1536; Percent complete: 38.4%; Average loss: 3.4182\nIteration: 1537; Percent complete: 38.4%; Average loss: 3.3987\nIteration: 1538; Percent complete: 38.5%; Average loss: 3.3547\nIteration: 1539; Percent complete: 38.5%; Average loss: 3.2081\nIteration: 1540; Percent complete: 38.5%; Average loss: 3.2996\nIteration: 1541; Percent complete: 38.5%; Average loss: 3.5305\nIteration: 1542; Percent complete: 38.6%; Average loss: 3.2152\nIteration: 1543; Percent complete: 38.6%; Average loss: 3.3101\nIteration: 1544; Percent complete: 38.6%; Average loss: 3.4342\nIteration: 1545; Percent complete: 38.6%; Average loss: 3.2707\nIteration: 1546; Percent complete: 38.6%; Average loss: 3.2699\nIteration: 1547; Percent complete: 38.7%; Average loss: 3.1564\nIteration: 1548; Percent complete: 38.7%; Average loss: 3.1187\nIteration: 1549; Percent complete: 38.7%; Average loss: 3.3413\nIteration: 1550; Percent complete: 38.8%; Average loss: 3.2659\nIteration: 1551; Percent complete: 38.8%; Average loss: 3.1290\nIteration: 1552; Percent complete: 38.8%; Average loss: 3.3013\nIteration: 1553; Percent complete: 38.8%; Average loss: 3.0445\nIteration: 1554; Percent complete: 38.9%; Average loss: 3.2740\nIteration: 1555; Percent complete: 38.9%; Average loss: 3.3470\nIteration: 1556; Percent complete: 38.9%; Average loss: 3.2288\nIteration: 1557; Percent complete: 38.9%; Average loss: 3.2551\nIteration: 1558; Percent complete: 39.0%; Average loss: 3.3083\nIteration: 1559; Percent complete: 39.0%; Average loss: 3.1384\nIteration: 1560; Percent complete: 39.0%; Average loss: 3.1709\nIteration: 1561; Percent complete: 39.0%; Average loss: 3.5479\nIteration: 1562; Percent complete: 39.1%; Average loss: 2.9897\nIteration: 1563; Percent complete: 39.1%; Average loss: 3.3071\nIteration: 1564; Percent complete: 39.1%; Average loss: 3.2523\nIteration: 1565; Percent complete: 39.1%; Average loss: 3.3210\nIteration: 1566; Percent complete: 39.1%; Average loss: 3.3396\nIteration: 1567; Percent complete: 39.2%; Average loss: 3.3799\nIteration: 1568; Percent complete: 39.2%; Average loss: 3.3228\nIteration: 1569; Percent complete: 39.2%; Average loss: 3.3341\nIteration: 1570; Percent complete: 39.2%; Average loss: 3.2184\nIteration: 1571; Percent complete: 39.3%; Average loss: 3.5470\nIteration: 1572; Percent complete: 39.3%; Average loss: 3.5556\nIteration: 1573; Percent complete: 39.3%; Average loss: 3.1624\nIteration: 1574; Percent complete: 39.4%; Average loss: 3.0140\nIteration: 1575; Percent complete: 39.4%; Average loss: 3.1834\nIteration: 1576; Percent complete: 39.4%; Average loss: 3.5848\nIteration: 1577; Percent complete: 39.4%; Average loss: 3.2771\nIteration: 1578; Percent complete: 39.5%; Average loss: 3.1089\nIteration: 1579; Percent complete: 39.5%; Average loss: 3.1347\nIteration: 1580; Percent complete: 39.5%; Average loss: 3.5095\nIteration: 1581; Percent complete: 39.5%; Average loss: 3.2434\nIteration: 1582; Percent complete: 39.6%; Average loss: 3.0671\nIteration: 1583; Percent complete: 39.6%; Average loss: 3.2215\nIteration: 1584; Percent complete: 39.6%; Average loss: 3.2419\nIteration: 1585; Percent complete: 39.6%; Average loss: 2.9541\nIteration: 1586; Percent complete: 39.6%; Average loss: 3.5249\nIteration: 1587; Percent complete: 39.7%; Average loss: 3.4752\nIteration: 1588; Percent complete: 39.7%; Average loss: 3.3206\nIteration: 1589; Percent complete: 39.7%; Average loss: 3.1705\nIteration: 1590; Percent complete: 39.8%; Average loss: 3.3459\nIteration: 1591; Percent complete: 39.8%; Average loss: 3.3426\nIteration: 1592; Percent complete: 39.8%; Average loss: 3.3059\nIteration: 1593; Percent complete: 39.8%; Average loss: 3.1301\nIteration: 1594; Percent complete: 39.9%; Average loss: 3.1475\nIteration: 1595; Percent complete: 39.9%; Average loss: 3.3832\nIteration: 1596; Percent complete: 39.9%; Average loss: 3.0842\nIteration: 1597; Percent complete: 39.9%; Average loss: 3.3128\nIteration: 1598; Percent complete: 40.0%; Average loss: 3.3439\nIteration: 1599; Percent complete: 40.0%; Average loss: 3.3516\nIteration: 1600; Percent complete: 40.0%; Average loss: 3.3107\nIteration: 1601; Percent complete: 40.0%; Average loss: 3.3348\nIteration: 1602; Percent complete: 40.1%; Average loss: 3.4472\nIteration: 1603; Percent complete: 40.1%; Average loss: 3.2171\nIteration: 1604; Percent complete: 40.1%; Average loss: 3.0263\nIteration: 1605; Percent complete: 40.1%; Average loss: 3.5251\nIteration: 1606; Percent complete: 40.2%; Average loss: 3.2938\nIteration: 1607; Percent complete: 40.2%; Average loss: 3.2922\nIteration: 1608; Percent complete: 40.2%; Average loss: 3.3545\nIteration: 1609; Percent complete: 40.2%; Average loss: 3.3421\nIteration: 1610; Percent complete: 40.2%; Average loss: 2.9921\nIteration: 1611; Percent complete: 40.3%; Average loss: 3.3793\nIteration: 1612; Percent complete: 40.3%; Average loss: 3.2133\nIteration: 1613; Percent complete: 40.3%; Average loss: 3.3602\nIteration: 1614; Percent complete: 40.4%; Average loss: 3.1960\nIteration: 1615; Percent complete: 40.4%; Average loss: 3.3844\nIteration: 1616; Percent complete: 40.4%; Average loss: 3.4310\nIteration: 1617; Percent complete: 40.4%; Average loss: 3.3387\nIteration: 1618; Percent complete: 40.5%; Average loss: 3.4326\nIteration: 1619; Percent complete: 40.5%; Average loss: 3.1054\nIteration: 1620; Percent complete: 40.5%; Average loss: 3.0750\nIteration: 1621; Percent complete: 40.5%; Average loss: 3.2185\nIteration: 1622; Percent complete: 40.6%; Average loss: 3.1631\nIteration: 1623; Percent complete: 40.6%; Average loss: 3.3284\nIteration: 1624; Percent complete: 40.6%; Average loss: 3.3500\nIteration: 1625; Percent complete: 40.6%; Average loss: 3.4004\nIteration: 1626; Percent complete: 40.6%; Average loss: 3.1588\nIteration: 1627; Percent complete: 40.7%; Average loss: 3.1177\nIteration: 1628; Percent complete: 40.7%; Average loss: 3.2455\nIteration: 1629; Percent complete: 40.7%; Average loss: 3.2142\nIteration: 1630; Percent complete: 40.8%; Average loss: 3.2046\nIteration: 1631; Percent complete: 40.8%; Average loss: 2.8996\nIteration: 1632; Percent complete: 40.8%; Average loss: 3.0055\nIteration: 1633; Percent complete: 40.8%; Average loss: 3.1092\nIteration: 1634; Percent complete: 40.8%; Average loss: 3.2667\nIteration: 1635; Percent complete: 40.9%; Average loss: 3.1997\nIteration: 1636; Percent complete: 40.9%; Average loss: 3.2286\nIteration: 1637; Percent complete: 40.9%; Average loss: 3.2381\nIteration: 1638; Percent complete: 40.9%; Average loss: 2.9473\nIteration: 1639; Percent complete: 41.0%; Average loss: 3.2157\nIteration: 1640; Percent complete: 41.0%; Average loss: 3.3053\nIteration: 1641; Percent complete: 41.0%; Average loss: 3.4498\nIteration: 1642; Percent complete: 41.0%; Average loss: 3.0239\nIteration: 1643; Percent complete: 41.1%; Average loss: 3.2381\nIteration: 1644; Percent complete: 41.1%; Average loss: 3.2260\nIteration: 1645; Percent complete: 41.1%; Average loss: 3.2529\nIteration: 1646; Percent complete: 41.1%; Average loss: 3.2393\nIteration: 1647; Percent complete: 41.2%; Average loss: 3.1122\nIteration: 1648; Percent complete: 41.2%; Average loss: 3.5494\nIteration: 1649; Percent complete: 41.2%; Average loss: 3.2909\nIteration: 1650; Percent complete: 41.2%; Average loss: 3.0830\nIteration: 1651; Percent complete: 41.3%; Average loss: 3.2945\nIteration: 1652; Percent complete: 41.3%; Average loss: 3.2810\nIteration: 1653; Percent complete: 41.3%; Average loss: 2.9034\nIteration: 1654; Percent complete: 41.3%; Average loss: 3.3096\nIteration: 1655; Percent complete: 41.4%; Average loss: 3.4415\nIteration: 1656; Percent complete: 41.4%; Average loss: 3.2992\nIteration: 1657; Percent complete: 41.4%; Average loss: 3.2122\nIteration: 1658; Percent complete: 41.4%; Average loss: 3.2743\nIteration: 1659; Percent complete: 41.5%; Average loss: 3.2238\nIteration: 1660; Percent complete: 41.5%; Average loss: 3.2414\nIteration: 1661; Percent complete: 41.5%; Average loss: 3.0948\nIteration: 1662; Percent complete: 41.5%; Average loss: 3.1310\nIteration: 1663; Percent complete: 41.6%; Average loss: 3.4402\nIteration: 1664; Percent complete: 41.6%; Average loss: 3.2428\nIteration: 1665; Percent complete: 41.6%; Average loss: 3.1464\nIteration: 1666; Percent complete: 41.6%; Average loss: 3.3959\nIteration: 1667; Percent complete: 41.7%; Average loss: 3.2994\nIteration: 1668; Percent complete: 41.7%; Average loss: 3.0371\nIteration: 1669; Percent complete: 41.7%; Average loss: 3.3442\nIteration: 1670; Percent complete: 41.8%; Average loss: 2.9115\nIteration: 1671; Percent complete: 41.8%; Average loss: 3.3993\nIteration: 1672; Percent complete: 41.8%; Average loss: 3.3059\nIteration: 1673; Percent complete: 41.8%; Average loss: 3.2444\nIteration: 1674; Percent complete: 41.9%; Average loss: 3.2474\nIteration: 1675; Percent complete: 41.9%; Average loss: 3.3108\nIteration: 1676; Percent complete: 41.9%; Average loss: 3.3755\nIteration: 1677; Percent complete: 41.9%; Average loss: 3.3056\nIteration: 1678; Percent complete: 41.9%; Average loss: 3.2580\nIteration: 1679; Percent complete: 42.0%; Average loss: 2.9335\nIteration: 1680; Percent complete: 42.0%; Average loss: 3.2319\nIteration: 1681; Percent complete: 42.0%; Average loss: 3.3370\nIteration: 1682; Percent complete: 42.0%; Average loss: 3.3520\nIteration: 1683; Percent complete: 42.1%; Average loss: 3.2527\nIteration: 1684; Percent complete: 42.1%; Average loss: 3.2468\nIteration: 1685; Percent complete: 42.1%; Average loss: 3.2896\nIteration: 1686; Percent complete: 42.1%; Average loss: 3.2757\nIteration: 1687; Percent complete: 42.2%; Average loss: 3.2605\nIteration: 1688; Percent complete: 42.2%; Average loss: 3.0724\nIteration: 1689; Percent complete: 42.2%; Average loss: 3.1156\nIteration: 1690; Percent complete: 42.2%; Average loss: 3.1227\nIteration: 1691; Percent complete: 42.3%; Average loss: 3.2669\nIteration: 1692; Percent complete: 42.3%; Average loss: 3.3315\nIteration: 1693; Percent complete: 42.3%; Average loss: 3.3342\nIteration: 1694; Percent complete: 42.4%; Average loss: 3.1501\nIteration: 1695; Percent complete: 42.4%; Average loss: 3.1143\nIteration: 1696; Percent complete: 42.4%; Average loss: 3.5941\nIteration: 1697; Percent complete: 42.4%; Average loss: 2.9563\nIteration: 1698; Percent complete: 42.4%; Average loss: 3.5446\nIteration: 1699; Percent complete: 42.5%; Average loss: 3.2919\nIteration: 1700; Percent complete: 42.5%; Average loss: 3.1458\nIteration: 1701; Percent complete: 42.5%; Average loss: 3.1674\nIteration: 1702; Percent complete: 42.5%; Average loss: 3.4379\nIteration: 1703; Percent complete: 42.6%; Average loss: 3.0182\nIteration: 1704; Percent complete: 42.6%; Average loss: 3.3106\nIteration: 1705; Percent complete: 42.6%; Average loss: 3.2856\nIteration: 1706; Percent complete: 42.6%; Average loss: 3.2208\nIteration: 1707; Percent complete: 42.7%; Average loss: 3.3667\nIteration: 1708; Percent complete: 42.7%; Average loss: 3.1252\nIteration: 1709; Percent complete: 42.7%; Average loss: 3.3366\nIteration: 1710; Percent complete: 42.8%; Average loss: 2.9048\nIteration: 1711; Percent complete: 42.8%; Average loss: 3.1006\nIteration: 1712; Percent complete: 42.8%; Average loss: 3.3816\nIteration: 1713; Percent complete: 42.8%; Average loss: 3.4182\nIteration: 1714; Percent complete: 42.9%; Average loss: 3.1358\nIteration: 1715; Percent complete: 42.9%; Average loss: 3.2915\nIteration: 1716; Percent complete: 42.9%; Average loss: 3.2658\nIteration: 1717; Percent complete: 42.9%; Average loss: 3.0673\nIteration: 1718; Percent complete: 43.0%; Average loss: 3.2859\nIteration: 1719; Percent complete: 43.0%; Average loss: 3.3400\nIteration: 1720; Percent complete: 43.0%; Average loss: 3.3320\nIteration: 1721; Percent complete: 43.0%; Average loss: 3.2035\nIteration: 1722; Percent complete: 43.0%; Average loss: 3.1135\nIteration: 1723; Percent complete: 43.1%; Average loss: 3.2804\nIteration: 1724; Percent complete: 43.1%; Average loss: 3.1442\nIteration: 1725; Percent complete: 43.1%; Average loss: 3.1558\nIteration: 1726; Percent complete: 43.1%; Average loss: 3.5618\nIteration: 1727; Percent complete: 43.2%; Average loss: 3.3802\nIteration: 1728; Percent complete: 43.2%; Average loss: 3.1952\nIteration: 1729; Percent complete: 43.2%; Average loss: 3.3397\nIteration: 1730; Percent complete: 43.2%; Average loss: 3.2225\nIteration: 1731; Percent complete: 43.3%; Average loss: 3.0006\nIteration: 1732; Percent complete: 43.3%; Average loss: 3.0523\nIteration: 1733; Percent complete: 43.3%; Average loss: 3.1861\nIteration: 1734; Percent complete: 43.4%; Average loss: 3.4638\nIteration: 1735; Percent complete: 43.4%; Average loss: 3.1254\nIteration: 1736; Percent complete: 43.4%; Average loss: 3.3500\nIteration: 1737; Percent complete: 43.4%; Average loss: 3.0386\nIteration: 1738; Percent complete: 43.5%; Average loss: 3.3934\nIteration: 1739; Percent complete: 43.5%; Average loss: 3.1373\nIteration: 1740; Percent complete: 43.5%; Average loss: 3.3749\nIteration: 1741; Percent complete: 43.5%; Average loss: 3.3237\nIteration: 1742; Percent complete: 43.5%; Average loss: 3.4568\nIteration: 1743; Percent complete: 43.6%; Average loss: 3.3779\nIteration: 1744; Percent complete: 43.6%; Average loss: 3.4309\nIteration: 1745; Percent complete: 43.6%; Average loss: 3.1608\nIteration: 1746; Percent complete: 43.6%; Average loss: 3.3955\nIteration: 1747; Percent complete: 43.7%; Average loss: 3.1232\nIteration: 1748; Percent complete: 43.7%; Average loss: 3.3005\nIteration: 1749; Percent complete: 43.7%; Average loss: 3.3261\nIteration: 1750; Percent complete: 43.8%; Average loss: 3.2326\nIteration: 1751; Percent complete: 43.8%; Average loss: 3.4771\nIteration: 1752; Percent complete: 43.8%; Average loss: 3.0741\nIteration: 1753; Percent complete: 43.8%; Average loss: 3.4196\nIteration: 1754; Percent complete: 43.9%; Average loss: 3.2699\nIteration: 1755; Percent complete: 43.9%; Average loss: 3.1772\nIteration: 1756; Percent complete: 43.9%; Average loss: 3.3076\nIteration: 1757; Percent complete: 43.9%; Average loss: 3.1515\nIteration: 1758; Percent complete: 44.0%; Average loss: 3.1647\nIteration: 1759; Percent complete: 44.0%; Average loss: 3.2306\nIteration: 1760; Percent complete: 44.0%; Average loss: 2.9710\nIteration: 1761; Percent complete: 44.0%; Average loss: 2.9897\nIteration: 1762; Percent complete: 44.0%; Average loss: 3.3363\nIteration: 1763; Percent complete: 44.1%; Average loss: 3.1663\nIteration: 1764; Percent complete: 44.1%; Average loss: 3.0023\nIteration: 1765; Percent complete: 44.1%; Average loss: 3.2110\nIteration: 1766; Percent complete: 44.1%; Average loss: 3.3550\nIteration: 1767; Percent complete: 44.2%; Average loss: 3.2824\nIteration: 1768; Percent complete: 44.2%; Average loss: 2.9963\nIteration: 1769; Percent complete: 44.2%; Average loss: 3.2497\nIteration: 1770; Percent complete: 44.2%; Average loss: 3.2452\nIteration: 1771; Percent complete: 44.3%; Average loss: 3.1619\nIteration: 1772; Percent complete: 44.3%; Average loss: 3.1827\nIteration: 1773; Percent complete: 44.3%; Average loss: 3.3688\nIteration: 1774; Percent complete: 44.4%; Average loss: 3.1387\nIteration: 1775; Percent complete: 44.4%; Average loss: 3.4917\nIteration: 1776; Percent complete: 44.4%; Average loss: 3.1815\nIteration: 1777; Percent complete: 44.4%; Average loss: 3.3093\nIteration: 1778; Percent complete: 44.5%; Average loss: 2.9900\nIteration: 1779; Percent complete: 44.5%; Average loss: 3.1359\nIteration: 1780; Percent complete: 44.5%; Average loss: 3.2875\nIteration: 1781; Percent complete: 44.5%; Average loss: 3.2450\nIteration: 1782; Percent complete: 44.5%; Average loss: 3.2531\nIteration: 1783; Percent complete: 44.6%; Average loss: 3.3852\nIteration: 1784; Percent complete: 44.6%; Average loss: 3.1133\nIteration: 1785; Percent complete: 44.6%; Average loss: 3.1021\nIteration: 1786; Percent complete: 44.6%; Average loss: 3.1096\nIteration: 1787; Percent complete: 44.7%; Average loss: 3.2369\nIteration: 1788; Percent complete: 44.7%; Average loss: 3.1895\nIteration: 1789; Percent complete: 44.7%; Average loss: 3.2218\nIteration: 1790; Percent complete: 44.8%; Average loss: 3.0793\nIteration: 1791; Percent complete: 44.8%; Average loss: 3.3000\nIteration: 1792; Percent complete: 44.8%; Average loss: 3.1939\nIteration: 1793; Percent complete: 44.8%; Average loss: 3.4376\nIteration: 1794; Percent complete: 44.9%; Average loss: 3.6111\nIteration: 1795; Percent complete: 44.9%; Average loss: 3.0718\nIteration: 1796; Percent complete: 44.9%; Average loss: 3.2271\nIteration: 1797; Percent complete: 44.9%; Average loss: 3.1396\nIteration: 1798; Percent complete: 45.0%; Average loss: 3.0013\nIteration: 1799; Percent complete: 45.0%; Average loss: 3.1261\nIteration: 1800; Percent complete: 45.0%; Average loss: 3.1368\nIteration: 1801; Percent complete: 45.0%; Average loss: 3.3021\nIteration: 1802; Percent complete: 45.1%; Average loss: 3.1574\nIteration: 1803; Percent complete: 45.1%; Average loss: 3.1993\nIteration: 1804; Percent complete: 45.1%; Average loss: 3.1508\nIteration: 1805; Percent complete: 45.1%; Average loss: 3.1429\nIteration: 1806; Percent complete: 45.1%; Average loss: 3.1074\nIteration: 1807; Percent complete: 45.2%; Average loss: 3.2533\nIteration: 1808; Percent complete: 45.2%; Average loss: 3.1647\nIteration: 1809; Percent complete: 45.2%; Average loss: 3.0370\nIteration: 1810; Percent complete: 45.2%; Average loss: 3.3683\nIteration: 1811; Percent complete: 45.3%; Average loss: 3.3187\nIteration: 1812; Percent complete: 45.3%; Average loss: 3.3750\nIteration: 1813; Percent complete: 45.3%; Average loss: 3.3384\nIteration: 1814; Percent complete: 45.4%; Average loss: 3.0969\nIteration: 1815; Percent complete: 45.4%; Average loss: 2.9660\nIteration: 1816; Percent complete: 45.4%; Average loss: 3.2604\nIteration: 1817; Percent complete: 45.4%; Average loss: 3.0856\nIteration: 1818; Percent complete: 45.5%; Average loss: 3.1006\nIteration: 1819; Percent complete: 45.5%; Average loss: 3.2190\nIteration: 1820; Percent complete: 45.5%; Average loss: 3.1253\nIteration: 1821; Percent complete: 45.5%; Average loss: 3.0561\nIteration: 1822; Percent complete: 45.6%; Average loss: 3.0234\nIteration: 1823; Percent complete: 45.6%; Average loss: 3.3133\nIteration: 1824; Percent complete: 45.6%; Average loss: 3.2200\nIteration: 1825; Percent complete: 45.6%; Average loss: 3.3360\nIteration: 1826; Percent complete: 45.6%; Average loss: 3.1831\nIteration: 1827; Percent complete: 45.7%; Average loss: 3.4040\nIteration: 1828; Percent complete: 45.7%; Average loss: 3.1663\nIteration: 1829; Percent complete: 45.7%; Average loss: 3.0796\nIteration: 1830; Percent complete: 45.8%; Average loss: 3.1017\nIteration: 1831; Percent complete: 45.8%; Average loss: 3.2009\nIteration: 1832; Percent complete: 45.8%; Average loss: 3.1398\nIteration: 1833; Percent complete: 45.8%; Average loss: 3.2170\nIteration: 1834; Percent complete: 45.9%; Average loss: 3.4318\nIteration: 1835; Percent complete: 45.9%; Average loss: 2.9799\nIteration: 1836; Percent complete: 45.9%; Average loss: 3.0560\nIteration: 1837; Percent complete: 45.9%; Average loss: 3.2631\nIteration: 1838; Percent complete: 46.0%; Average loss: 3.4028\nIteration: 1839; Percent complete: 46.0%; Average loss: 3.1622\nIteration: 1840; Percent complete: 46.0%; Average loss: 3.0347\nIteration: 1841; Percent complete: 46.0%; Average loss: 3.2900\nIteration: 1842; Percent complete: 46.1%; Average loss: 3.0089\nIteration: 1843; Percent complete: 46.1%; Average loss: 3.2647\nIteration: 1844; Percent complete: 46.1%; Average loss: 3.1915\nIteration: 1845; Percent complete: 46.1%; Average loss: 3.1799\nIteration: 1846; Percent complete: 46.2%; Average loss: 3.2022\nIteration: 1847; Percent complete: 46.2%; Average loss: 2.9630\nIteration: 1848; Percent complete: 46.2%; Average loss: 3.0755\nIteration: 1849; Percent complete: 46.2%; Average loss: 3.4042\nIteration: 1850; Percent complete: 46.2%; Average loss: 3.4335\nIteration: 1851; Percent complete: 46.3%; Average loss: 3.1853\nIteration: 1852; Percent complete: 46.3%; Average loss: 3.0357\nIteration: 1853; Percent complete: 46.3%; Average loss: 3.1002\nIteration: 1854; Percent complete: 46.4%; Average loss: 3.1801\nIteration: 1855; Percent complete: 46.4%; Average loss: 2.7403\nIteration: 1856; Percent complete: 46.4%; Average loss: 3.2981\nIteration: 1857; Percent complete: 46.4%; Average loss: 3.1020\nIteration: 1858; Percent complete: 46.5%; Average loss: 3.3167\nIteration: 1859; Percent complete: 46.5%; Average loss: 3.2449\nIteration: 1860; Percent complete: 46.5%; Average loss: 2.9320\nIteration: 1861; Percent complete: 46.5%; Average loss: 3.1681\nIteration: 1862; Percent complete: 46.6%; Average loss: 3.4855\nIteration: 1863; Percent complete: 46.6%; Average loss: 3.0315\nIteration: 1864; Percent complete: 46.6%; Average loss: 3.1844\nIteration: 1865; Percent complete: 46.6%; Average loss: 3.0859\nIteration: 1866; Percent complete: 46.7%; Average loss: 3.1695\nIteration: 1867; Percent complete: 46.7%; Average loss: 3.2172\nIteration: 1868; Percent complete: 46.7%; Average loss: 3.2776\nIteration: 1869; Percent complete: 46.7%; Average loss: 3.1799\nIteration: 1870; Percent complete: 46.8%; Average loss: 3.3009\nIteration: 1871; Percent complete: 46.8%; Average loss: 3.2578\nIteration: 1872; Percent complete: 46.8%; Average loss: 3.3361\nIteration: 1873; Percent complete: 46.8%; Average loss: 3.3199\nIteration: 1874; Percent complete: 46.9%; Average loss: 3.4202\nIteration: 1875; Percent complete: 46.9%; Average loss: 3.5224\nIteration: 1876; Percent complete: 46.9%; Average loss: 3.3263\nIteration: 1877; Percent complete: 46.9%; Average loss: 3.3154\nIteration: 1878; Percent complete: 46.9%; Average loss: 3.4712\nIteration: 1879; Percent complete: 47.0%; Average loss: 2.9961\nIteration: 1880; Percent complete: 47.0%; Average loss: 3.0412\nIteration: 1881; Percent complete: 47.0%; Average loss: 3.0904\nIteration: 1882; Percent complete: 47.0%; Average loss: 3.0415\nIteration: 1883; Percent complete: 47.1%; Average loss: 3.2995\nIteration: 1884; Percent complete: 47.1%; Average loss: 3.1182\nIteration: 1885; Percent complete: 47.1%; Average loss: 3.2468\nIteration: 1886; Percent complete: 47.1%; Average loss: 3.1400\nIteration: 1887; Percent complete: 47.2%; Average loss: 3.2043\nIteration: 1888; Percent complete: 47.2%; Average loss: 3.3377\nIteration: 1889; Percent complete: 47.2%; Average loss: 2.9979\nIteration: 1890; Percent complete: 47.2%; Average loss: 3.5200\nIteration: 1891; Percent complete: 47.3%; Average loss: 3.2088\nIteration: 1892; Percent complete: 47.3%; Average loss: 3.2517\nIteration: 1893; Percent complete: 47.3%; Average loss: 3.4250\nIteration: 1894; Percent complete: 47.3%; Average loss: 3.2970\nIteration: 1895; Percent complete: 47.4%; Average loss: 3.0345\nIteration: 1896; Percent complete: 47.4%; Average loss: 3.3465\nIteration: 1897; Percent complete: 47.4%; Average loss: 3.3238\nIteration: 1898; Percent complete: 47.4%; Average loss: 2.9242\nIteration: 1899; Percent complete: 47.5%; Average loss: 3.2225\nIteration: 1900; Percent complete: 47.5%; Average loss: 3.1491\nIteration: 1901; Percent complete: 47.5%; Average loss: 3.2633\nIteration: 1902; Percent complete: 47.5%; Average loss: 3.0975\nIteration: 1903; Percent complete: 47.6%; Average loss: 2.8999\nIteration: 1904; Percent complete: 47.6%; Average loss: 3.2975\nIteration: 1905; Percent complete: 47.6%; Average loss: 3.2998\nIteration: 1906; Percent complete: 47.6%; Average loss: 3.1062\nIteration: 1907; Percent complete: 47.7%; Average loss: 3.6247\nIteration: 1908; Percent complete: 47.7%; Average loss: 3.3114\nIteration: 1909; Percent complete: 47.7%; Average loss: 3.3423\nIteration: 1910; Percent complete: 47.8%; Average loss: 3.2826\nIteration: 1911; Percent complete: 47.8%; Average loss: 2.9898\nIteration: 1912; Percent complete: 47.8%; Average loss: 3.0803\nIteration: 1913; Percent complete: 47.8%; Average loss: 3.2872\nIteration: 1914; Percent complete: 47.9%; Average loss: 3.0832\nIteration: 1915; Percent complete: 47.9%; Average loss: 3.1341\nIteration: 1916; Percent complete: 47.9%; Average loss: 3.3294\nIteration: 1917; Percent complete: 47.9%; Average loss: 3.0916\nIteration: 1918; Percent complete: 47.9%; Average loss: 2.9956\nIteration: 1919; Percent complete: 48.0%; Average loss: 2.7791\nIteration: 1920; Percent complete: 48.0%; Average loss: 3.1243\nIteration: 1921; Percent complete: 48.0%; Average loss: 3.2670\nIteration: 1922; Percent complete: 48.0%; Average loss: 3.0830\nIteration: 1923; Percent complete: 48.1%; Average loss: 3.0180\nIteration: 1924; Percent complete: 48.1%; Average loss: 3.4138\nIteration: 1925; Percent complete: 48.1%; Average loss: 3.2344\nIteration: 1926; Percent complete: 48.1%; Average loss: 3.5231\nIteration: 1927; Percent complete: 48.2%; Average loss: 3.1609\nIteration: 1928; Percent complete: 48.2%; Average loss: 3.1302\nIteration: 1929; Percent complete: 48.2%; Average loss: 3.2320\nIteration: 1930; Percent complete: 48.2%; Average loss: 3.3420\nIteration: 1931; Percent complete: 48.3%; Average loss: 3.3234\nIteration: 1932; Percent complete: 48.3%; Average loss: 3.2923\nIteration: 1933; Percent complete: 48.3%; Average loss: 3.0797\nIteration: 1934; Percent complete: 48.4%; Average loss: 3.3985\nIteration: 1935; Percent complete: 48.4%; Average loss: 3.2650\nIteration: 1936; Percent complete: 48.4%; Average loss: 3.1445\nIteration: 1937; Percent complete: 48.4%; Average loss: 2.9605\nIteration: 1938; Percent complete: 48.4%; Average loss: 3.3589\nIteration: 1939; Percent complete: 48.5%; Average loss: 3.3360\nIteration: 1940; Percent complete: 48.5%; Average loss: 3.3683\nIteration: 1941; Percent complete: 48.5%; Average loss: 3.2654\nIteration: 1942; Percent complete: 48.5%; Average loss: 3.2770\nIteration: 1943; Percent complete: 48.6%; Average loss: 3.0343\nIteration: 1944; Percent complete: 48.6%; Average loss: 3.2506\nIteration: 1945; Percent complete: 48.6%; Average loss: 3.2456\nIteration: 1946; Percent complete: 48.6%; Average loss: 3.1970\nIteration: 1947; Percent complete: 48.7%; Average loss: 3.1541\nIteration: 1948; Percent complete: 48.7%; Average loss: 2.9942\nIteration: 1949; Percent complete: 48.7%; Average loss: 3.1128\nIteration: 1950; Percent complete: 48.8%; Average loss: 2.9329\nIteration: 1951; Percent complete: 48.8%; Average loss: 3.1105\nIteration: 1952; Percent complete: 48.8%; Average loss: 3.2333\nIteration: 1953; Percent complete: 48.8%; Average loss: 2.9393\nIteration: 1954; Percent complete: 48.9%; Average loss: 2.7745\nIteration: 1955; Percent complete: 48.9%; Average loss: 3.0650\nIteration: 1956; Percent complete: 48.9%; Average loss: 3.2339\nIteration: 1957; Percent complete: 48.9%; Average loss: 3.1640\nIteration: 1958; Percent complete: 48.9%; Average loss: 3.0913\nIteration: 1959; Percent complete: 49.0%; Average loss: 3.3392\nIteration: 1960; Percent complete: 49.0%; Average loss: 3.2705\nIteration: 1961; Percent complete: 49.0%; Average loss: 3.1410\nIteration: 1962; Percent complete: 49.0%; Average loss: 3.1485\nIteration: 1963; Percent complete: 49.1%; Average loss: 3.0191\nIteration: 1964; Percent complete: 49.1%; Average loss: 3.0934\nIteration: 1965; Percent complete: 49.1%; Average loss: 3.1616\nIteration: 1966; Percent complete: 49.1%; Average loss: 3.0047\nIteration: 1967; Percent complete: 49.2%; Average loss: 3.0401\nIteration: 1968; Percent complete: 49.2%; Average loss: 3.0247\nIteration: 1969; Percent complete: 49.2%; Average loss: 3.0614\nIteration: 1970; Percent complete: 49.2%; Average loss: 2.9663\nIteration: 1971; Percent complete: 49.3%; Average loss: 3.1205\nIteration: 1972; Percent complete: 49.3%; Average loss: 3.0304\nIteration: 1973; Percent complete: 49.3%; Average loss: 3.1785\nIteration: 1974; Percent complete: 49.4%; Average loss: 3.3070\nIteration: 1975; Percent complete: 49.4%; Average loss: 3.2098\nIteration: 1976; Percent complete: 49.4%; Average loss: 3.1443\nIteration: 1977; Percent complete: 49.4%; Average loss: 3.0870\nIteration: 1978; Percent complete: 49.5%; Average loss: 3.1661\nIteration: 1979; Percent complete: 49.5%; Average loss: 3.1918\nIteration: 1980; Percent complete: 49.5%; Average loss: 3.2904\nIteration: 1981; Percent complete: 49.5%; Average loss: 3.2926\nIteration: 1982; Percent complete: 49.5%; Average loss: 3.1861\nIteration: 1983; Percent complete: 49.6%; Average loss: 3.1295\nIteration: 1984; Percent complete: 49.6%; Average loss: 3.1571\nIteration: 1985; Percent complete: 49.6%; Average loss: 3.1826\nIteration: 1986; Percent complete: 49.6%; Average loss: 3.2201\nIteration: 1987; Percent complete: 49.7%; Average loss: 3.1901\nIteration: 1988; Percent complete: 49.7%; Average loss: 3.1322\nIteration: 1989; Percent complete: 49.7%; Average loss: 2.9740\nIteration: 1990; Percent complete: 49.8%; Average loss: 3.1995\nIteration: 1991; Percent complete: 49.8%; Average loss: 3.4718\nIteration: 1992; Percent complete: 49.8%; Average loss: 2.8365\nIteration: 1993; Percent complete: 49.8%; Average loss: 3.3967\nIteration: 1994; Percent complete: 49.9%; Average loss: 3.2794\nIteration: 1995; Percent complete: 49.9%; Average loss: 3.2413\nIteration: 1996; Percent complete: 49.9%; Average loss: 3.1476\nIteration: 1997; Percent complete: 49.9%; Average loss: 3.1095\nIteration: 1998; Percent complete: 50.0%; Average loss: 3.1998\nIteration: 1999; Percent complete: 50.0%; Average loss: 3.2043\nIteration: 2000; Percent complete: 50.0%; Average loss: 3.1486\nIteration: 2001; Percent complete: 50.0%; Average loss: 3.0050\nIteration: 2002; Percent complete: 50.0%; Average loss: 2.8859\nIteration: 2003; Percent complete: 50.1%; Average loss: 3.2551\nIteration: 2004; Percent complete: 50.1%; Average loss: 2.9216\nIteration: 2005; Percent complete: 50.1%; Average loss: 3.4049\nIteration: 2006; Percent complete: 50.1%; Average loss: 3.0173\nIteration: 2007; Percent complete: 50.2%; Average loss: 3.0962\nIteration: 2008; Percent complete: 50.2%; Average loss: 2.9738\nIteration: 2009; Percent complete: 50.2%; Average loss: 3.1171\nIteration: 2010; Percent complete: 50.2%; Average loss: 3.2948\nIteration: 2011; Percent complete: 50.3%; Average loss: 3.1059\nIteration: 2012; Percent complete: 50.3%; Average loss: 3.1453\nIteration: 2013; Percent complete: 50.3%; Average loss: 3.0664\nIteration: 2014; Percent complete: 50.3%; Average loss: 3.1457\nIteration: 2015; Percent complete: 50.4%; Average loss: 3.3143\nIteration: 2016; Percent complete: 50.4%; Average loss: 3.1464\nIteration: 2017; Percent complete: 50.4%; Average loss: 3.2500\nIteration: 2018; Percent complete: 50.4%; Average loss: 3.2601\nIteration: 2019; Percent complete: 50.5%; Average loss: 3.2624\nIteration: 2020; Percent complete: 50.5%; Average loss: 3.1732\nIteration: 2021; Percent complete: 50.5%; Average loss: 3.0593\nIteration: 2022; Percent complete: 50.5%; Average loss: 3.3634\nIteration: 2023; Percent complete: 50.6%; Average loss: 3.1482\nIteration: 2024; Percent complete: 50.6%; Average loss: 3.0322\nIteration: 2025; Percent complete: 50.6%; Average loss: 2.9021\nIteration: 2026; Percent complete: 50.6%; Average loss: 2.9662\nIteration: 2027; Percent complete: 50.7%; Average loss: 3.0251\nIteration: 2028; Percent complete: 50.7%; Average loss: 3.0984\nIteration: 2029; Percent complete: 50.7%; Average loss: 3.4965\nIteration: 2030; Percent complete: 50.7%; Average loss: 2.8948\nIteration: 2031; Percent complete: 50.8%; Average loss: 3.2733\nIteration: 2032; Percent complete: 50.8%; Average loss: 3.2498\nIteration: 2033; Percent complete: 50.8%; Average loss: 3.2901\nIteration: 2034; Percent complete: 50.8%; Average loss: 3.1582\nIteration: 2035; Percent complete: 50.9%; Average loss: 3.2065\nIteration: 2036; Percent complete: 50.9%; Average loss: 2.9740\nIteration: 2037; Percent complete: 50.9%; Average loss: 3.1848\nIteration: 2038; Percent complete: 50.9%; Average loss: 3.4157\nIteration: 2039; Percent complete: 51.0%; Average loss: 3.1850\nIteration: 2040; Percent complete: 51.0%; Average loss: 3.2072\nIteration: 2041; Percent complete: 51.0%; Average loss: 3.0758\nIteration: 2042; Percent complete: 51.0%; Average loss: 3.3606\nIteration: 2043; Percent complete: 51.1%; Average loss: 3.1471\nIteration: 2044; Percent complete: 51.1%; Average loss: 3.0496\nIteration: 2045; Percent complete: 51.1%; Average loss: 3.1355\nIteration: 2046; Percent complete: 51.1%; Average loss: 3.0696\nIteration: 2047; Percent complete: 51.2%; Average loss: 2.9528\nIteration: 2048; Percent complete: 51.2%; Average loss: 3.0399\nIteration: 2049; Percent complete: 51.2%; Average loss: 3.0017\nIteration: 2050; Percent complete: 51.2%; Average loss: 3.1973\nIteration: 2051; Percent complete: 51.3%; Average loss: 3.2219\nIteration: 2052; Percent complete: 51.3%; Average loss: 3.2159\nIteration: 2053; Percent complete: 51.3%; Average loss: 3.3684\nIteration: 2054; Percent complete: 51.3%; Average loss: 2.9612\nIteration: 2055; Percent complete: 51.4%; Average loss: 3.3585\nIteration: 2056; Percent complete: 51.4%; Average loss: 3.2832\nIteration: 2057; Percent complete: 51.4%; Average loss: 2.8819\nIteration: 2058; Percent complete: 51.4%; Average loss: 3.3102\nIteration: 2059; Percent complete: 51.5%; Average loss: 2.9867\nIteration: 2060; Percent complete: 51.5%; Average loss: 3.1341\nIteration: 2061; Percent complete: 51.5%; Average loss: 3.2594\nIteration: 2062; Percent complete: 51.5%; Average loss: 2.9817\nIteration: 2063; Percent complete: 51.6%; Average loss: 2.7519\nIteration: 2064; Percent complete: 51.6%; Average loss: 3.2083\nIteration: 2065; Percent complete: 51.6%; Average loss: 3.0307\nIteration: 2066; Percent complete: 51.6%; Average loss: 3.0579\nIteration: 2067; Percent complete: 51.7%; Average loss: 3.1198\nIteration: 2068; Percent complete: 51.7%; Average loss: 2.8256\nIteration: 2069; Percent complete: 51.7%; Average loss: 3.2775\nIteration: 2070; Percent complete: 51.7%; Average loss: 3.2205\nIteration: 2071; Percent complete: 51.8%; Average loss: 3.2063\nIteration: 2072; Percent complete: 51.8%; Average loss: 2.9257\nIteration: 2073; Percent complete: 51.8%; Average loss: 2.9995\nIteration: 2074; Percent complete: 51.8%; Average loss: 3.0989\nIteration: 2075; Percent complete: 51.9%; Average loss: 3.1216\nIteration: 2076; Percent complete: 51.9%; Average loss: 3.4623\nIteration: 2077; Percent complete: 51.9%; Average loss: 2.8132\nIteration: 2078; Percent complete: 51.9%; Average loss: 3.2058\nIteration: 2079; Percent complete: 52.0%; Average loss: 3.3079\nIteration: 2080; Percent complete: 52.0%; Average loss: 3.1268\nIteration: 2081; Percent complete: 52.0%; Average loss: 3.0875\nIteration: 2082; Percent complete: 52.0%; Average loss: 3.2497\nIteration: 2083; Percent complete: 52.1%; Average loss: 3.1988\nIteration: 2084; Percent complete: 52.1%; Average loss: 3.2183\nIteration: 2085; Percent complete: 52.1%; Average loss: 3.0966\nIteration: 2086; Percent complete: 52.1%; Average loss: 3.2293\nIteration: 2087; Percent complete: 52.2%; Average loss: 3.1841\nIteration: 2088; Percent complete: 52.2%; Average loss: 3.3184\nIteration: 2089; Percent complete: 52.2%; Average loss: 3.1096\nIteration: 2090; Percent complete: 52.2%; Average loss: 3.2491\nIteration: 2091; Percent complete: 52.3%; Average loss: 3.0601\nIteration: 2092; Percent complete: 52.3%; Average loss: 3.1596\nIteration: 2093; Percent complete: 52.3%; Average loss: 3.2423\nIteration: 2094; Percent complete: 52.3%; Average loss: 3.0280\nIteration: 2095; Percent complete: 52.4%; Average loss: 3.2470\nIteration: 2096; Percent complete: 52.4%; Average loss: 3.1136\nIteration: 2097; Percent complete: 52.4%; Average loss: 3.1662\nIteration: 2098; Percent complete: 52.4%; Average loss: 2.8747\nIteration: 2099; Percent complete: 52.5%; Average loss: 3.1763\nIteration: 2100; Percent complete: 52.5%; Average loss: 3.3284\nIteration: 2101; Percent complete: 52.5%; Average loss: 2.9558\nIteration: 2102; Percent complete: 52.5%; Average loss: 3.1492\nIteration: 2103; Percent complete: 52.6%; Average loss: 2.9807\nIteration: 2104; Percent complete: 52.6%; Average loss: 3.3502\nIteration: 2105; Percent complete: 52.6%; Average loss: 3.0743\nIteration: 2106; Percent complete: 52.6%; Average loss: 2.9433\nIteration: 2107; Percent complete: 52.7%; Average loss: 3.2990\nIteration: 2108; Percent complete: 52.7%; Average loss: 3.2171\nIteration: 2109; Percent complete: 52.7%; Average loss: 3.1286\nIteration: 2110; Percent complete: 52.8%; Average loss: 3.1164\nIteration: 2111; Percent complete: 52.8%; Average loss: 3.2117\nIteration: 2112; Percent complete: 52.8%; Average loss: 3.0799\nIteration: 2113; Percent complete: 52.8%; Average loss: 3.1831\nIteration: 2114; Percent complete: 52.8%; Average loss: 3.0799\nIteration: 2115; Percent complete: 52.9%; Average loss: 3.3775\nIteration: 2116; Percent complete: 52.9%; Average loss: 3.1750\nIteration: 2117; Percent complete: 52.9%; Average loss: 3.1465\nIteration: 2118; Percent complete: 52.9%; Average loss: 3.1082\nIteration: 2119; Percent complete: 53.0%; Average loss: 3.1692\nIteration: 2120; Percent complete: 53.0%; Average loss: 3.1884\nIteration: 2121; Percent complete: 53.0%; Average loss: 3.1537\nIteration: 2122; Percent complete: 53.0%; Average loss: 3.1300\nIteration: 2123; Percent complete: 53.1%; Average loss: 3.1021\nIteration: 2124; Percent complete: 53.1%; Average loss: 3.0690\nIteration: 2125; Percent complete: 53.1%; Average loss: 3.2636\nIteration: 2126; Percent complete: 53.1%; Average loss: 3.1502\nIteration: 2127; Percent complete: 53.2%; Average loss: 3.2276\nIteration: 2128; Percent complete: 53.2%; Average loss: 3.0741\nIteration: 2129; Percent complete: 53.2%; Average loss: 2.9935\nIteration: 2130; Percent complete: 53.2%; Average loss: 3.3164\nIteration: 2131; Percent complete: 53.3%; Average loss: 2.9759\nIteration: 2132; Percent complete: 53.3%; Average loss: 2.9071\nIteration: 2133; Percent complete: 53.3%; Average loss: 3.0754\nIteration: 2134; Percent complete: 53.3%; Average loss: 3.2928\nIteration: 2135; Percent complete: 53.4%; Average loss: 2.9031\nIteration: 2136; Percent complete: 53.4%; Average loss: 3.3762\nIteration: 2137; Percent complete: 53.4%; Average loss: 3.2502\nIteration: 2138; Percent complete: 53.4%; Average loss: 3.2331\nIteration: 2139; Percent complete: 53.5%; Average loss: 3.0842\nIteration: 2140; Percent complete: 53.5%; Average loss: 3.0647\nIteration: 2141; Percent complete: 53.5%; Average loss: 3.0953\nIteration: 2142; Percent complete: 53.5%; Average loss: 2.9234\nIteration: 2143; Percent complete: 53.6%; Average loss: 2.8788\nIteration: 2144; Percent complete: 53.6%; Average loss: 2.9339\nIteration: 2145; Percent complete: 53.6%; Average loss: 3.0916\nIteration: 2146; Percent complete: 53.6%; Average loss: 2.9287\nIteration: 2147; Percent complete: 53.7%; Average loss: 2.9481\nIteration: 2148; Percent complete: 53.7%; Average loss: 3.0572\nIteration: 2149; Percent complete: 53.7%; Average loss: 2.9175\nIteration: 2150; Percent complete: 53.8%; Average loss: 3.1349\nIteration: 2151; Percent complete: 53.8%; Average loss: 3.2780\nIteration: 2152; Percent complete: 53.8%; Average loss: 2.9975\nIteration: 2153; Percent complete: 53.8%; Average loss: 3.1129\nIteration: 2154; Percent complete: 53.8%; Average loss: 3.3223\nIteration: 2155; Percent complete: 53.9%; Average loss: 3.1537\nIteration: 2156; Percent complete: 53.9%; Average loss: 3.2544\nIteration: 2157; Percent complete: 53.9%; Average loss: 3.0646\nIteration: 2158; Percent complete: 53.9%; Average loss: 2.9887\nIteration: 2159; Percent complete: 54.0%; Average loss: 3.0909\nIteration: 2160; Percent complete: 54.0%; Average loss: 3.3198\nIteration: 2161; Percent complete: 54.0%; Average loss: 3.1357\nIteration: 2162; Percent complete: 54.0%; Average loss: 2.9436\nIteration: 2163; Percent complete: 54.1%; Average loss: 3.1283\nIteration: 2164; Percent complete: 54.1%; Average loss: 3.0925\nIteration: 2165; Percent complete: 54.1%; Average loss: 3.1486\nIteration: 2166; Percent complete: 54.1%; Average loss: 3.3432\nIteration: 2167; Percent complete: 54.2%; Average loss: 3.1610\nIteration: 2168; Percent complete: 54.2%; Average loss: 3.0833\nIteration: 2169; Percent complete: 54.2%; Average loss: 2.9640\nIteration: 2170; Percent complete: 54.2%; Average loss: 3.1142\nIteration: 2171; Percent complete: 54.3%; Average loss: 3.3246\nIteration: 2172; Percent complete: 54.3%; Average loss: 2.8447\nIteration: 2173; Percent complete: 54.3%; Average loss: 2.9974\nIteration: 2174; Percent complete: 54.4%; Average loss: 3.1100\nIteration: 2175; Percent complete: 54.4%; Average loss: 2.8862\nIteration: 2176; Percent complete: 54.4%; Average loss: 3.1206\nIteration: 2177; Percent complete: 54.4%; Average loss: 2.9850\nIteration: 2178; Percent complete: 54.4%; Average loss: 3.0245\nIteration: 2179; Percent complete: 54.5%; Average loss: 3.0205\nIteration: 2180; Percent complete: 54.5%; Average loss: 3.1692\nIteration: 2181; Percent complete: 54.5%; Average loss: 3.0222\nIteration: 2182; Percent complete: 54.5%; Average loss: 3.1972\nIteration: 2183; Percent complete: 54.6%; Average loss: 3.0783\nIteration: 2184; Percent complete: 54.6%; Average loss: 3.3448\nIteration: 2185; Percent complete: 54.6%; Average loss: 3.0013\nIteration: 2186; Percent complete: 54.6%; Average loss: 3.1204\nIteration: 2187; Percent complete: 54.7%; Average loss: 3.2731\nIteration: 2188; Percent complete: 54.7%; Average loss: 2.9822\nIteration: 2189; Percent complete: 54.7%; Average loss: 3.2510\nIteration: 2190; Percent complete: 54.8%; Average loss: 3.0989\nIteration: 2191; Percent complete: 54.8%; Average loss: 3.0792\nIteration: 2192; Percent complete: 54.8%; Average loss: 3.0635\nIteration: 2193; Percent complete: 54.8%; Average loss: 3.1468\nIteration: 2194; Percent complete: 54.9%; Average loss: 3.1004\nIteration: 2195; Percent complete: 54.9%; Average loss: 3.0295\nIteration: 2196; Percent complete: 54.9%; Average loss: 3.0944\nIteration: 2197; Percent complete: 54.9%; Average loss: 3.3688\nIteration: 2198; Percent complete: 54.9%; Average loss: 3.2815\nIteration: 2199; Percent complete: 55.0%; Average loss: 3.2117\nIteration: 2200; Percent complete: 55.0%; Average loss: 2.8051\nIteration: 2201; Percent complete: 55.0%; Average loss: 3.2529\nIteration: 2202; Percent complete: 55.0%; Average loss: 3.0133\nIteration: 2203; Percent complete: 55.1%; Average loss: 3.1293\nIteration: 2204; Percent complete: 55.1%; Average loss: 3.1045\nIteration: 2205; Percent complete: 55.1%; Average loss: 3.2235\nIteration: 2206; Percent complete: 55.1%; Average loss: 2.8278\nIteration: 2207; Percent complete: 55.2%; Average loss: 2.7818\nIteration: 2208; Percent complete: 55.2%; Average loss: 3.0848\nIteration: 2209; Percent complete: 55.2%; Average loss: 3.2720\nIteration: 2210; Percent complete: 55.2%; Average loss: 3.1727\nIteration: 2211; Percent complete: 55.3%; Average loss: 3.0062\nIteration: 2212; Percent complete: 55.3%; Average loss: 3.2171\nIteration: 2213; Percent complete: 55.3%; Average loss: 2.9761\nIteration: 2214; Percent complete: 55.4%; Average loss: 3.1307\nIteration: 2215; Percent complete: 55.4%; Average loss: 3.1060\nIteration: 2216; Percent complete: 55.4%; Average loss: 3.1024\nIteration: 2217; Percent complete: 55.4%; Average loss: 3.2204\nIteration: 2218; Percent complete: 55.5%; Average loss: 2.9114\nIteration: 2219; Percent complete: 55.5%; Average loss: 3.1029\nIteration: 2220; Percent complete: 55.5%; Average loss: 2.9503\nIteration: 2221; Percent complete: 55.5%; Average loss: 3.2414\nIteration: 2222; Percent complete: 55.5%; Average loss: 3.2315\nIteration: 2223; Percent complete: 55.6%; Average loss: 2.9734\nIteration: 2224; Percent complete: 55.6%; Average loss: 3.1241\nIteration: 2225; Percent complete: 55.6%; Average loss: 2.8698\nIteration: 2226; Percent complete: 55.6%; Average loss: 3.2170\nIteration: 2227; Percent complete: 55.7%; Average loss: 2.9866\nIteration: 2228; Percent complete: 55.7%; Average loss: 3.2984\nIteration: 2229; Percent complete: 55.7%; Average loss: 3.1009\nIteration: 2230; Percent complete: 55.8%; Average loss: 3.0558\nIteration: 2231; Percent complete: 55.8%; Average loss: 3.0007\nIteration: 2232; Percent complete: 55.8%; Average loss: 3.1445\nIteration: 2233; Percent complete: 55.8%; Average loss: 3.1230\nIteration: 2234; Percent complete: 55.9%; Average loss: 3.1559\nIteration: 2235; Percent complete: 55.9%; Average loss: 2.9503\nIteration: 2236; Percent complete: 55.9%; Average loss: 3.0358\nIteration: 2237; Percent complete: 55.9%; Average loss: 3.3038\nIteration: 2238; Percent complete: 56.0%; Average loss: 2.9246\nIteration: 2239; Percent complete: 56.0%; Average loss: 3.1999\nIteration: 2240; Percent complete: 56.0%; Average loss: 3.1591\nIteration: 2241; Percent complete: 56.0%; Average loss: 3.1629\nIteration: 2242; Percent complete: 56.0%; Average loss: 3.0216\nIteration: 2243; Percent complete: 56.1%; Average loss: 3.1181\nIteration: 2244; Percent complete: 56.1%; Average loss: 3.0204\nIteration: 2245; Percent complete: 56.1%; Average loss: 3.3100\nIteration: 2246; Percent complete: 56.1%; Average loss: 3.1681\nIteration: 2247; Percent complete: 56.2%; Average loss: 3.1339\nIteration: 2248; Percent complete: 56.2%; Average loss: 3.0934\nIteration: 2249; Percent complete: 56.2%; Average loss: 3.0730\nIteration: 2250; Percent complete: 56.2%; Average loss: 2.8235\nIteration: 2251; Percent complete: 56.3%; Average loss: 3.1757\nIteration: 2252; Percent complete: 56.3%; Average loss: 3.1618\nIteration: 2253; Percent complete: 56.3%; Average loss: 3.2697\nIteration: 2254; Percent complete: 56.4%; Average loss: 3.0638\nIteration: 2255; Percent complete: 56.4%; Average loss: 3.0800\nIteration: 2256; Percent complete: 56.4%; Average loss: 3.3978\nIteration: 2257; Percent complete: 56.4%; Average loss: 3.0699\nIteration: 2258; Percent complete: 56.5%; Average loss: 3.0650\nIteration: 2259; Percent complete: 56.5%; Average loss: 3.1072\nIteration: 2260; Percent complete: 56.5%; Average loss: 3.1143\nIteration: 2261; Percent complete: 56.5%; Average loss: 3.2148\nIteration: 2262; Percent complete: 56.5%; Average loss: 2.8711\nIteration: 2263; Percent complete: 56.6%; Average loss: 3.2003\nIteration: 2264; Percent complete: 56.6%; Average loss: 2.8259\nIteration: 2265; Percent complete: 56.6%; Average loss: 2.9600\nIteration: 2266; Percent complete: 56.6%; Average loss: 3.4322\nIteration: 2267; Percent complete: 56.7%; Average loss: 3.1836\nIteration: 2268; Percent complete: 56.7%; Average loss: 3.0480\nIteration: 2269; Percent complete: 56.7%; Average loss: 2.9268\nIteration: 2270; Percent complete: 56.8%; Average loss: 2.9621\nIteration: 2271; Percent complete: 56.8%; Average loss: 3.0946\nIteration: 2272; Percent complete: 56.8%; Average loss: 2.8959\nIteration: 2273; Percent complete: 56.8%; Average loss: 3.2412\nIteration: 2274; Percent complete: 56.9%; Average loss: 2.9046\nIteration: 2275; Percent complete: 56.9%; Average loss: 2.9129\nIteration: 2276; Percent complete: 56.9%; Average loss: 3.1281\nIteration: 2277; Percent complete: 56.9%; Average loss: 3.0005\nIteration: 2278; Percent complete: 57.0%; Average loss: 2.9114\nIteration: 2279; Percent complete: 57.0%; Average loss: 3.0400\nIteration: 2280; Percent complete: 57.0%; Average loss: 3.1205\nIteration: 2281; Percent complete: 57.0%; Average loss: 3.1997\nIteration: 2282; Percent complete: 57.0%; Average loss: 3.3686\nIteration: 2283; Percent complete: 57.1%; Average loss: 3.0627\nIteration: 2284; Percent complete: 57.1%; Average loss: 3.2887\nIteration: 2285; Percent complete: 57.1%; Average loss: 3.2121\nIteration: 2286; Percent complete: 57.1%; Average loss: 3.1563\nIteration: 2287; Percent complete: 57.2%; Average loss: 3.3471\nIteration: 2288; Percent complete: 57.2%; Average loss: 3.0219\nIteration: 2289; Percent complete: 57.2%; Average loss: 2.8409\nIteration: 2290; Percent complete: 57.2%; Average loss: 3.2478\nIteration: 2291; Percent complete: 57.3%; Average loss: 3.0755\nIteration: 2292; Percent complete: 57.3%; Average loss: 3.0410\nIteration: 2293; Percent complete: 57.3%; Average loss: 2.8968\nIteration: 2294; Percent complete: 57.4%; Average loss: 3.1336\nIteration: 2295; Percent complete: 57.4%; Average loss: 3.0763\nIteration: 2296; Percent complete: 57.4%; Average loss: 2.9567\nIteration: 2297; Percent complete: 57.4%; Average loss: 2.8520\nIteration: 2298; Percent complete: 57.5%; Average loss: 3.3219\nIteration: 2299; Percent complete: 57.5%; Average loss: 3.0026\nIteration: 2300; Percent complete: 57.5%; Average loss: 3.1432\nIteration: 2301; Percent complete: 57.5%; Average loss: 3.0134\nIteration: 2302; Percent complete: 57.6%; Average loss: 3.0390\nIteration: 2303; Percent complete: 57.6%; Average loss: 3.2520\nIteration: 2304; Percent complete: 57.6%; Average loss: 2.9082\nIteration: 2305; Percent complete: 57.6%; Average loss: 3.2721\nIteration: 2306; Percent complete: 57.6%; Average loss: 3.1437\nIteration: 2307; Percent complete: 57.7%; Average loss: 2.9228\nIteration: 2308; Percent complete: 57.7%; Average loss: 3.2196\nIteration: 2309; Percent complete: 57.7%; Average loss: 3.1704\nIteration: 2310; Percent complete: 57.8%; Average loss: 2.9264\nIteration: 2311; Percent complete: 57.8%; Average loss: 2.8594\nIteration: 2312; Percent complete: 57.8%; Average loss: 2.8211\nIteration: 2313; Percent complete: 57.8%; Average loss: 2.8481\nIteration: 2314; Percent complete: 57.9%; Average loss: 3.0352\nIteration: 2315; Percent complete: 57.9%; Average loss: 3.1474\nIteration: 2316; Percent complete: 57.9%; Average loss: 3.0737\nIteration: 2317; Percent complete: 57.9%; Average loss: 3.1159\nIteration: 2318; Percent complete: 58.0%; Average loss: 3.0456\nIteration: 2319; Percent complete: 58.0%; Average loss: 2.9218\nIteration: 2320; Percent complete: 58.0%; Average loss: 3.1432\nIteration: 2321; Percent complete: 58.0%; Average loss: 2.9403\nIteration: 2322; Percent complete: 58.1%; Average loss: 2.8033\nIteration: 2323; Percent complete: 58.1%; Average loss: 3.0154\nIteration: 2324; Percent complete: 58.1%; Average loss: 2.7436\nIteration: 2325; Percent complete: 58.1%; Average loss: 3.0757\nIteration: 2326; Percent complete: 58.1%; Average loss: 2.8191\nIteration: 2327; Percent complete: 58.2%; Average loss: 3.1855\nIteration: 2328; Percent complete: 58.2%; Average loss: 2.9467\nIteration: 2329; Percent complete: 58.2%; Average loss: 3.0253\nIteration: 2330; Percent complete: 58.2%; Average loss: 3.1172\nIteration: 2331; Percent complete: 58.3%; Average loss: 2.8600\nIteration: 2332; Percent complete: 58.3%; Average loss: 3.1202\nIteration: 2333; Percent complete: 58.3%; Average loss: 2.8558\nIteration: 2334; Percent complete: 58.4%; Average loss: 3.0908\nIteration: 2335; Percent complete: 58.4%; Average loss: 3.1772\nIteration: 2336; Percent complete: 58.4%; Average loss: 3.1101\nIteration: 2337; Percent complete: 58.4%; Average loss: 2.9439\nIteration: 2338; Percent complete: 58.5%; Average loss: 2.8124\nIteration: 2339; Percent complete: 58.5%; Average loss: 3.0995\nIteration: 2340; Percent complete: 58.5%; Average loss: 2.8552\nIteration: 2341; Percent complete: 58.5%; Average loss: 2.9371\nIteration: 2342; Percent complete: 58.6%; Average loss: 3.2035\nIteration: 2343; Percent complete: 58.6%; Average loss: 2.9483\nIteration: 2344; Percent complete: 58.6%; Average loss: 3.1354\nIteration: 2345; Percent complete: 58.6%; Average loss: 2.9082\nIteration: 2346; Percent complete: 58.7%; Average loss: 2.8882\nIteration: 2347; Percent complete: 58.7%; Average loss: 3.1368\nIteration: 2348; Percent complete: 58.7%; Average loss: 3.1026\nIteration: 2349; Percent complete: 58.7%; Average loss: 3.1461\nIteration: 2350; Percent complete: 58.8%; Average loss: 3.1884\nIteration: 2351; Percent complete: 58.8%; Average loss: 3.0966\nIteration: 2352; Percent complete: 58.8%; Average loss: 3.3303\nIteration: 2353; Percent complete: 58.8%; Average loss: 3.2362\nIteration: 2354; Percent complete: 58.9%; Average loss: 2.9640\nIteration: 2355; Percent complete: 58.9%; Average loss: 2.9874\nIteration: 2356; Percent complete: 58.9%; Average loss: 3.0373\nIteration: 2357; Percent complete: 58.9%; Average loss: 3.2087\nIteration: 2358; Percent complete: 59.0%; Average loss: 3.0771\nIteration: 2359; Percent complete: 59.0%; Average loss: 2.9348\nIteration: 2360; Percent complete: 59.0%; Average loss: 3.2582\nIteration: 2361; Percent complete: 59.0%; Average loss: 3.0158\nIteration: 2362; Percent complete: 59.1%; Average loss: 2.9477\nIteration: 2363; Percent complete: 59.1%; Average loss: 2.8441\nIteration: 2364; Percent complete: 59.1%; Average loss: 2.9501\nIteration: 2365; Percent complete: 59.1%; Average loss: 3.0187\nIteration: 2366; Percent complete: 59.2%; Average loss: 3.1778\nIteration: 2367; Percent complete: 59.2%; Average loss: 3.1024\nIteration: 2368; Percent complete: 59.2%; Average loss: 2.8303\nIteration: 2369; Percent complete: 59.2%; Average loss: 2.7696\nIteration: 2370; Percent complete: 59.2%; Average loss: 3.0341\nIteration: 2371; Percent complete: 59.3%; Average loss: 2.8442\nIteration: 2372; Percent complete: 59.3%; Average loss: 3.2097\nIteration: 2373; Percent complete: 59.3%; Average loss: 3.0131\nIteration: 2374; Percent complete: 59.4%; Average loss: 3.0004\nIteration: 2375; Percent complete: 59.4%; Average loss: 2.8801\nIteration: 2376; Percent complete: 59.4%; Average loss: 2.9802\nIteration: 2377; Percent complete: 59.4%; Average loss: 3.1016\nIteration: 2378; Percent complete: 59.5%; Average loss: 2.9231\nIteration: 2379; Percent complete: 59.5%; Average loss: 3.0553\nIteration: 2380; Percent complete: 59.5%; Average loss: 3.0324\nIteration: 2381; Percent complete: 59.5%; Average loss: 3.1904\nIteration: 2382; Percent complete: 59.6%; Average loss: 2.8655\nIteration: 2383; Percent complete: 59.6%; Average loss: 3.1704\nIteration: 2384; Percent complete: 59.6%; Average loss: 2.9729\nIteration: 2385; Percent complete: 59.6%; Average loss: 3.0116\nIteration: 2386; Percent complete: 59.7%; Average loss: 3.0682\nIteration: 2387; Percent complete: 59.7%; Average loss: 3.0260\nIteration: 2388; Percent complete: 59.7%; Average loss: 2.8843\nIteration: 2389; Percent complete: 59.7%; Average loss: 2.8741\nIteration: 2390; Percent complete: 59.8%; Average loss: 2.9491\nIteration: 2391; Percent complete: 59.8%; Average loss: 3.0344\nIteration: 2392; Percent complete: 59.8%; Average loss: 2.9555\nIteration: 2393; Percent complete: 59.8%; Average loss: 3.3177\nIteration: 2394; Percent complete: 59.9%; Average loss: 3.4478\nIteration: 2395; Percent complete: 59.9%; Average loss: 2.9278\nIteration: 2396; Percent complete: 59.9%; Average loss: 2.7909\nIteration: 2397; Percent complete: 59.9%; Average loss: 3.0607\nIteration: 2398; Percent complete: 60.0%; Average loss: 3.0718\nIteration: 2399; Percent complete: 60.0%; Average loss: 2.9447\nIteration: 2400; Percent complete: 60.0%; Average loss: 3.0070\nIteration: 2401; Percent complete: 60.0%; Average loss: 3.2717\nIteration: 2402; Percent complete: 60.1%; Average loss: 2.9154\nIteration: 2403; Percent complete: 60.1%; Average loss: 3.0696\nIteration: 2404; Percent complete: 60.1%; Average loss: 2.8306\nIteration: 2405; Percent complete: 60.1%; Average loss: 3.0116\nIteration: 2406; Percent complete: 60.2%; Average loss: 3.0693\nIteration: 2407; Percent complete: 60.2%; Average loss: 2.9287\nIteration: 2408; Percent complete: 60.2%; Average loss: 2.9554\nIteration: 2409; Percent complete: 60.2%; Average loss: 2.9610\nIteration: 2410; Percent complete: 60.2%; Average loss: 3.0359\nIteration: 2411; Percent complete: 60.3%; Average loss: 3.1561\nIteration: 2412; Percent complete: 60.3%; Average loss: 3.1181\nIteration: 2413; Percent complete: 60.3%; Average loss: 2.9304\nIteration: 2414; Percent complete: 60.4%; Average loss: 2.9126\nIteration: 2415; Percent complete: 60.4%; Average loss: 3.2447\nIteration: 2416; Percent complete: 60.4%; Average loss: 2.9572\nIteration: 2417; Percent complete: 60.4%; Average loss: 3.2200\nIteration: 2418; Percent complete: 60.5%; Average loss: 2.8883\nIteration: 2419; Percent complete: 60.5%; Average loss: 2.9051\nIteration: 2420; Percent complete: 60.5%; Average loss: 2.8821\nIteration: 2421; Percent complete: 60.5%; Average loss: 2.9496\nIteration: 2422; Percent complete: 60.6%; Average loss: 2.8697\nIteration: 2423; Percent complete: 60.6%; Average loss: 3.0734\nIteration: 2424; Percent complete: 60.6%; Average loss: 2.9691\nIteration: 2425; Percent complete: 60.6%; Average loss: 3.0322\nIteration: 2426; Percent complete: 60.7%; Average loss: 2.9804\nIteration: 2427; Percent complete: 60.7%; Average loss: 3.2117\nIteration: 2428; Percent complete: 60.7%; Average loss: 2.8316\nIteration: 2429; Percent complete: 60.7%; Average loss: 2.9900\nIteration: 2430; Percent complete: 60.8%; Average loss: 3.2787\nIteration: 2431; Percent complete: 60.8%; Average loss: 2.9954\nIteration: 2432; Percent complete: 60.8%; Average loss: 2.9577\nIteration: 2433; Percent complete: 60.8%; Average loss: 3.1017\nIteration: 2434; Percent complete: 60.9%; Average loss: 2.8691\nIteration: 2435; Percent complete: 60.9%; Average loss: 2.8060\nIteration: 2436; Percent complete: 60.9%; Average loss: 3.0508\nIteration: 2437; Percent complete: 60.9%; Average loss: 3.1215\nIteration: 2438; Percent complete: 61.0%; Average loss: 3.2497\nIteration: 2439; Percent complete: 61.0%; Average loss: 2.9899\nIteration: 2440; Percent complete: 61.0%; Average loss: 2.8929\nIteration: 2441; Percent complete: 61.0%; Average loss: 3.1921\nIteration: 2442; Percent complete: 61.1%; Average loss: 3.0835\nIteration: 2443; Percent complete: 61.1%; Average loss: 3.1041\nIteration: 2444; Percent complete: 61.1%; Average loss: 3.1464\nIteration: 2445; Percent complete: 61.1%; Average loss: 3.1107\nIteration: 2446; Percent complete: 61.2%; Average loss: 3.3611\nIteration: 2447; Percent complete: 61.2%; Average loss: 2.9552\nIteration: 2448; Percent complete: 61.2%; Average loss: 2.9422\nIteration: 2449; Percent complete: 61.2%; Average loss: 2.8773\nIteration: 2450; Percent complete: 61.3%; Average loss: 3.1303\nIteration: 2451; Percent complete: 61.3%; Average loss: 2.9068\nIteration: 2452; Percent complete: 61.3%; Average loss: 2.9946\nIteration: 2453; Percent complete: 61.3%; Average loss: 2.9010\nIteration: 2454; Percent complete: 61.4%; Average loss: 3.1017\nIteration: 2455; Percent complete: 61.4%; Average loss: 2.9909\nIteration: 2456; Percent complete: 61.4%; Average loss: 3.1973\nIteration: 2457; Percent complete: 61.4%; Average loss: 2.9747\nIteration: 2458; Percent complete: 61.5%; Average loss: 3.0290\nIteration: 2459; Percent complete: 61.5%; Average loss: 2.9583\nIteration: 2460; Percent complete: 61.5%; Average loss: 3.0595\nIteration: 2461; Percent complete: 61.5%; Average loss: 3.0109\nIteration: 2462; Percent complete: 61.6%; Average loss: 2.8759\nIteration: 2463; Percent complete: 61.6%; Average loss: 3.1632\nIteration: 2464; Percent complete: 61.6%; Average loss: 2.9112\nIteration: 2465; Percent complete: 61.6%; Average loss: 2.8555\nIteration: 2466; Percent complete: 61.7%; Average loss: 3.0676\nIteration: 2467; Percent complete: 61.7%; Average loss: 2.7501\nIteration: 2468; Percent complete: 61.7%; Average loss: 3.0826\nIteration: 2469; Percent complete: 61.7%; Average loss: 3.0452\nIteration: 2470; Percent complete: 61.8%; Average loss: 3.0278\nIteration: 2471; Percent complete: 61.8%; Average loss: 2.9524\nIteration: 2472; Percent complete: 61.8%; Average loss: 2.9643\nIteration: 2473; Percent complete: 61.8%; Average loss: 3.0050\nIteration: 2474; Percent complete: 61.9%; Average loss: 3.2178\nIteration: 2475; Percent complete: 61.9%; Average loss: 3.0657\nIteration: 2476; Percent complete: 61.9%; Average loss: 2.9466\nIteration: 2477; Percent complete: 61.9%; Average loss: 3.1142\nIteration: 2478; Percent complete: 62.0%; Average loss: 2.9518\nIteration: 2479; Percent complete: 62.0%; Average loss: 3.0039\nIteration: 2480; Percent complete: 62.0%; Average loss: 3.1440\nIteration: 2481; Percent complete: 62.0%; Average loss: 2.9393\nIteration: 2482; Percent complete: 62.1%; Average loss: 2.9626\nIteration: 2483; Percent complete: 62.1%; Average loss: 2.9732\nIteration: 2484; Percent complete: 62.1%; Average loss: 2.9480\nIteration: 2485; Percent complete: 62.1%; Average loss: 3.1770\nIteration: 2486; Percent complete: 62.2%; Average loss: 3.0652\nIteration: 2487; Percent complete: 62.2%; Average loss: 2.9782\nIteration: 2488; Percent complete: 62.2%; Average loss: 2.8361\nIteration: 2489; Percent complete: 62.2%; Average loss: 2.7699\nIteration: 2490; Percent complete: 62.3%; Average loss: 2.9304\nIteration: 2491; Percent complete: 62.3%; Average loss: 3.0758\nIteration: 2492; Percent complete: 62.3%; Average loss: 3.0152\nIteration: 2493; Percent complete: 62.3%; Average loss: 3.2259\nIteration: 2494; Percent complete: 62.4%; Average loss: 2.8268\nIteration: 2495; Percent complete: 62.4%; Average loss: 3.1917\nIteration: 2496; Percent complete: 62.4%; Average loss: 2.9922\nIteration: 2497; Percent complete: 62.4%; Average loss: 3.0814\nIteration: 2498; Percent complete: 62.5%; Average loss: 2.8673\nIteration: 2499; Percent complete: 62.5%; Average loss: 2.9771\nIteration: 2500; Percent complete: 62.5%; Average loss: 3.1022\nIteration: 2501; Percent complete: 62.5%; Average loss: 2.9839\nIteration: 2502; Percent complete: 62.5%; Average loss: 2.9720\nIteration: 2503; Percent complete: 62.6%; Average loss: 3.1701\nIteration: 2504; Percent complete: 62.6%; Average loss: 3.2190\nIteration: 2505; Percent complete: 62.6%; Average loss: 2.9681\nIteration: 2506; Percent complete: 62.6%; Average loss: 3.2687\nIteration: 2507; Percent complete: 62.7%; Average loss: 3.1586\nIteration: 2508; Percent complete: 62.7%; Average loss: 2.6662\nIteration: 2509; Percent complete: 62.7%; Average loss: 2.9393\nIteration: 2510; Percent complete: 62.7%; Average loss: 2.7260\nIteration: 2511; Percent complete: 62.8%; Average loss: 3.1051\nIteration: 2512; Percent complete: 62.8%; Average loss: 2.9074\nIteration: 2513; Percent complete: 62.8%; Average loss: 3.1521\nIteration: 2514; Percent complete: 62.8%; Average loss: 3.0740\nIteration: 2515; Percent complete: 62.9%; Average loss: 2.9576\nIteration: 2516; Percent complete: 62.9%; Average loss: 2.8930\nIteration: 2517; Percent complete: 62.9%; Average loss: 2.9063\nIteration: 2518; Percent complete: 62.9%; Average loss: 3.1495\nIteration: 2519; Percent complete: 63.0%; Average loss: 3.1204\nIteration: 2520; Percent complete: 63.0%; Average loss: 2.8258\nIteration: 2521; Percent complete: 63.0%; Average loss: 3.1073\nIteration: 2522; Percent complete: 63.0%; Average loss: 2.9743\nIteration: 2523; Percent complete: 63.1%; Average loss: 3.3750\nIteration: 2524; Percent complete: 63.1%; Average loss: 3.2054\nIteration: 2525; Percent complete: 63.1%; Average loss: 3.0086\nIteration: 2526; Percent complete: 63.1%; Average loss: 2.9062\nIteration: 2527; Percent complete: 63.2%; Average loss: 2.8190\nIteration: 2528; Percent complete: 63.2%; Average loss: 2.7942\nIteration: 2529; Percent complete: 63.2%; Average loss: 2.9562\nIteration: 2530; Percent complete: 63.2%; Average loss: 2.8436\nIteration: 2531; Percent complete: 63.3%; Average loss: 2.9055\nIteration: 2532; Percent complete: 63.3%; Average loss: 3.1120\nIteration: 2533; Percent complete: 63.3%; Average loss: 3.2967\nIteration: 2534; Percent complete: 63.3%; Average loss: 3.0331\nIteration: 2535; Percent complete: 63.4%; Average loss: 2.7651\nIteration: 2536; Percent complete: 63.4%; Average loss: 3.1052\nIteration: 2537; Percent complete: 63.4%; Average loss: 3.0762\nIteration: 2538; Percent complete: 63.4%; Average loss: 2.7298\nIteration: 2539; Percent complete: 63.5%; Average loss: 2.7802\nIteration: 2540; Percent complete: 63.5%; Average loss: 3.0984\nIteration: 2541; Percent complete: 63.5%; Average loss: 2.9153\nIteration: 2542; Percent complete: 63.5%; Average loss: 3.0523\nIteration: 2543; Percent complete: 63.6%; Average loss: 2.9578\nIteration: 2544; Percent complete: 63.6%; Average loss: 3.0569\nIteration: 2545; Percent complete: 63.6%; Average loss: 2.9302\nIteration: 2546; Percent complete: 63.6%; Average loss: 2.9281\nIteration: 2547; Percent complete: 63.7%; Average loss: 2.9693\nIteration: 2548; Percent complete: 63.7%; Average loss: 3.1212\nIteration: 2549; Percent complete: 63.7%; Average loss: 3.1797\nIteration: 2550; Percent complete: 63.7%; Average loss: 2.7627\nIteration: 2551; Percent complete: 63.8%; Average loss: 3.0774\nIteration: 2552; Percent complete: 63.8%; Average loss: 3.0175\nIteration: 2553; Percent complete: 63.8%; Average loss: 2.9514\nIteration: 2554; Percent complete: 63.8%; Average loss: 2.9009\nIteration: 2555; Percent complete: 63.9%; Average loss: 3.0352\nIteration: 2556; Percent complete: 63.9%; Average loss: 2.8194\nIteration: 2557; Percent complete: 63.9%; Average loss: 2.8940\nIteration: 2558; Percent complete: 63.9%; Average loss: 3.1330\nIteration: 2559; Percent complete: 64.0%; Average loss: 2.6637\nIteration: 2560; Percent complete: 64.0%; Average loss: 3.1542\nIteration: 2561; Percent complete: 64.0%; Average loss: 3.0568\nIteration: 2562; Percent complete: 64.0%; Average loss: 3.0805\nIteration: 2563; Percent complete: 64.1%; Average loss: 2.7105\nIteration: 2564; Percent complete: 64.1%; Average loss: 2.8907\nIteration: 2565; Percent complete: 64.1%; Average loss: 3.0787\nIteration: 2566; Percent complete: 64.1%; Average loss: 2.9166\nIteration: 2567; Percent complete: 64.2%; Average loss: 2.9756\nIteration: 2568; Percent complete: 64.2%; Average loss: 2.9957\nIteration: 2569; Percent complete: 64.2%; Average loss: 2.7843\nIteration: 2570; Percent complete: 64.2%; Average loss: 3.0735\nIteration: 2571; Percent complete: 64.3%; Average loss: 2.8943\nIteration: 2572; Percent complete: 64.3%; Average loss: 3.1497\nIteration: 2573; Percent complete: 64.3%; Average loss: 3.0941\nIteration: 2574; Percent complete: 64.3%; Average loss: 2.8466\nIteration: 2575; Percent complete: 64.4%; Average loss: 2.9699\nIteration: 2576; Percent complete: 64.4%; Average loss: 3.2465\nIteration: 2577; Percent complete: 64.4%; Average loss: 3.0331\nIteration: 2578; Percent complete: 64.5%; Average loss: 3.2694\nIteration: 2579; Percent complete: 64.5%; Average loss: 2.8945\nIteration: 2580; Percent complete: 64.5%; Average loss: 3.2947\nIteration: 2581; Percent complete: 64.5%; Average loss: 3.1053\nIteration: 2582; Percent complete: 64.5%; Average loss: 2.9105\nIteration: 2583; Percent complete: 64.6%; Average loss: 3.0404\nIteration: 2584; Percent complete: 64.6%; Average loss: 3.0720\nIteration: 2585; Percent complete: 64.6%; Average loss: 3.1743\nIteration: 2586; Percent complete: 64.6%; Average loss: 2.8426\nIteration: 2587; Percent complete: 64.7%; Average loss: 2.9663\nIteration: 2588; Percent complete: 64.7%; Average loss: 2.9726\nIteration: 2589; Percent complete: 64.7%; Average loss: 2.7452\nIteration: 2590; Percent complete: 64.8%; Average loss: 2.8744\nIteration: 2591; Percent complete: 64.8%; Average loss: 3.0081\nIteration: 2592; Percent complete: 64.8%; Average loss: 3.0706\nIteration: 2593; Percent complete: 64.8%; Average loss: 3.0197\nIteration: 2594; Percent complete: 64.8%; Average loss: 3.1186\nIteration: 2595; Percent complete: 64.9%; Average loss: 2.9524\nIteration: 2596; Percent complete: 64.9%; Average loss: 3.0212\nIteration: 2597; Percent complete: 64.9%; Average loss: 2.9030\nIteration: 2598; Percent complete: 65.0%; Average loss: 2.8421\nIteration: 2599; Percent complete: 65.0%; Average loss: 3.1475\nIteration: 2600; Percent complete: 65.0%; Average loss: 2.9266\nIteration: 2601; Percent complete: 65.0%; Average loss: 3.0638\nIteration: 2602; Percent complete: 65.0%; Average loss: 2.8385\nIteration: 2603; Percent complete: 65.1%; Average loss: 2.9311\nIteration: 2604; Percent complete: 65.1%; Average loss: 2.6921\nIteration: 2605; Percent complete: 65.1%; Average loss: 3.1303\nIteration: 2606; Percent complete: 65.1%; Average loss: 2.9563\nIteration: 2607; Percent complete: 65.2%; Average loss: 3.1481\nIteration: 2608; Percent complete: 65.2%; Average loss: 2.6808\nIteration: 2609; Percent complete: 65.2%; Average loss: 3.1330\nIteration: 2610; Percent complete: 65.2%; Average loss: 2.9807\nIteration: 2611; Percent complete: 65.3%; Average loss: 2.9887\nIteration: 2612; Percent complete: 65.3%; Average loss: 3.1119\nIteration: 2613; Percent complete: 65.3%; Average loss: 2.8530\nIteration: 2614; Percent complete: 65.3%; Average loss: 3.1544\nIteration: 2615; Percent complete: 65.4%; Average loss: 2.8571\nIteration: 2616; Percent complete: 65.4%; Average loss: 2.7785\nIteration: 2617; Percent complete: 65.4%; Average loss: 2.8984\nIteration: 2618; Percent complete: 65.5%; Average loss: 2.9561\nIteration: 2619; Percent complete: 65.5%; Average loss: 3.0475\nIteration: 2620; Percent complete: 65.5%; Average loss: 3.0681\nIteration: 2621; Percent complete: 65.5%; Average loss: 2.9469\nIteration: 2622; Percent complete: 65.5%; Average loss: 2.9799\nIteration: 2623; Percent complete: 65.6%; Average loss: 3.0262\nIteration: 2624; Percent complete: 65.6%; Average loss: 2.8772\nIteration: 2625; Percent complete: 65.6%; Average loss: 3.0224\nIteration: 2626; Percent complete: 65.6%; Average loss: 2.9597\nIteration: 2627; Percent complete: 65.7%; Average loss: 2.8670\nIteration: 2628; Percent complete: 65.7%; Average loss: 3.0548\nIteration: 2629; Percent complete: 65.7%; Average loss: 2.9718\nIteration: 2630; Percent complete: 65.8%; Average loss: 3.0244\nIteration: 2631; Percent complete: 65.8%; Average loss: 2.8506\nIteration: 2632; Percent complete: 65.8%; Average loss: 3.0487\nIteration: 2633; Percent complete: 65.8%; Average loss: 3.0478\nIteration: 2634; Percent complete: 65.8%; Average loss: 3.0958\nIteration: 2635; Percent complete: 65.9%; Average loss: 2.9930\nIteration: 2636; Percent complete: 65.9%; Average loss: 2.8496\nIteration: 2637; Percent complete: 65.9%; Average loss: 2.9644\nIteration: 2638; Percent complete: 66.0%; Average loss: 2.9570\nIteration: 2639; Percent complete: 66.0%; Average loss: 2.9547\nIteration: 2640; Percent complete: 66.0%; Average loss: 2.8570\nIteration: 2641; Percent complete: 66.0%; Average loss: 3.0776\nIteration: 2642; Percent complete: 66.0%; Average loss: 2.9157\nIteration: 2643; Percent complete: 66.1%; Average loss: 2.8260\nIteration: 2644; Percent complete: 66.1%; Average loss: 3.1313\nIteration: 2645; Percent complete: 66.1%; Average loss: 2.8677\nIteration: 2646; Percent complete: 66.1%; Average loss: 3.1838\nIteration: 2647; Percent complete: 66.2%; Average loss: 2.8356\nIteration: 2648; Percent complete: 66.2%; Average loss: 3.1245\nIteration: 2649; Percent complete: 66.2%; Average loss: 3.0851\nIteration: 2650; Percent complete: 66.2%; Average loss: 3.0940\nIteration: 2651; Percent complete: 66.3%; Average loss: 3.3121\nIteration: 2652; Percent complete: 66.3%; Average loss: 2.9538\nIteration: 2653; Percent complete: 66.3%; Average loss: 2.9273\nIteration: 2654; Percent complete: 66.3%; Average loss: 3.0619\nIteration: 2655; Percent complete: 66.4%; Average loss: 2.9668\nIteration: 2656; Percent complete: 66.4%; Average loss: 3.0289\nIteration: 2657; Percent complete: 66.4%; Average loss: 2.8361\nIteration: 2658; Percent complete: 66.5%; Average loss: 2.8662\nIteration: 2659; Percent complete: 66.5%; Average loss: 2.9618\nIteration: 2660; Percent complete: 66.5%; Average loss: 3.0376\nIteration: 2661; Percent complete: 66.5%; Average loss: 2.8951\nIteration: 2662; Percent complete: 66.5%; Average loss: 2.8619\nIteration: 2663; Percent complete: 66.6%; Average loss: 2.7971\nIteration: 2664; Percent complete: 66.6%; Average loss: 3.1209\nIteration: 2665; Percent complete: 66.6%; Average loss: 2.8006\nIteration: 2666; Percent complete: 66.6%; Average loss: 3.1413\nIteration: 2667; Percent complete: 66.7%; Average loss: 3.0152\nIteration: 2668; Percent complete: 66.7%; Average loss: 3.0679\nIteration: 2669; Percent complete: 66.7%; Average loss: 2.7804\nIteration: 2670; Percent complete: 66.8%; Average loss: 2.9272\nIteration: 2671; Percent complete: 66.8%; Average loss: 3.0809\nIteration: 2672; Percent complete: 66.8%; Average loss: 2.7626\nIteration: 2673; Percent complete: 66.8%; Average loss: 2.7616\nIteration: 2674; Percent complete: 66.8%; Average loss: 2.8771\nIteration: 2675; Percent complete: 66.9%; Average loss: 3.0056\nIteration: 2676; Percent complete: 66.9%; Average loss: 3.1268\nIteration: 2677; Percent complete: 66.9%; Average loss: 3.0055\nIteration: 2678; Percent complete: 67.0%; Average loss: 3.1016\nIteration: 2679; Percent complete: 67.0%; Average loss: 3.0860\nIteration: 2680; Percent complete: 67.0%; Average loss: 2.7231\nIteration: 2681; Percent complete: 67.0%; Average loss: 2.8743\nIteration: 2682; Percent complete: 67.0%; Average loss: 3.0374\nIteration: 2683; Percent complete: 67.1%; Average loss: 2.7892\nIteration: 2684; Percent complete: 67.1%; Average loss: 2.9158\nIteration: 2685; Percent complete: 67.1%; Average loss: 2.9082\nIteration: 2686; Percent complete: 67.2%; Average loss: 3.0939\nIteration: 2687; Percent complete: 67.2%; Average loss: 3.1350\nIteration: 2688; Percent complete: 67.2%; Average loss: 3.1556\nIteration: 2689; Percent complete: 67.2%; Average loss: 2.9803\nIteration: 2690; Percent complete: 67.2%; Average loss: 2.8870\nIteration: 2691; Percent complete: 67.3%; Average loss: 3.0333\nIteration: 2692; Percent complete: 67.3%; Average loss: 2.8998\nIteration: 2693; Percent complete: 67.3%; Average loss: 3.1455\nIteration: 2694; Percent complete: 67.3%; Average loss: 3.2152\nIteration: 2695; Percent complete: 67.4%; Average loss: 2.8884\nIteration: 2696; Percent complete: 67.4%; Average loss: 2.8971\nIteration: 2697; Percent complete: 67.4%; Average loss: 2.9054\nIteration: 2698; Percent complete: 67.5%; Average loss: 2.9766\nIteration: 2699; Percent complete: 67.5%; Average loss: 2.8702\nIteration: 2700; Percent complete: 67.5%; Average loss: 2.7922\nIteration: 2701; Percent complete: 67.5%; Average loss: 3.0036\nIteration: 2702; Percent complete: 67.5%; Average loss: 2.7924\nIteration: 2703; Percent complete: 67.6%; Average loss: 3.1065\nIteration: 2704; Percent complete: 67.6%; Average loss: 3.0001\nIteration: 2705; Percent complete: 67.6%; Average loss: 2.7668\nIteration: 2706; Percent complete: 67.7%; Average loss: 2.9596\nIteration: 2707; Percent complete: 67.7%; Average loss: 2.8138\nIteration: 2708; Percent complete: 67.7%; Average loss: 3.0077\nIteration: 2709; Percent complete: 67.7%; Average loss: 2.9595\nIteration: 2710; Percent complete: 67.8%; Average loss: 2.8242\nIteration: 2711; Percent complete: 67.8%; Average loss: 2.7181\nIteration: 2712; Percent complete: 67.8%; Average loss: 2.8513\nIteration: 2713; Percent complete: 67.8%; Average loss: 2.9990\nIteration: 2714; Percent complete: 67.8%; Average loss: 3.0135\nIteration: 2715; Percent complete: 67.9%; Average loss: 2.9590\nIteration: 2716; Percent complete: 67.9%; Average loss: 2.8292\nIteration: 2717; Percent complete: 67.9%; Average loss: 2.8943\nIteration: 2718; Percent complete: 68.0%; Average loss: 2.7732\nIteration: 2719; Percent complete: 68.0%; Average loss: 3.0007\nIteration: 2720; Percent complete: 68.0%; Average loss: 2.6478\nIteration: 2721; Percent complete: 68.0%; Average loss: 2.9383\nIteration: 2722; Percent complete: 68.0%; Average loss: 2.6512\nIteration: 2723; Percent complete: 68.1%; Average loss: 2.8870\nIteration: 2724; Percent complete: 68.1%; Average loss: 2.8604\nIteration: 2725; Percent complete: 68.1%; Average loss: 3.0194\nIteration: 2726; Percent complete: 68.2%; Average loss: 2.9083\nIteration: 2727; Percent complete: 68.2%; Average loss: 3.2360\nIteration: 2728; Percent complete: 68.2%; Average loss: 2.8117\nIteration: 2729; Percent complete: 68.2%; Average loss: 2.8078\nIteration: 2730; Percent complete: 68.2%; Average loss: 2.8479\nIteration: 2731; Percent complete: 68.3%; Average loss: 3.0506\nIteration: 2732; Percent complete: 68.3%; Average loss: 3.0683\nIteration: 2733; Percent complete: 68.3%; Average loss: 3.0255\nIteration: 2734; Percent complete: 68.3%; Average loss: 2.9388\nIteration: 2735; Percent complete: 68.4%; Average loss: 3.0021\nIteration: 2736; Percent complete: 68.4%; Average loss: 3.1183\nIteration: 2737; Percent complete: 68.4%; Average loss: 2.9945\nIteration: 2738; Percent complete: 68.5%; Average loss: 2.7862\nIteration: 2739; Percent complete: 68.5%; Average loss: 3.2774\nIteration: 2740; Percent complete: 68.5%; Average loss: 2.9959\nIteration: 2741; Percent complete: 68.5%; Average loss: 3.1202\nIteration: 2742; Percent complete: 68.5%; Average loss: 2.7069\nIteration: 2743; Percent complete: 68.6%; Average loss: 2.8309\nIteration: 2744; Percent complete: 68.6%; Average loss: 2.7343\nIteration: 2745; Percent complete: 68.6%; Average loss: 2.7592\nIteration: 2746; Percent complete: 68.7%; Average loss: 3.0390\nIteration: 2747; Percent complete: 68.7%; Average loss: 3.0713\nIteration: 2748; Percent complete: 68.7%; Average loss: 2.8011\nIteration: 2749; Percent complete: 68.7%; Average loss: 2.8274\nIteration: 2750; Percent complete: 68.8%; Average loss: 3.0008\nIteration: 2751; Percent complete: 68.8%; Average loss: 3.1282\nIteration: 2752; Percent complete: 68.8%; Average loss: 3.1865\nIteration: 2753; Percent complete: 68.8%; Average loss: 2.9586\nIteration: 2754; Percent complete: 68.8%; Average loss: 3.1017\nIteration: 2755; Percent complete: 68.9%; Average loss: 3.0130\nIteration: 2756; Percent complete: 68.9%; Average loss: 3.0711\nIteration: 2757; Percent complete: 68.9%; Average loss: 2.9001\nIteration: 2758; Percent complete: 69.0%; Average loss: 3.1154\nIteration: 2759; Percent complete: 69.0%; Average loss: 2.9738\nIteration: 2760; Percent complete: 69.0%; Average loss: 2.9840\nIteration: 2761; Percent complete: 69.0%; Average loss: 2.9522\nIteration: 2762; Percent complete: 69.0%; Average loss: 3.2513\nIteration: 2763; Percent complete: 69.1%; Average loss: 2.8899\nIteration: 2764; Percent complete: 69.1%; Average loss: 3.2699\nIteration: 2765; Percent complete: 69.1%; Average loss: 2.6767\nIteration: 2766; Percent complete: 69.2%; Average loss: 2.9669\nIteration: 2767; Percent complete: 69.2%; Average loss: 3.0290\nIteration: 2768; Percent complete: 69.2%; Average loss: 2.8535\nIteration: 2769; Percent complete: 69.2%; Average loss: 2.9166\nIteration: 2770; Percent complete: 69.2%; Average loss: 2.9665\nIteration: 2771; Percent complete: 69.3%; Average loss: 2.9184\nIteration: 2772; Percent complete: 69.3%; Average loss: 2.8983\nIteration: 2773; Percent complete: 69.3%; Average loss: 3.0441\nIteration: 2774; Percent complete: 69.3%; Average loss: 3.0029\nIteration: 2775; Percent complete: 69.4%; Average loss: 2.8013\nIteration: 2776; Percent complete: 69.4%; Average loss: 3.0467\nIteration: 2777; Percent complete: 69.4%; Average loss: 2.9060\nIteration: 2778; Percent complete: 69.5%; Average loss: 2.8157\nIteration: 2779; Percent complete: 69.5%; Average loss: 2.6680\nIteration: 2780; Percent complete: 69.5%; Average loss: 3.0726\nIteration: 2781; Percent complete: 69.5%; Average loss: 2.6675\nIteration: 2782; Percent complete: 69.5%; Average loss: 2.8852\nIteration: 2783; Percent complete: 69.6%; Average loss: 2.9512\nIteration: 2784; Percent complete: 69.6%; Average loss: 2.9167\nIteration: 2785; Percent complete: 69.6%; Average loss: 2.8015\nIteration: 2786; Percent complete: 69.7%; Average loss: 3.0494\nIteration: 2787; Percent complete: 69.7%; Average loss: 2.9060\nIteration: 2788; Percent complete: 69.7%; Average loss: 2.8612\nIteration: 2789; Percent complete: 69.7%; Average loss: 3.0404\nIteration: 2790; Percent complete: 69.8%; Average loss: 3.0595\nIteration: 2791; Percent complete: 69.8%; Average loss: 2.9446\nIteration: 2792; Percent complete: 69.8%; Average loss: 2.9746\nIteration: 2793; Percent complete: 69.8%; Average loss: 2.8943\nIteration: 2794; Percent complete: 69.8%; Average loss: 2.8406\nIteration: 2795; Percent complete: 69.9%; Average loss: 2.6724\nIteration: 2796; Percent complete: 69.9%; Average loss: 3.0550\nIteration: 2797; Percent complete: 69.9%; Average loss: 2.8759\nIteration: 2798; Percent complete: 70.0%; Average loss: 2.7552\nIteration: 2799; Percent complete: 70.0%; Average loss: 2.9633\nIteration: 2800; Percent complete: 70.0%; Average loss: 3.0025\nIteration: 2801; Percent complete: 70.0%; Average loss: 2.8867\nIteration: 2802; Percent complete: 70.0%; Average loss: 3.1300\nIteration: 2803; Percent complete: 70.1%; Average loss: 2.8817\nIteration: 2804; Percent complete: 70.1%; Average loss: 3.0319\nIteration: 2805; Percent complete: 70.1%; Average loss: 2.6972\nIteration: 2806; Percent complete: 70.2%; Average loss: 2.6345\nIteration: 2807; Percent complete: 70.2%; Average loss: 3.1107\nIteration: 2808; Percent complete: 70.2%; Average loss: 2.7778\nIteration: 2809; Percent complete: 70.2%; Average loss: 2.9476\nIteration: 2810; Percent complete: 70.2%; Average loss: 2.9715\nIteration: 2811; Percent complete: 70.3%; Average loss: 3.0096\nIteration: 2812; Percent complete: 70.3%; Average loss: 2.6231\nIteration: 2813; Percent complete: 70.3%; Average loss: 2.7767\nIteration: 2814; Percent complete: 70.3%; Average loss: 3.1318\nIteration: 2815; Percent complete: 70.4%; Average loss: 3.1241\nIteration: 2816; Percent complete: 70.4%; Average loss: 2.8290\nIteration: 2817; Percent complete: 70.4%; Average loss: 2.9403\nIteration: 2818; Percent complete: 70.5%; Average loss: 3.1410\nIteration: 2819; Percent complete: 70.5%; Average loss: 2.8400\nIteration: 2820; Percent complete: 70.5%; Average loss: 2.7225\nIteration: 2821; Percent complete: 70.5%; Average loss: 2.8434\nIteration: 2822; Percent complete: 70.5%; Average loss: 2.9541\nIteration: 2823; Percent complete: 70.6%; Average loss: 2.9491\nIteration: 2824; Percent complete: 70.6%; Average loss: 2.9203\nIteration: 2825; Percent complete: 70.6%; Average loss: 3.0708\nIteration: 2826; Percent complete: 70.7%; Average loss: 2.8433\nIteration: 2827; Percent complete: 70.7%; Average loss: 2.9287\nIteration: 2828; Percent complete: 70.7%; Average loss: 2.9787\nIteration: 2829; Percent complete: 70.7%; Average loss: 3.1608\nIteration: 2830; Percent complete: 70.8%; Average loss: 2.8909\nIteration: 2831; Percent complete: 70.8%; Average loss: 2.9665\nIteration: 2832; Percent complete: 70.8%; Average loss: 3.1463\nIteration: 2833; Percent complete: 70.8%; Average loss: 3.0613\nIteration: 2834; Percent complete: 70.9%; Average loss: 2.9221\nIteration: 2835; Percent complete: 70.9%; Average loss: 2.9823\nIteration: 2836; Percent complete: 70.9%; Average loss: 2.7030\nIteration: 2837; Percent complete: 70.9%; Average loss: 2.8529\nIteration: 2838; Percent complete: 71.0%; Average loss: 2.7976\nIteration: 2839; Percent complete: 71.0%; Average loss: 2.8879\nIteration: 2840; Percent complete: 71.0%; Average loss: 2.8134\nIteration: 2841; Percent complete: 71.0%; Average loss: 2.7151\nIteration: 2842; Percent complete: 71.0%; Average loss: 2.9766\nIteration: 2843; Percent complete: 71.1%; Average loss: 2.8513\nIteration: 2844; Percent complete: 71.1%; Average loss: 2.8917\nIteration: 2845; Percent complete: 71.1%; Average loss: 2.7369\nIteration: 2846; Percent complete: 71.2%; Average loss: 2.7967\nIteration: 2847; Percent complete: 71.2%; Average loss: 2.7545\nIteration: 2848; Percent complete: 71.2%; Average loss: 2.9205\nIteration: 2849; Percent complete: 71.2%; Average loss: 2.8688\nIteration: 2850; Percent complete: 71.2%; Average loss: 2.8212\nIteration: 2851; Percent complete: 71.3%; Average loss: 3.0108\nIteration: 2852; Percent complete: 71.3%; Average loss: 2.9584\nIteration: 2853; Percent complete: 71.3%; Average loss: 2.9606\nIteration: 2854; Percent complete: 71.4%; Average loss: 2.9843\nIteration: 2855; Percent complete: 71.4%; Average loss: 2.8742\nIteration: 2856; Percent complete: 71.4%; Average loss: 3.0873\nIteration: 2857; Percent complete: 71.4%; Average loss: 2.8299\nIteration: 2858; Percent complete: 71.5%; Average loss: 2.8773\nIteration: 2859; Percent complete: 71.5%; Average loss: 2.6443\nIteration: 2860; Percent complete: 71.5%; Average loss: 3.0741\nIteration: 2861; Percent complete: 71.5%; Average loss: 2.8728\nIteration: 2862; Percent complete: 71.5%; Average loss: 3.0293\nIteration: 2863; Percent complete: 71.6%; Average loss: 2.9021\nIteration: 2864; Percent complete: 71.6%; Average loss: 2.8031\nIteration: 2865; Percent complete: 71.6%; Average loss: 2.9720\nIteration: 2866; Percent complete: 71.7%; Average loss: 2.8820\nIteration: 2867; Percent complete: 71.7%; Average loss: 2.8344\nIteration: 2868; Percent complete: 71.7%; Average loss: 2.8544\nIteration: 2869; Percent complete: 71.7%; Average loss: 2.8687\nIteration: 2870; Percent complete: 71.8%; Average loss: 3.0050\nIteration: 2871; Percent complete: 71.8%; Average loss: 3.0698\nIteration: 2872; Percent complete: 71.8%; Average loss: 2.8611\nIteration: 2873; Percent complete: 71.8%; Average loss: 2.9150\nIteration: 2874; Percent complete: 71.9%; Average loss: 3.0438\nIteration: 2875; Percent complete: 71.9%; Average loss: 2.7605\nIteration: 2876; Percent complete: 71.9%; Average loss: 2.9258\nIteration: 2877; Percent complete: 71.9%; Average loss: 2.9906\nIteration: 2878; Percent complete: 72.0%; Average loss: 2.9563\nIteration: 2879; Percent complete: 72.0%; Average loss: 3.0083\nIteration: 2880; Percent complete: 72.0%; Average loss: 2.8353\nIteration: 2881; Percent complete: 72.0%; Average loss: 3.1247\nIteration: 2882; Percent complete: 72.0%; Average loss: 2.9226\nIteration: 2883; Percent complete: 72.1%; Average loss: 3.0169\nIteration: 2884; Percent complete: 72.1%; Average loss: 3.1328\nIteration: 2885; Percent complete: 72.1%; Average loss: 2.8497\nIteration: 2886; Percent complete: 72.2%; Average loss: 3.1534\nIteration: 2887; Percent complete: 72.2%; Average loss: 2.7811\nIteration: 2888; Percent complete: 72.2%; Average loss: 2.7304\nIteration: 2889; Percent complete: 72.2%; Average loss: 2.7028\nIteration: 2890; Percent complete: 72.2%; Average loss: 3.1353\nIteration: 2891; Percent complete: 72.3%; Average loss: 2.8483\nIteration: 2892; Percent complete: 72.3%; Average loss: 2.8841\nIteration: 2893; Percent complete: 72.3%; Average loss: 2.7714\nIteration: 2894; Percent complete: 72.4%; Average loss: 2.9640\nIteration: 2895; Percent complete: 72.4%; Average loss: 2.8677\nIteration: 2896; Percent complete: 72.4%; Average loss: 2.9259\nIteration: 2897; Percent complete: 72.4%; Average loss: 2.8288\nIteration: 2898; Percent complete: 72.5%; Average loss: 3.0168\nIteration: 2899; Percent complete: 72.5%; Average loss: 2.8252\nIteration: 2900; Percent complete: 72.5%; Average loss: 3.0521\nIteration: 2901; Percent complete: 72.5%; Average loss: 3.2590\nIteration: 2902; Percent complete: 72.5%; Average loss: 2.8427\nIteration: 2903; Percent complete: 72.6%; Average loss: 2.9874\nIteration: 2904; Percent complete: 72.6%; Average loss: 2.7376\nIteration: 2905; Percent complete: 72.6%; Average loss: 2.6835\nIteration: 2906; Percent complete: 72.7%; Average loss: 2.9490\nIteration: 2907; Percent complete: 72.7%; Average loss: 2.9280\nIteration: 2908; Percent complete: 72.7%; Average loss: 3.0248\nIteration: 2909; Percent complete: 72.7%; Average loss: 2.7467\nIteration: 2910; Percent complete: 72.8%; Average loss: 3.0207\nIteration: 2911; Percent complete: 72.8%; Average loss: 2.6284\nIteration: 2912; Percent complete: 72.8%; Average loss: 2.9513\nIteration: 2913; Percent complete: 72.8%; Average loss: 2.9954\nIteration: 2914; Percent complete: 72.9%; Average loss: 2.8839\nIteration: 2915; Percent complete: 72.9%; Average loss: 2.9073\nIteration: 2916; Percent complete: 72.9%; Average loss: 2.8566\nIteration: 2917; Percent complete: 72.9%; Average loss: 2.9328\nIteration: 2918; Percent complete: 73.0%; Average loss: 3.0008\nIteration: 2919; Percent complete: 73.0%; Average loss: 2.8212\nIteration: 2920; Percent complete: 73.0%; Average loss: 2.6040\nIteration: 2921; Percent complete: 73.0%; Average loss: 2.7399\nIteration: 2922; Percent complete: 73.0%; Average loss: 2.9181\nIteration: 2923; Percent complete: 73.1%; Average loss: 3.2707\nIteration: 2924; Percent complete: 73.1%; Average loss: 3.1993\nIteration: 2925; Percent complete: 73.1%; Average loss: 3.0256\nIteration: 2926; Percent complete: 73.2%; Average loss: 3.0509\nIteration: 2927; Percent complete: 73.2%; Average loss: 2.9373\nIteration: 2928; Percent complete: 73.2%; Average loss: 3.1481\nIteration: 2929; Percent complete: 73.2%; Average loss: 2.7432\nIteration: 2930; Percent complete: 73.2%; Average loss: 3.0492\nIteration: 2931; Percent complete: 73.3%; Average loss: 2.9239\nIteration: 2932; Percent complete: 73.3%; Average loss: 3.0058\nIteration: 2933; Percent complete: 73.3%; Average loss: 2.5176\nIteration: 2934; Percent complete: 73.4%; Average loss: 2.9645\nIteration: 2935; Percent complete: 73.4%; Average loss: 2.8866\nIteration: 2936; Percent complete: 73.4%; Average loss: 2.8802\nIteration: 2937; Percent complete: 73.4%; Average loss: 2.8339\nIteration: 2938; Percent complete: 73.5%; Average loss: 2.8039\nIteration: 2939; Percent complete: 73.5%; Average loss: 2.6907\nIteration: 2940; Percent complete: 73.5%; Average loss: 2.8316\nIteration: 2941; Percent complete: 73.5%; Average loss: 2.7052\nIteration: 2942; Percent complete: 73.6%; Average loss: 2.7243\nIteration: 2943; Percent complete: 73.6%; Average loss: 2.7991\nIteration: 2944; Percent complete: 73.6%; Average loss: 2.6597\nIteration: 2945; Percent complete: 73.6%; Average loss: 2.9609\nIteration: 2946; Percent complete: 73.7%; Average loss: 2.6110\nIteration: 2947; Percent complete: 73.7%; Average loss: 2.9446\nIteration: 2948; Percent complete: 73.7%; Average loss: 2.8618\nIteration: 2949; Percent complete: 73.7%; Average loss: 3.0129\nIteration: 2950; Percent complete: 73.8%; Average loss: 2.5455\nIteration: 2951; Percent complete: 73.8%; Average loss: 2.9591\nIteration: 2952; Percent complete: 73.8%; Average loss: 2.8516\nIteration: 2953; Percent complete: 73.8%; Average loss: 2.7101\nIteration: 2954; Percent complete: 73.9%; Average loss: 2.7846\nIteration: 2955; Percent complete: 73.9%; Average loss: 2.9013\nIteration: 2956; Percent complete: 73.9%; Average loss: 2.8268\nIteration: 2957; Percent complete: 73.9%; Average loss: 2.8742\nIteration: 2958; Percent complete: 74.0%; Average loss: 2.8774\nIteration: 2959; Percent complete: 74.0%; Average loss: 2.9027\nIteration: 2960; Percent complete: 74.0%; Average loss: 2.8410\nIteration: 2961; Percent complete: 74.0%; Average loss: 2.6141\nIteration: 2962; Percent complete: 74.1%; Average loss: 2.9844\nIteration: 2963; Percent complete: 74.1%; Average loss: 3.1719\nIteration: 2964; Percent complete: 74.1%; Average loss: 2.7819\nIteration: 2965; Percent complete: 74.1%; Average loss: 2.7666\nIteration: 2966; Percent complete: 74.2%; Average loss: 2.7740\nIteration: 2967; Percent complete: 74.2%; Average loss: 2.8605\nIteration: 2968; Percent complete: 74.2%; Average loss: 2.6457\nIteration: 2969; Percent complete: 74.2%; Average loss: 2.9178\nIteration: 2970; Percent complete: 74.2%; Average loss: 2.8718\nIteration: 2971; Percent complete: 74.3%; Average loss: 2.9415\nIteration: 2972; Percent complete: 74.3%; Average loss: 2.7657\nIteration: 2973; Percent complete: 74.3%; Average loss: 2.8472\nIteration: 2974; Percent complete: 74.4%; Average loss: 2.9125\nIteration: 2975; Percent complete: 74.4%; Average loss: 3.0677\nIteration: 2976; Percent complete: 74.4%; Average loss: 2.8180\nIteration: 2977; Percent complete: 74.4%; Average loss: 2.6965\nIteration: 2978; Percent complete: 74.5%; Average loss: 3.0768\nIteration: 2979; Percent complete: 74.5%; Average loss: 2.7904\nIteration: 2980; Percent complete: 74.5%; Average loss: 2.8919\nIteration: 2981; Percent complete: 74.5%; Average loss: 2.8750\nIteration: 2982; Percent complete: 74.6%; Average loss: 2.7713\nIteration: 2983; Percent complete: 74.6%; Average loss: 3.0087\nIteration: 2984; Percent complete: 74.6%; Average loss: 2.8752\nIteration: 2985; Percent complete: 74.6%; Average loss: 2.9721\nIteration: 2986; Percent complete: 74.7%; Average loss: 2.7359\nIteration: 2987; Percent complete: 74.7%; Average loss: 3.0291\nIteration: 2988; Percent complete: 74.7%; Average loss: 2.8949\nIteration: 2989; Percent complete: 74.7%; Average loss: 2.7829\nIteration: 2990; Percent complete: 74.8%; Average loss: 2.6283\nIteration: 2991; Percent complete: 74.8%; Average loss: 2.9185\nIteration: 2992; Percent complete: 74.8%; Average loss: 2.8110\nIteration: 2993; Percent complete: 74.8%; Average loss: 3.0128\nIteration: 2994; Percent complete: 74.9%; Average loss: 3.0318\nIteration: 2995; Percent complete: 74.9%; Average loss: 2.9427\nIteration: 2996; Percent complete: 74.9%; Average loss: 2.7806\nIteration: 2997; Percent complete: 74.9%; Average loss: 2.6621\nIteration: 2998; Percent complete: 75.0%; Average loss: 2.8774\nIteration: 2999; Percent complete: 75.0%; Average loss: 2.9665\nIteration: 3000; Percent complete: 75.0%; Average loss: 2.7491\nIteration: 3001; Percent complete: 75.0%; Average loss: 2.9418\nIteration: 3002; Percent complete: 75.0%; Average loss: 2.6673\nIteration: 3003; Percent complete: 75.1%; Average loss: 2.9274\nIteration: 3004; Percent complete: 75.1%; Average loss: 2.8275\nIteration: 3005; Percent complete: 75.1%; Average loss: 2.5012\nIteration: 3006; Percent complete: 75.1%; Average loss: 2.7599\nIteration: 3007; Percent complete: 75.2%; Average loss: 3.0838\nIteration: 3008; Percent complete: 75.2%; Average loss: 2.7825\nIteration: 3009; Percent complete: 75.2%; Average loss: 2.8678\nIteration: 3010; Percent complete: 75.2%; Average loss: 2.8059\nIteration: 3011; Percent complete: 75.3%; Average loss: 2.7766\nIteration: 3012; Percent complete: 75.3%; Average loss: 2.9969\nIteration: 3013; Percent complete: 75.3%; Average loss: 2.8771\nIteration: 3014; Percent complete: 75.3%; Average loss: 2.8623\nIteration: 3015; Percent complete: 75.4%; Average loss: 2.8680\nIteration: 3016; Percent complete: 75.4%; Average loss: 2.9020\nIteration: 3017; Percent complete: 75.4%; Average loss: 2.6145\nIteration: 3018; Percent complete: 75.4%; Average loss: 2.8580\nIteration: 3019; Percent complete: 75.5%; Average loss: 2.8303\nIteration: 3020; Percent complete: 75.5%; Average loss: 2.8751\nIteration: 3021; Percent complete: 75.5%; Average loss: 2.9760\nIteration: 3022; Percent complete: 75.5%; Average loss: 2.6849\nIteration: 3023; Percent complete: 75.6%; Average loss: 2.8359\nIteration: 3024; Percent complete: 75.6%; Average loss: 2.8636\nIteration: 3025; Percent complete: 75.6%; Average loss: 2.8103\nIteration: 3026; Percent complete: 75.6%; Average loss: 2.7999\nIteration: 3027; Percent complete: 75.7%; Average loss: 2.7252\nIteration: 3028; Percent complete: 75.7%; Average loss: 2.8864\nIteration: 3029; Percent complete: 75.7%; Average loss: 2.8602\nIteration: 3030; Percent complete: 75.8%; Average loss: 2.7373\nIteration: 3031; Percent complete: 75.8%; Average loss: 2.8304\nIteration: 3032; Percent complete: 75.8%; Average loss: 2.9073\nIteration: 3033; Percent complete: 75.8%; Average loss: 2.7727\nIteration: 3034; Percent complete: 75.8%; Average loss: 2.6717\nIteration: 3035; Percent complete: 75.9%; Average loss: 2.8265\nIteration: 3036; Percent complete: 75.9%; Average loss: 2.9542\nIteration: 3037; Percent complete: 75.9%; Average loss: 2.9508\nIteration: 3038; Percent complete: 75.9%; Average loss: 3.1535\nIteration: 3039; Percent complete: 76.0%; Average loss: 2.6597\nIteration: 3040; Percent complete: 76.0%; Average loss: 3.1220\nIteration: 3041; Percent complete: 76.0%; Average loss: 3.0342\nIteration: 3042; Percent complete: 76.0%; Average loss: 2.7757\nIteration: 3043; Percent complete: 76.1%; Average loss: 2.8703\nIteration: 3044; Percent complete: 76.1%; Average loss: 2.8774\nIteration: 3045; Percent complete: 76.1%; Average loss: 2.8610\nIteration: 3046; Percent complete: 76.1%; Average loss: 2.7961\nIteration: 3047; Percent complete: 76.2%; Average loss: 3.0974\nIteration: 3048; Percent complete: 76.2%; Average loss: 3.0215\nIteration: 3049; Percent complete: 76.2%; Average loss: 2.9060\nIteration: 3050; Percent complete: 76.2%; Average loss: 2.9770\nIteration: 3051; Percent complete: 76.3%; Average loss: 2.5993\nIteration: 3052; Percent complete: 76.3%; Average loss: 2.9841\nIteration: 3053; Percent complete: 76.3%; Average loss: 2.8696\nIteration: 3054; Percent complete: 76.3%; Average loss: 2.7701\nIteration: 3055; Percent complete: 76.4%; Average loss: 2.7542\nIteration: 3056; Percent complete: 76.4%; Average loss: 2.6398\nIteration: 3057; Percent complete: 76.4%; Average loss: 2.8936\nIteration: 3058; Percent complete: 76.4%; Average loss: 2.7849\nIteration: 3059; Percent complete: 76.5%; Average loss: 2.7183\nIteration: 3060; Percent complete: 76.5%; Average loss: 2.9500\nIteration: 3061; Percent complete: 76.5%; Average loss: 2.8087\nIteration: 3062; Percent complete: 76.5%; Average loss: 2.7541\nIteration: 3063; Percent complete: 76.6%; Average loss: 2.7264\nIteration: 3064; Percent complete: 76.6%; Average loss: 2.6694\nIteration: 3065; Percent complete: 76.6%; Average loss: 2.8824\nIteration: 3066; Percent complete: 76.6%; Average loss: 2.7067\nIteration: 3067; Percent complete: 76.7%; Average loss: 3.0661\nIteration: 3068; Percent complete: 76.7%; Average loss: 2.6737\nIteration: 3069; Percent complete: 76.7%; Average loss: 3.2269\nIteration: 3070; Percent complete: 76.8%; Average loss: 3.0127\nIteration: 3071; Percent complete: 76.8%; Average loss: 2.6178\nIteration: 3072; Percent complete: 76.8%; Average loss: 2.7305\nIteration: 3073; Percent complete: 76.8%; Average loss: 2.6913\nIteration: 3074; Percent complete: 76.8%; Average loss: 2.7019\nIteration: 3075; Percent complete: 76.9%; Average loss: 2.9047\nIteration: 3076; Percent complete: 76.9%; Average loss: 2.7750\nIteration: 3077; Percent complete: 76.9%; Average loss: 2.9480\nIteration: 3078; Percent complete: 77.0%; Average loss: 3.1177\nIteration: 3079; Percent complete: 77.0%; Average loss: 2.9546\nIteration: 3080; Percent complete: 77.0%; Average loss: 2.9692\nIteration: 3081; Percent complete: 77.0%; Average loss: 2.8822\nIteration: 3082; Percent complete: 77.0%; Average loss: 2.7970\nIteration: 3083; Percent complete: 77.1%; Average loss: 3.2068\nIteration: 3084; Percent complete: 77.1%; Average loss: 3.0095\nIteration: 3085; Percent complete: 77.1%; Average loss: 2.9469\nIteration: 3086; Percent complete: 77.1%; Average loss: 2.7258\nIteration: 3087; Percent complete: 77.2%; Average loss: 2.7998\nIteration: 3088; Percent complete: 77.2%; Average loss: 2.8994\nIteration: 3089; Percent complete: 77.2%; Average loss: 2.7620\nIteration: 3090; Percent complete: 77.2%; Average loss: 2.8673\nIteration: 3091; Percent complete: 77.3%; Average loss: 2.9499\nIteration: 3092; Percent complete: 77.3%; Average loss: 2.7940\nIteration: 3093; Percent complete: 77.3%; Average loss: 3.0348\nIteration: 3094; Percent complete: 77.3%; Average loss: 3.1409\nIteration: 3095; Percent complete: 77.4%; Average loss: 2.6942\nIteration: 3096; Percent complete: 77.4%; Average loss: 2.7191\nIteration: 3097; Percent complete: 77.4%; Average loss: 2.7284\nIteration: 3098; Percent complete: 77.5%; Average loss: 2.9125\nIteration: 3099; Percent complete: 77.5%; Average loss: 2.9832\nIteration: 3100; Percent complete: 77.5%; Average loss: 2.9186\nIteration: 3101; Percent complete: 77.5%; Average loss: 2.5352\nIteration: 3102; Percent complete: 77.5%; Average loss: 2.6868\nIteration: 3103; Percent complete: 77.6%; Average loss: 2.8396\nIteration: 3104; Percent complete: 77.6%; Average loss: 2.7115\nIteration: 3105; Percent complete: 77.6%; Average loss: 2.6684\nIteration: 3106; Percent complete: 77.6%; Average loss: 2.6640\nIteration: 3107; Percent complete: 77.7%; Average loss: 2.8353\nIteration: 3108; Percent complete: 77.7%; Average loss: 2.8010\nIteration: 3109; Percent complete: 77.7%; Average loss: 2.9245\nIteration: 3110; Percent complete: 77.8%; Average loss: 2.9125\nIteration: 3111; Percent complete: 77.8%; Average loss: 2.9366\nIteration: 3112; Percent complete: 77.8%; Average loss: 2.7121\nIteration: 3113; Percent complete: 77.8%; Average loss: 2.6130\nIteration: 3114; Percent complete: 77.8%; Average loss: 2.8276\nIteration: 3115; Percent complete: 77.9%; Average loss: 2.6991\nIteration: 3116; Percent complete: 77.9%; Average loss: 2.9880\nIteration: 3117; Percent complete: 77.9%; Average loss: 2.7076\nIteration: 3118; Percent complete: 78.0%; Average loss: 3.0226\nIteration: 3119; Percent complete: 78.0%; Average loss: 2.7603\nIteration: 3120; Percent complete: 78.0%; Average loss: 2.7394\nIteration: 3121; Percent complete: 78.0%; Average loss: 2.9222\nIteration: 3122; Percent complete: 78.0%; Average loss: 2.7477\nIteration: 3123; Percent complete: 78.1%; Average loss: 2.9457\nIteration: 3124; Percent complete: 78.1%; Average loss: 2.9447\nIteration: 3125; Percent complete: 78.1%; Average loss: 2.6853\nIteration: 3126; Percent complete: 78.1%; Average loss: 2.8192\nIteration: 3127; Percent complete: 78.2%; Average loss: 2.8095\nIteration: 3128; Percent complete: 78.2%; Average loss: 3.0675\nIteration: 3129; Percent complete: 78.2%; Average loss: 2.7516\nIteration: 3130; Percent complete: 78.2%; Average loss: 2.7933\nIteration: 3131; Percent complete: 78.3%; Average loss: 3.0875\nIteration: 3132; Percent complete: 78.3%; Average loss: 2.8887\nIteration: 3133; Percent complete: 78.3%; Average loss: 2.8805\nIteration: 3134; Percent complete: 78.3%; Average loss: 2.9482\nIteration: 3135; Percent complete: 78.4%; Average loss: 2.8914\nIteration: 3136; Percent complete: 78.4%; Average loss: 2.7436\nIteration: 3137; Percent complete: 78.4%; Average loss: 2.6627\nIteration: 3138; Percent complete: 78.5%; Average loss: 2.9821\nIteration: 3139; Percent complete: 78.5%; Average loss: 2.7407\nIteration: 3140; Percent complete: 78.5%; Average loss: 2.9136\nIteration: 3141; Percent complete: 78.5%; Average loss: 2.9117\nIteration: 3142; Percent complete: 78.5%; Average loss: 2.9574\nIteration: 3143; Percent complete: 78.6%; Average loss: 2.8471\nIteration: 3144; Percent complete: 78.6%; Average loss: 2.8367\nIteration: 3145; Percent complete: 78.6%; Average loss: 2.7106\nIteration: 3146; Percent complete: 78.6%; Average loss: 2.7326\nIteration: 3147; Percent complete: 78.7%; Average loss: 2.5889\nIteration: 3148; Percent complete: 78.7%; Average loss: 2.8631\nIteration: 3149; Percent complete: 78.7%; Average loss: 3.0054\nIteration: 3150; Percent complete: 78.8%; Average loss: 2.8505\nIteration: 3151; Percent complete: 78.8%; Average loss: 2.9475\nIteration: 3152; Percent complete: 78.8%; Average loss: 2.5371\nIteration: 3153; Percent complete: 78.8%; Average loss: 2.7682\nIteration: 3154; Percent complete: 78.8%; Average loss: 2.6160\nIteration: 3155; Percent complete: 78.9%; Average loss: 2.9875\nIteration: 3156; Percent complete: 78.9%; Average loss: 2.9195\nIteration: 3157; Percent complete: 78.9%; Average loss: 2.7000\nIteration: 3158; Percent complete: 79.0%; Average loss: 2.6674\nIteration: 3159; Percent complete: 79.0%; Average loss: 2.7519\nIteration: 3160; Percent complete: 79.0%; Average loss: 2.9763\nIteration: 3161; Percent complete: 79.0%; Average loss: 2.8373\nIteration: 3162; Percent complete: 79.0%; Average loss: 2.7271\nIteration: 3163; Percent complete: 79.1%; Average loss: 2.8593\nIteration: 3164; Percent complete: 79.1%; Average loss: 2.7079\nIteration: 3165; Percent complete: 79.1%; Average loss: 2.9859\nIteration: 3166; Percent complete: 79.1%; Average loss: 2.8262\nIteration: 3167; Percent complete: 79.2%; Average loss: 2.7332\nIteration: 3168; Percent complete: 79.2%; Average loss: 2.7686\nIteration: 3169; Percent complete: 79.2%; Average loss: 2.7245\nIteration: 3170; Percent complete: 79.2%; Average loss: 2.8894\nIteration: 3171; Percent complete: 79.3%; Average loss: 2.8816\nIteration: 3172; Percent complete: 79.3%; Average loss: 2.9796\nIteration: 3173; Percent complete: 79.3%; Average loss: 3.0283\nIteration: 3174; Percent complete: 79.3%; Average loss: 2.8048\nIteration: 3175; Percent complete: 79.4%; Average loss: 2.9227\nIteration: 3176; Percent complete: 79.4%; Average loss: 2.8022\nIteration: 3177; Percent complete: 79.4%; Average loss: 3.0894\nIteration: 3178; Percent complete: 79.5%; Average loss: 2.7977\nIteration: 3179; Percent complete: 79.5%; Average loss: 2.6000\nIteration: 3180; Percent complete: 79.5%; Average loss: 2.5607\nIteration: 3181; Percent complete: 79.5%; Average loss: 3.0518\nIteration: 3182; Percent complete: 79.5%; Average loss: 2.8705\nIteration: 3183; Percent complete: 79.6%; Average loss: 2.8401\nIteration: 3184; Percent complete: 79.6%; Average loss: 2.6489\nIteration: 3185; Percent complete: 79.6%; Average loss: 2.9489\nIteration: 3186; Percent complete: 79.7%; Average loss: 2.6658\nIteration: 3187; Percent complete: 79.7%; Average loss: 2.7745\nIteration: 3188; Percent complete: 79.7%; Average loss: 2.9562\nIteration: 3189; Percent complete: 79.7%; Average loss: 2.9251\nIteration: 3190; Percent complete: 79.8%; Average loss: 2.8314\nIteration: 3191; Percent complete: 79.8%; Average loss: 2.7971\nIteration: 3192; Percent complete: 79.8%; Average loss: 2.9759\nIteration: 3193; Percent complete: 79.8%; Average loss: 2.9386\nIteration: 3194; Percent complete: 79.8%; Average loss: 2.9664\nIteration: 3195; Percent complete: 79.9%; Average loss: 2.9878\nIteration: 3196; Percent complete: 79.9%; Average loss: 2.4147\nIteration: 3197; Percent complete: 79.9%; Average loss: 2.6368\nIteration: 3198; Percent complete: 80.0%; Average loss: 2.6812\nIteration: 3199; Percent complete: 80.0%; Average loss: 2.7889\nIteration: 3200; Percent complete: 80.0%; Average loss: 2.6537\nIteration: 3201; Percent complete: 80.0%; Average loss: 2.7020\nIteration: 3202; Percent complete: 80.0%; Average loss: 2.7102\nIteration: 3203; Percent complete: 80.1%; Average loss: 2.8633\nIteration: 3204; Percent complete: 80.1%; Average loss: 2.8033\nIteration: 3205; Percent complete: 80.1%; Average loss: 2.7883\nIteration: 3206; Percent complete: 80.2%; Average loss: 3.0074\nIteration: 3207; Percent complete: 80.2%; Average loss: 2.9194\nIteration: 3208; Percent complete: 80.2%; Average loss: 2.6205\nIteration: 3209; Percent complete: 80.2%; Average loss: 2.9109\nIteration: 3210; Percent complete: 80.2%; Average loss: 2.9035\nIteration: 3211; Percent complete: 80.3%; Average loss: 2.7490\nIteration: 3212; Percent complete: 80.3%; Average loss: 2.8170\nIteration: 3213; Percent complete: 80.3%; Average loss: 2.8880\nIteration: 3214; Percent complete: 80.3%; Average loss: 2.9440\nIteration: 3215; Percent complete: 80.4%; Average loss: 2.7362\nIteration: 3216; Percent complete: 80.4%; Average loss: 3.0625\nIteration: 3217; Percent complete: 80.4%; Average loss: 2.8285\nIteration: 3218; Percent complete: 80.5%; Average loss: 2.7277\nIteration: 3219; Percent complete: 80.5%; Average loss: 2.8303\nIteration: 3220; Percent complete: 80.5%; Average loss: 2.7397\nIteration: 3221; Percent complete: 80.5%; Average loss: 2.8727\nIteration: 3222; Percent complete: 80.5%; Average loss: 2.5514\nIteration: 3223; Percent complete: 80.6%; Average loss: 2.8547\nIteration: 3224; Percent complete: 80.6%; Average loss: 2.7239\nIteration: 3225; Percent complete: 80.6%; Average loss: 2.9192\nIteration: 3226; Percent complete: 80.7%; Average loss: 2.7026\nIteration: 3227; Percent complete: 80.7%; Average loss: 2.8384\nIteration: 3228; Percent complete: 80.7%; Average loss: 2.8362\nIteration: 3229; Percent complete: 80.7%; Average loss: 2.8572\nIteration: 3230; Percent complete: 80.8%; Average loss: 2.8348\nIteration: 3231; Percent complete: 80.8%; Average loss: 2.8730\nIteration: 3232; Percent complete: 80.8%; Average loss: 2.8575\nIteration: 3233; Percent complete: 80.8%; Average loss: 2.8151\nIteration: 3234; Percent complete: 80.8%; Average loss: 2.8402\nIteration: 3235; Percent complete: 80.9%; Average loss: 2.9419\nIteration: 3236; Percent complete: 80.9%; Average loss: 2.8464\nIteration: 3237; Percent complete: 80.9%; Average loss: 2.8259\nIteration: 3238; Percent complete: 81.0%; Average loss: 2.7663\nIteration: 3239; Percent complete: 81.0%; Average loss: 2.5814\nIteration: 3240; Percent complete: 81.0%; Average loss: 2.6974\nIteration: 3241; Percent complete: 81.0%; Average loss: 2.9196\nIteration: 3242; Percent complete: 81.0%; Average loss: 2.9279\nIteration: 3243; Percent complete: 81.1%; Average loss: 2.6379\nIteration: 3244; Percent complete: 81.1%; Average loss: 2.7800\nIteration: 3245; Percent complete: 81.1%; Average loss: 2.7398\nIteration: 3246; Percent complete: 81.2%; Average loss: 2.9242\nIteration: 3247; Percent complete: 81.2%; Average loss: 2.6668\nIteration: 3248; Percent complete: 81.2%; Average loss: 2.8384\nIteration: 3249; Percent complete: 81.2%; Average loss: 2.7323\nIteration: 3250; Percent complete: 81.2%; Average loss: 2.8586\nIteration: 3251; Percent complete: 81.3%; Average loss: 2.8211\nIteration: 3252; Percent complete: 81.3%; Average loss: 2.6851\nIteration: 3253; Percent complete: 81.3%; Average loss: 2.7664\nIteration: 3254; Percent complete: 81.3%; Average loss: 2.6800\nIteration: 3255; Percent complete: 81.4%; Average loss: 2.8271\nIteration: 3256; Percent complete: 81.4%; Average loss: 2.9637\nIteration: 3257; Percent complete: 81.4%; Average loss: 2.8886\nIteration: 3258; Percent complete: 81.5%; Average loss: 3.0379\nIteration: 3259; Percent complete: 81.5%; Average loss: 2.7619\nIteration: 3260; Percent complete: 81.5%; Average loss: 3.1797\nIteration: 3261; Percent complete: 81.5%; Average loss: 2.7985\nIteration: 3262; Percent complete: 81.5%; Average loss: 2.8862\nIteration: 3263; Percent complete: 81.6%; Average loss: 2.5969\nIteration: 3264; Percent complete: 81.6%; Average loss: 2.8230\nIteration: 3265; Percent complete: 81.6%; Average loss: 2.5330\nIteration: 3266; Percent complete: 81.7%; Average loss: 3.0918\nIteration: 3267; Percent complete: 81.7%; Average loss: 2.6617\nIteration: 3268; Percent complete: 81.7%; Average loss: 2.8632\nIteration: 3269; Percent complete: 81.7%; Average loss: 2.6655\nIteration: 3270; Percent complete: 81.8%; Average loss: 2.9277\nIteration: 3271; Percent complete: 81.8%; Average loss: 2.5870\nIteration: 3272; Percent complete: 81.8%; Average loss: 2.8736\nIteration: 3273; Percent complete: 81.8%; Average loss: 2.6652\nIteration: 3274; Percent complete: 81.8%; Average loss: 2.8585\nIteration: 3275; Percent complete: 81.9%; Average loss: 2.8629\nIteration: 3276; Percent complete: 81.9%; Average loss: 3.1277\nIteration: 3277; Percent complete: 81.9%; Average loss: 2.7385\nIteration: 3278; Percent complete: 82.0%; Average loss: 2.7417\nIteration: 3279; Percent complete: 82.0%; Average loss: 3.0596\nIteration: 3280; Percent complete: 82.0%; Average loss: 2.6278\nIteration: 3281; Percent complete: 82.0%; Average loss: 2.8900\nIteration: 3282; Percent complete: 82.0%; Average loss: 3.1863\nIteration: 3283; Percent complete: 82.1%; Average loss: 2.7455\nIteration: 3284; Percent complete: 82.1%; Average loss: 2.7880\nIteration: 3285; Percent complete: 82.1%; Average loss: 2.7749\nIteration: 3286; Percent complete: 82.2%; Average loss: 2.8057\nIteration: 3287; Percent complete: 82.2%; Average loss: 2.9132\nIteration: 3288; Percent complete: 82.2%; Average loss: 2.8515\nIteration: 3289; Percent complete: 82.2%; Average loss: 2.7795\nIteration: 3290; Percent complete: 82.2%; Average loss: 2.7646\nIteration: 3291; Percent complete: 82.3%; Average loss: 2.7265\nIteration: 3292; Percent complete: 82.3%; Average loss: 2.9237\nIteration: 3293; Percent complete: 82.3%; Average loss: 2.8840\nIteration: 3294; Percent complete: 82.3%; Average loss: 2.6851\nIteration: 3295; Percent complete: 82.4%; Average loss: 2.7531\nIteration: 3296; Percent complete: 82.4%; Average loss: 2.7625\nIteration: 3297; Percent complete: 82.4%; Average loss: 2.7549\nIteration: 3298; Percent complete: 82.5%; Average loss: 2.6250\nIteration: 3299; Percent complete: 82.5%; Average loss: 2.8961\nIteration: 3300; Percent complete: 82.5%; Average loss: 2.7800\nIteration: 3301; Percent complete: 82.5%; Average loss: 2.6326\nIteration: 3302; Percent complete: 82.5%; Average loss: 2.7498\nIteration: 3303; Percent complete: 82.6%; Average loss: 2.9957\nIteration: 3304; Percent complete: 82.6%; Average loss: 2.7427\nIteration: 3305; Percent complete: 82.6%; Average loss: 2.7970\nIteration: 3306; Percent complete: 82.7%; Average loss: 2.7600\nIteration: 3307; Percent complete: 82.7%; Average loss: 2.7297\nIteration: 3308; Percent complete: 82.7%; Average loss: 2.6125\nIteration: 3309; Percent complete: 82.7%; Average loss: 2.7000\nIteration: 3310; Percent complete: 82.8%; Average loss: 2.4323\nIteration: 3311; Percent complete: 82.8%; Average loss: 2.7529\nIteration: 3312; Percent complete: 82.8%; Average loss: 2.7143\nIteration: 3313; Percent complete: 82.8%; Average loss: 2.9784\nIteration: 3314; Percent complete: 82.8%; Average loss: 2.9241\nIteration: 3315; Percent complete: 82.9%; Average loss: 2.6539\nIteration: 3316; Percent complete: 82.9%; Average loss: 2.6783\nIteration: 3317; Percent complete: 82.9%; Average loss: 2.6933\nIteration: 3318; Percent complete: 83.0%; Average loss: 3.0726\nIteration: 3319; Percent complete: 83.0%; Average loss: 2.4894\nIteration: 3320; Percent complete: 83.0%; Average loss: 2.8003\nIteration: 3321; Percent complete: 83.0%; Average loss: 2.8343\nIteration: 3322; Percent complete: 83.0%; Average loss: 2.7305\nIteration: 3323; Percent complete: 83.1%; Average loss: 2.8210\nIteration: 3324; Percent complete: 83.1%; Average loss: 2.8781\nIteration: 3325; Percent complete: 83.1%; Average loss: 2.6693\nIteration: 3326; Percent complete: 83.2%; Average loss: 2.7445\nIteration: 3327; Percent complete: 83.2%; Average loss: 3.0778\nIteration: 3328; Percent complete: 83.2%; Average loss: 2.9852\nIteration: 3329; Percent complete: 83.2%; Average loss: 2.8845\nIteration: 3330; Percent complete: 83.2%; Average loss: 2.6503\nIteration: 3331; Percent complete: 83.3%; Average loss: 2.7464\nIteration: 3332; Percent complete: 83.3%; Average loss: 3.1266\nIteration: 3333; Percent complete: 83.3%; Average loss: 2.7947\nIteration: 3334; Percent complete: 83.4%; Average loss: 2.8417\nIteration: 3335; Percent complete: 83.4%; Average loss: 2.9340\nIteration: 3336; Percent complete: 83.4%; Average loss: 2.6872\nIteration: 3337; Percent complete: 83.4%; Average loss: 2.8036\nIteration: 3338; Percent complete: 83.5%; Average loss: 2.7018\nIteration: 3339; Percent complete: 83.5%; Average loss: 2.7068\nIteration: 3340; Percent complete: 83.5%; Average loss: 2.7263\nIteration: 3341; Percent complete: 83.5%; Average loss: 2.8385\nIteration: 3342; Percent complete: 83.5%; Average loss: 2.9035\nIteration: 3343; Percent complete: 83.6%; Average loss: 2.7506\nIteration: 3344; Percent complete: 83.6%; Average loss: 2.6803\nIteration: 3345; Percent complete: 83.6%; Average loss: 2.9002\nIteration: 3346; Percent complete: 83.7%; Average loss: 2.6133\nIteration: 3347; Percent complete: 83.7%; Average loss: 2.7239\nIteration: 3348; Percent complete: 83.7%; Average loss: 2.9864\nIteration: 3349; Percent complete: 83.7%; Average loss: 2.6743\nIteration: 3350; Percent complete: 83.8%; Average loss: 2.8507\nIteration: 3351; Percent complete: 83.8%; Average loss: 2.8352\nIteration: 3352; Percent complete: 83.8%; Average loss: 2.6602\nIteration: 3353; Percent complete: 83.8%; Average loss: 2.6870\nIteration: 3354; Percent complete: 83.9%; Average loss: 2.7548\nIteration: 3355; Percent complete: 83.9%; Average loss: 2.9584\nIteration: 3356; Percent complete: 83.9%; Average loss: 2.8756\nIteration: 3357; Percent complete: 83.9%; Average loss: 2.7144\nIteration: 3358; Percent complete: 84.0%; Average loss: 3.0527\nIteration: 3359; Percent complete: 84.0%; Average loss: 2.8183\nIteration: 3360; Percent complete: 84.0%; Average loss: 2.8642\nIteration: 3361; Percent complete: 84.0%; Average loss: 2.7919\nIteration: 3362; Percent complete: 84.0%; Average loss: 2.7148\nIteration: 3363; Percent complete: 84.1%; Average loss: 2.8908\nIteration: 3364; Percent complete: 84.1%; Average loss: 2.7621\nIteration: 3365; Percent complete: 84.1%; Average loss: 2.8098\nIteration: 3366; Percent complete: 84.2%; Average loss: 2.7727\nIteration: 3367; Percent complete: 84.2%; Average loss: 2.7694\nIteration: 3368; Percent complete: 84.2%; Average loss: 2.7259\nIteration: 3369; Percent complete: 84.2%; Average loss: 2.6827\nIteration: 3370; Percent complete: 84.2%; Average loss: 2.9253\nIteration: 3371; Percent complete: 84.3%; Average loss: 2.8051\nIteration: 3372; Percent complete: 84.3%; Average loss: 2.9181\nIteration: 3373; Percent complete: 84.3%; Average loss: 2.7305\nIteration: 3374; Percent complete: 84.4%; Average loss: 2.8848\nIteration: 3375; Percent complete: 84.4%; Average loss: 2.8713\nIteration: 3376; Percent complete: 84.4%; Average loss: 2.8501\nIteration: 3377; Percent complete: 84.4%; Average loss: 2.7153\nIteration: 3378; Percent complete: 84.5%; Average loss: 2.6896\nIteration: 3379; Percent complete: 84.5%; Average loss: 2.7023\nIteration: 3380; Percent complete: 84.5%; Average loss: 2.7480\nIteration: 3381; Percent complete: 84.5%; Average loss: 2.6409\nIteration: 3382; Percent complete: 84.5%; Average loss: 2.8681\nIteration: 3383; Percent complete: 84.6%; Average loss: 2.7525\nIteration: 3384; Percent complete: 84.6%; Average loss: 2.9841\nIteration: 3385; Percent complete: 84.6%; Average loss: 2.7451\nIteration: 3386; Percent complete: 84.7%; Average loss: 2.9402\nIteration: 3387; Percent complete: 84.7%; Average loss: 2.6737\nIteration: 3388; Percent complete: 84.7%; Average loss: 2.7671\nIteration: 3389; Percent complete: 84.7%; Average loss: 2.8104\nIteration: 3390; Percent complete: 84.8%; Average loss: 2.8261\nIteration: 3391; Percent complete: 84.8%; Average loss: 2.6339\nIteration: 3392; Percent complete: 84.8%; Average loss: 2.8811\nIteration: 3393; Percent complete: 84.8%; Average loss: 3.0213\nIteration: 3394; Percent complete: 84.9%; Average loss: 2.7761\nIteration: 3395; Percent complete: 84.9%; Average loss: 2.8774\nIteration: 3396; Percent complete: 84.9%; Average loss: 2.7137\nIteration: 3397; Percent complete: 84.9%; Average loss: 3.0231\nIteration: 3398; Percent complete: 85.0%; Average loss: 2.6013\nIteration: 3399; Percent complete: 85.0%; Average loss: 2.8356\nIteration: 3400; Percent complete: 85.0%; Average loss: 2.6886\nIteration: 3401; Percent complete: 85.0%; Average loss: 2.7147\nIteration: 3402; Percent complete: 85.0%; Average loss: 2.6621\nIteration: 3403; Percent complete: 85.1%; Average loss: 2.5970\nIteration: 3404; Percent complete: 85.1%; Average loss: 2.7219\nIteration: 3405; Percent complete: 85.1%; Average loss: 2.7602\nIteration: 3406; Percent complete: 85.2%; Average loss: 2.7465\nIteration: 3407; Percent complete: 85.2%; Average loss: 2.7905\nIteration: 3408; Percent complete: 85.2%; Average loss: 2.7185\nIteration: 3409; Percent complete: 85.2%; Average loss: 2.8905\nIteration: 3410; Percent complete: 85.2%; Average loss: 2.7174\nIteration: 3411; Percent complete: 85.3%; Average loss: 2.9169\nIteration: 3412; Percent complete: 85.3%; Average loss: 3.0093\nIteration: 3413; Percent complete: 85.3%; Average loss: 2.7731\nIteration: 3414; Percent complete: 85.4%; Average loss: 2.7196\nIteration: 3415; Percent complete: 85.4%; Average loss: 2.6585\nIteration: 3416; Percent complete: 85.4%; Average loss: 2.7218\nIteration: 3417; Percent complete: 85.4%; Average loss: 2.5129\nIteration: 3418; Percent complete: 85.5%; Average loss: 2.8137\nIteration: 3419; Percent complete: 85.5%; Average loss: 2.8131\nIteration: 3420; Percent complete: 85.5%; Average loss: 2.6761\nIteration: 3421; Percent complete: 85.5%; Average loss: 2.8693\nIteration: 3422; Percent complete: 85.5%; Average loss: 2.8103\nIteration: 3423; Percent complete: 85.6%; Average loss: 2.9200\nIteration: 3424; Percent complete: 85.6%; Average loss: 2.8051\nIteration: 3425; Percent complete: 85.6%; Average loss: 2.6720\nIteration: 3426; Percent complete: 85.7%; Average loss: 2.4841\nIteration: 3427; Percent complete: 85.7%; Average loss: 2.7157\nIteration: 3428; Percent complete: 85.7%; Average loss: 2.7569\nIteration: 3429; Percent complete: 85.7%; Average loss: 2.7455\nIteration: 3430; Percent complete: 85.8%; Average loss: 2.8448\nIteration: 3431; Percent complete: 85.8%; Average loss: 2.6965\nIteration: 3432; Percent complete: 85.8%; Average loss: 2.9030\nIteration: 3433; Percent complete: 85.8%; Average loss: 2.6824\nIteration: 3434; Percent complete: 85.9%; Average loss: 2.3599\nIteration: 3435; Percent complete: 85.9%; Average loss: 2.5710\nIteration: 3436; Percent complete: 85.9%; Average loss: 2.9232\nIteration: 3437; Percent complete: 85.9%; Average loss: 2.6894\nIteration: 3438; Percent complete: 86.0%; Average loss: 2.8490\nIteration: 3439; Percent complete: 86.0%; Average loss: 2.7648\nIteration: 3440; Percent complete: 86.0%; Average loss: 2.6776\nIteration: 3441; Percent complete: 86.0%; Average loss: 2.5621\nIteration: 3442; Percent complete: 86.1%; Average loss: 2.7664\nIteration: 3443; Percent complete: 86.1%; Average loss: 2.6716\nIteration: 3444; Percent complete: 86.1%; Average loss: 2.7856\nIteration: 3445; Percent complete: 86.1%; Average loss: 2.8997\nIteration: 3446; Percent complete: 86.2%; Average loss: 2.7383\nIteration: 3447; Percent complete: 86.2%; Average loss: 2.6218\nIteration: 3448; Percent complete: 86.2%; Average loss: 2.9077\nIteration: 3449; Percent complete: 86.2%; Average loss: 2.8567\nIteration: 3450; Percent complete: 86.2%; Average loss: 2.8507\nIteration: 3451; Percent complete: 86.3%; Average loss: 2.7809\nIteration: 3452; Percent complete: 86.3%; Average loss: 2.5725\nIteration: 3453; Percent complete: 86.3%; Average loss: 2.5623\nIteration: 3454; Percent complete: 86.4%; Average loss: 2.8206\nIteration: 3455; Percent complete: 86.4%; Average loss: 2.6140\nIteration: 3456; Percent complete: 86.4%; Average loss: 2.7530\nIteration: 3457; Percent complete: 86.4%; Average loss: 2.9363\nIteration: 3458; Percent complete: 86.5%; Average loss: 2.8526\nIteration: 3459; Percent complete: 86.5%; Average loss: 2.6976\nIteration: 3460; Percent complete: 86.5%; Average loss: 2.6927\nIteration: 3461; Percent complete: 86.5%; Average loss: 2.9574\nIteration: 3462; Percent complete: 86.6%; Average loss: 2.7961\nIteration: 3463; Percent complete: 86.6%; Average loss: 2.6233\nIteration: 3464; Percent complete: 86.6%; Average loss: 2.8143\nIteration: 3465; Percent complete: 86.6%; Average loss: 2.7615\nIteration: 3466; Percent complete: 86.7%; Average loss: 2.7172\nIteration: 3467; Percent complete: 86.7%; Average loss: 2.9917\nIteration: 3468; Percent complete: 86.7%; Average loss: 2.7541\nIteration: 3469; Percent complete: 86.7%; Average loss: 2.5535\nIteration: 3470; Percent complete: 86.8%; Average loss: 2.7543\nIteration: 3471; Percent complete: 86.8%; Average loss: 2.6908\nIteration: 3472; Percent complete: 86.8%; Average loss: 2.8370\nIteration: 3473; Percent complete: 86.8%; Average loss: 2.8920\nIteration: 3474; Percent complete: 86.9%; Average loss: 2.8120\nIteration: 3475; Percent complete: 86.9%; Average loss: 2.7563\nIteration: 3476; Percent complete: 86.9%; Average loss: 2.6482\nIteration: 3477; Percent complete: 86.9%; Average loss: 2.7110\nIteration: 3478; Percent complete: 87.0%; Average loss: 2.7343\nIteration: 3479; Percent complete: 87.0%; Average loss: 2.7911\nIteration: 3480; Percent complete: 87.0%; Average loss: 2.6266\nIteration: 3481; Percent complete: 87.0%; Average loss: 2.9047\nIteration: 3482; Percent complete: 87.1%; Average loss: 2.7309\nIteration: 3483; Percent complete: 87.1%; Average loss: 2.9512\nIteration: 3484; Percent complete: 87.1%; Average loss: 2.6050\nIteration: 3485; Percent complete: 87.1%; Average loss: 2.7572\nIteration: 3486; Percent complete: 87.2%; Average loss: 2.6869\nIteration: 3487; Percent complete: 87.2%; Average loss: 2.7924\nIteration: 3488; Percent complete: 87.2%; Average loss: 2.8764\nIteration: 3489; Percent complete: 87.2%; Average loss: 2.8227\nIteration: 3490; Percent complete: 87.2%; Average loss: 2.8000\nIteration: 3491; Percent complete: 87.3%; Average loss: 2.5533\nIteration: 3492; Percent complete: 87.3%; Average loss: 2.6875\nIteration: 3493; Percent complete: 87.3%; Average loss: 2.9403\nIteration: 3494; Percent complete: 87.4%; Average loss: 2.8550\nIteration: 3495; Percent complete: 87.4%; Average loss: 2.6922\nIteration: 3496; Percent complete: 87.4%; Average loss: 2.7231\nIteration: 3497; Percent complete: 87.4%; Average loss: 2.7317\nIteration: 3498; Percent complete: 87.5%; Average loss: 2.6722\nIteration: 3499; Percent complete: 87.5%; Average loss: 2.7480\nIteration: 3500; Percent complete: 87.5%; Average loss: 2.6623\nIteration: 3501; Percent complete: 87.5%; Average loss: 2.6810\nIteration: 3502; Percent complete: 87.5%; Average loss: 2.8088\nIteration: 3503; Percent complete: 87.6%; Average loss: 2.7496\nIteration: 3504; Percent complete: 87.6%; Average loss: 3.0289\nIteration: 3505; Percent complete: 87.6%; Average loss: 2.4371\nIteration: 3506; Percent complete: 87.6%; Average loss: 2.7386\nIteration: 3507; Percent complete: 87.7%; Average loss: 2.5342\nIteration: 3508; Percent complete: 87.7%; Average loss: 2.8615\nIteration: 3509; Percent complete: 87.7%; Average loss: 2.7548\nIteration: 3510; Percent complete: 87.8%; Average loss: 2.7513\nIteration: 3511; Percent complete: 87.8%; Average loss: 2.7690\nIteration: 3512; Percent complete: 87.8%; Average loss: 2.9658\nIteration: 3513; Percent complete: 87.8%; Average loss: 2.7561\nIteration: 3514; Percent complete: 87.8%; Average loss: 2.7883\nIteration: 3515; Percent complete: 87.9%; Average loss: 2.8261\nIteration: 3516; Percent complete: 87.9%; Average loss: 2.7924\nIteration: 3517; Percent complete: 87.9%; Average loss: 2.7085\nIteration: 3518; Percent complete: 87.9%; Average loss: 2.7715\nIteration: 3519; Percent complete: 88.0%; Average loss: 2.8533\nIteration: 3520; Percent complete: 88.0%; Average loss: 2.6701\nIteration: 3521; Percent complete: 88.0%; Average loss: 2.9065\nIteration: 3522; Percent complete: 88.0%; Average loss: 2.8488\nIteration: 3523; Percent complete: 88.1%; Average loss: 2.6726\nIteration: 3524; Percent complete: 88.1%; Average loss: 2.8922\nIteration: 3525; Percent complete: 88.1%; Average loss: 2.9357\nIteration: 3526; Percent complete: 88.1%; Average loss: 2.6594\nIteration: 3527; Percent complete: 88.2%; Average loss: 2.6320\nIteration: 3528; Percent complete: 88.2%; Average loss: 2.8013\nIteration: 3529; Percent complete: 88.2%; Average loss: 2.8309\nIteration: 3530; Percent complete: 88.2%; Average loss: 2.7215\nIteration: 3531; Percent complete: 88.3%; Average loss: 2.6949\nIteration: 3532; Percent complete: 88.3%; Average loss: 2.6946\nIteration: 3533; Percent complete: 88.3%; Average loss: 2.7427\nIteration: 3534; Percent complete: 88.3%; Average loss: 2.9937\nIteration: 3535; Percent complete: 88.4%; Average loss: 2.6314\nIteration: 3536; Percent complete: 88.4%; Average loss: 2.6310\nIteration: 3537; Percent complete: 88.4%; Average loss: 2.5841\nIteration: 3538; Percent complete: 88.4%; Average loss: 2.7778\nIteration: 3539; Percent complete: 88.5%; Average loss: 2.7681\nIteration: 3540; Percent complete: 88.5%; Average loss: 2.7585\nIteration: 3541; Percent complete: 88.5%; Average loss: 2.8577\nIteration: 3542; Percent complete: 88.5%; Average loss: 2.7026\nIteration: 3543; Percent complete: 88.6%; Average loss: 2.7566\nIteration: 3544; Percent complete: 88.6%; Average loss: 2.7224\nIteration: 3545; Percent complete: 88.6%; Average loss: 2.5305\nIteration: 3546; Percent complete: 88.6%; Average loss: 2.8510\nIteration: 3547; Percent complete: 88.7%; Average loss: 2.7318\nIteration: 3548; Percent complete: 88.7%; Average loss: 2.8322\nIteration: 3549; Percent complete: 88.7%; Average loss: 2.6010\nIteration: 3550; Percent complete: 88.8%; Average loss: 2.6736\nIteration: 3551; Percent complete: 88.8%; Average loss: 2.6517\nIteration: 3552; Percent complete: 88.8%; Average loss: 2.7738\nIteration: 3553; Percent complete: 88.8%; Average loss: 2.5774\nIteration: 3554; Percent complete: 88.8%; Average loss: 2.6907\nIteration: 3555; Percent complete: 88.9%; Average loss: 2.5954\nIteration: 3556; Percent complete: 88.9%; Average loss: 2.7402\nIteration: 3557; Percent complete: 88.9%; Average loss: 2.6571\nIteration: 3558; Percent complete: 88.9%; Average loss: 2.8368\nIteration: 3559; Percent complete: 89.0%; Average loss: 2.7677\nIteration: 3560; Percent complete: 89.0%; Average loss: 2.7813\nIteration: 3561; Percent complete: 89.0%; Average loss: 2.7718\nIteration: 3562; Percent complete: 89.0%; Average loss: 2.6324\nIteration: 3563; Percent complete: 89.1%; Average loss: 2.7476\nIteration: 3564; Percent complete: 89.1%; Average loss: 2.7246\nIteration: 3565; Percent complete: 89.1%; Average loss: 2.6985\nIteration: 3566; Percent complete: 89.1%; Average loss: 2.8073\nIteration: 3567; Percent complete: 89.2%; Average loss: 2.5402\nIteration: 3568; Percent complete: 89.2%; Average loss: 2.6243\nIteration: 3569; Percent complete: 89.2%; Average loss: 2.6921\nIteration: 3570; Percent complete: 89.2%; Average loss: 2.7288\nIteration: 3571; Percent complete: 89.3%; Average loss: 2.6122\nIteration: 3572; Percent complete: 89.3%; Average loss: 2.7927\nIteration: 3573; Percent complete: 89.3%; Average loss: 2.7860\nIteration: 3574; Percent complete: 89.3%; Average loss: 2.8623\nIteration: 3575; Percent complete: 89.4%; Average loss: 2.7446\nIteration: 3576; Percent complete: 89.4%; Average loss: 2.7072\nIteration: 3577; Percent complete: 89.4%; Average loss: 2.8343\nIteration: 3578; Percent complete: 89.5%; Average loss: 2.8552\nIteration: 3579; Percent complete: 89.5%; Average loss: 2.8599\nIteration: 3580; Percent complete: 89.5%; Average loss: 2.7589\nIteration: 3581; Percent complete: 89.5%; Average loss: 2.6246\nIteration: 3582; Percent complete: 89.5%; Average loss: 2.8361\nIteration: 3583; Percent complete: 89.6%; Average loss: 2.7309\nIteration: 3584; Percent complete: 89.6%; Average loss: 2.5576\nIteration: 3585; Percent complete: 89.6%; Average loss: 2.5414\nIteration: 3586; Percent complete: 89.6%; Average loss: 2.6869\nIteration: 3587; Percent complete: 89.7%; Average loss: 2.6803\nIteration: 3588; Percent complete: 89.7%; Average loss: 2.9184\nIteration: 3589; Percent complete: 89.7%; Average loss: 2.7509\nIteration: 3590; Percent complete: 89.8%; Average loss: 2.7421\nIteration: 3591; Percent complete: 89.8%; Average loss: 2.6143\nIteration: 3592; Percent complete: 89.8%; Average loss: 2.9010\nIteration: 3593; Percent complete: 89.8%; Average loss: 2.5994\nIteration: 3594; Percent complete: 89.8%; Average loss: 2.6629\nIteration: 3595; Percent complete: 89.9%; Average loss: 2.7460\nIteration: 3596; Percent complete: 89.9%; Average loss: 2.6132\nIteration: 3597; Percent complete: 89.9%; Average loss: 2.6206\nIteration: 3598; Percent complete: 90.0%; Average loss: 2.7026\nIteration: 3599; Percent complete: 90.0%; Average loss: 2.5524\nIteration: 3600; Percent complete: 90.0%; Average loss: 2.7043\nIteration: 3601; Percent complete: 90.0%; Average loss: 2.5782\nIteration: 3602; Percent complete: 90.0%; Average loss: 2.7275\nIteration: 3603; Percent complete: 90.1%; Average loss: 3.0225\nIteration: 3604; Percent complete: 90.1%; Average loss: 2.7205\nIteration: 3605; Percent complete: 90.1%; Average loss: 2.5712\nIteration: 3606; Percent complete: 90.1%; Average loss: 2.8352\nIteration: 3607; Percent complete: 90.2%; Average loss: 2.7551\nIteration: 3608; Percent complete: 90.2%; Average loss: 2.7106\nIteration: 3609; Percent complete: 90.2%; Average loss: 2.7108\nIteration: 3610; Percent complete: 90.2%; Average loss: 2.7452\nIteration: 3611; Percent complete: 90.3%; Average loss: 2.5867\nIteration: 3612; Percent complete: 90.3%; Average loss: 3.0297\nIteration: 3613; Percent complete: 90.3%; Average loss: 2.6971\nIteration: 3614; Percent complete: 90.3%; Average loss: 2.5612\nIteration: 3615; Percent complete: 90.4%; Average loss: 2.7790\nIteration: 3616; Percent complete: 90.4%; Average loss: 2.7324\nIteration: 3617; Percent complete: 90.4%; Average loss: 2.8411\nIteration: 3618; Percent complete: 90.5%; Average loss: 2.6714\nIteration: 3619; Percent complete: 90.5%; Average loss: 2.9698\nIteration: 3620; Percent complete: 90.5%; Average loss: 2.8932\nIteration: 3621; Percent complete: 90.5%; Average loss: 2.5257\nIteration: 3622; Percent complete: 90.5%; Average loss: 2.6367\nIteration: 3623; Percent complete: 90.6%; Average loss: 2.6018\nIteration: 3624; Percent complete: 90.6%; Average loss: 2.8073\nIteration: 3625; Percent complete: 90.6%; Average loss: 2.7967\nIteration: 3626; Percent complete: 90.6%; Average loss: 2.8270\nIteration: 3627; Percent complete: 90.7%; Average loss: 2.5751\nIteration: 3628; Percent complete: 90.7%; Average loss: 2.3910\nIteration: 3629; Percent complete: 90.7%; Average loss: 2.6469\nIteration: 3630; Percent complete: 90.8%; Average loss: 2.6466\nIteration: 3631; Percent complete: 90.8%; Average loss: 2.6274\nIteration: 3632; Percent complete: 90.8%; Average loss: 2.5870\nIteration: 3633; Percent complete: 90.8%; Average loss: 2.9997\nIteration: 3634; Percent complete: 90.8%; Average loss: 2.7060\nIteration: 3635; Percent complete: 90.9%; Average loss: 2.5527\nIteration: 3636; Percent complete: 90.9%; Average loss: 2.4404\nIteration: 3637; Percent complete: 90.9%; Average loss: 2.7342\nIteration: 3638; Percent complete: 91.0%; Average loss: 2.6359\nIteration: 3639; Percent complete: 91.0%; Average loss: 2.6425\nIteration: 3640; Percent complete: 91.0%; Average loss: 2.6811\nIteration: 3641; Percent complete: 91.0%; Average loss: 2.6596\nIteration: 3642; Percent complete: 91.0%; Average loss: 2.6201\nIteration: 3643; Percent complete: 91.1%; Average loss: 2.7256\nIteration: 3644; Percent complete: 91.1%; Average loss: 2.6976\nIteration: 3645; Percent complete: 91.1%; Average loss: 2.7811\nIteration: 3646; Percent complete: 91.1%; Average loss: 2.9863\nIteration: 3647; Percent complete: 91.2%; Average loss: 2.9437\nIteration: 3648; Percent complete: 91.2%; Average loss: 2.3777\nIteration: 3649; Percent complete: 91.2%; Average loss: 2.4449\nIteration: 3650; Percent complete: 91.2%; Average loss: 2.8876\nIteration: 3651; Percent complete: 91.3%; Average loss: 2.7103\nIteration: 3652; Percent complete: 91.3%; Average loss: 2.5483\nIteration: 3653; Percent complete: 91.3%; Average loss: 2.7140\nIteration: 3654; Percent complete: 91.3%; Average loss: 2.6338\nIteration: 3655; Percent complete: 91.4%; Average loss: 2.6607\nIteration: 3656; Percent complete: 91.4%; Average loss: 2.8829\nIteration: 3657; Percent complete: 91.4%; Average loss: 2.6027\nIteration: 3658; Percent complete: 91.5%; Average loss: 2.4911\nIteration: 3659; Percent complete: 91.5%; Average loss: 2.6605\nIteration: 3660; Percent complete: 91.5%; Average loss: 2.7785\nIteration: 3661; Percent complete: 91.5%; Average loss: 2.4792\nIteration: 3662; Percent complete: 91.5%; Average loss: 2.7606\nIteration: 3663; Percent complete: 91.6%; Average loss: 2.7633\nIteration: 3664; Percent complete: 91.6%; Average loss: 2.6742\nIteration: 3665; Percent complete: 91.6%; Average loss: 2.9145\nIteration: 3666; Percent complete: 91.6%; Average loss: 2.4644\nIteration: 3667; Percent complete: 91.7%; Average loss: 2.6308\nIteration: 3668; Percent complete: 91.7%; Average loss: 2.8235\nIteration: 3669; Percent complete: 91.7%; Average loss: 2.7651\nIteration: 3670; Percent complete: 91.8%; Average loss: 2.4484\nIteration: 3671; Percent complete: 91.8%; Average loss: 2.7147\nIteration: 3672; Percent complete: 91.8%; Average loss: 2.8173\nIteration: 3673; Percent complete: 91.8%; Average loss: 2.6460\nIteration: 3674; Percent complete: 91.8%; Average loss: 2.7741\nIteration: 3675; Percent complete: 91.9%; Average loss: 2.8009\nIteration: 3676; Percent complete: 91.9%; Average loss: 2.7350\nIteration: 3677; Percent complete: 91.9%; Average loss: 2.7214\nIteration: 3678; Percent complete: 92.0%; Average loss: 2.7942\nIteration: 3679; Percent complete: 92.0%; Average loss: 2.5398\nIteration: 3680; Percent complete: 92.0%; Average loss: 2.6709\nIteration: 3681; Percent complete: 92.0%; Average loss: 2.5789\nIteration: 3682; Percent complete: 92.0%; Average loss: 2.7089\nIteration: 3683; Percent complete: 92.1%; Average loss: 2.8108\nIteration: 3684; Percent complete: 92.1%; Average loss: 2.8953\nIteration: 3685; Percent complete: 92.1%; Average loss: 2.8069\nIteration: 3686; Percent complete: 92.2%; Average loss: 2.5598\nIteration: 3687; Percent complete: 92.2%; Average loss: 2.6788\nIteration: 3688; Percent complete: 92.2%; Average loss: 2.6578\nIteration: 3689; Percent complete: 92.2%; Average loss: 2.5733\nIteration: 3690; Percent complete: 92.2%; Average loss: 2.5394\nIteration: 3691; Percent complete: 92.3%; Average loss: 2.7595\nIteration: 3692; Percent complete: 92.3%; Average loss: 2.6993\nIteration: 3693; Percent complete: 92.3%; Average loss: 2.6394\nIteration: 3694; Percent complete: 92.3%; Average loss: 2.7234\nIteration: 3695; Percent complete: 92.4%; Average loss: 2.8402\nIteration: 3696; Percent complete: 92.4%; Average loss: 2.6325\nIteration: 3697; Percent complete: 92.4%; Average loss: 2.7394\nIteration: 3698; Percent complete: 92.5%; Average loss: 2.6577\nIteration: 3699; Percent complete: 92.5%; Average loss: 2.6768\nIteration: 3700; Percent complete: 92.5%; Average loss: 2.6339\nIteration: 3701; Percent complete: 92.5%; Average loss: 2.7687\nIteration: 3702; Percent complete: 92.5%; Average loss: 2.5581\nIteration: 3703; Percent complete: 92.6%; Average loss: 2.3890\nIteration: 3704; Percent complete: 92.6%; Average loss: 2.5166\nIteration: 3705; Percent complete: 92.6%; Average loss: 2.6574\nIteration: 3706; Percent complete: 92.7%; Average loss: 2.8217\nIteration: 3707; Percent complete: 92.7%; Average loss: 2.8537\nIteration: 3708; Percent complete: 92.7%; Average loss: 2.6134\nIteration: 3709; Percent complete: 92.7%; Average loss: 2.5957\nIteration: 3710; Percent complete: 92.8%; Average loss: 2.6504\nIteration: 3711; Percent complete: 92.8%; Average loss: 2.6417\nIteration: 3712; Percent complete: 92.8%; Average loss: 2.6389\nIteration: 3713; Percent complete: 92.8%; Average loss: 2.6593\nIteration: 3714; Percent complete: 92.8%; Average loss: 2.4605\nIteration: 3715; Percent complete: 92.9%; Average loss: 2.4199\nIteration: 3716; Percent complete: 92.9%; Average loss: 2.7858\nIteration: 3717; Percent complete: 92.9%; Average loss: 2.6824\nIteration: 3718; Percent complete: 93.0%; Average loss: 2.7859\nIteration: 3719; Percent complete: 93.0%; Average loss: 2.6717\nIteration: 3720; Percent complete: 93.0%; Average loss: 2.5590\nIteration: 3721; Percent complete: 93.0%; Average loss: 2.7930\nIteration: 3722; Percent complete: 93.0%; Average loss: 2.4259\nIteration: 3723; Percent complete: 93.1%; Average loss: 2.7552\nIteration: 3724; Percent complete: 93.1%; Average loss: 2.7032\nIteration: 3725; Percent complete: 93.1%; Average loss: 2.7130\nIteration: 3726; Percent complete: 93.2%; Average loss: 2.7723\nIteration: 3727; Percent complete: 93.2%; Average loss: 2.5918\nIteration: 3728; Percent complete: 93.2%; Average loss: 2.4840\nIteration: 3729; Percent complete: 93.2%; Average loss: 2.6046\nIteration: 3730; Percent complete: 93.2%; Average loss: 2.9047\nIteration: 3731; Percent complete: 93.3%; Average loss: 2.8653\nIteration: 3732; Percent complete: 93.3%; Average loss: 2.5445\nIteration: 3733; Percent complete: 93.3%; Average loss: 2.6236\nIteration: 3734; Percent complete: 93.3%; Average loss: 2.5323\nIteration: 3735; Percent complete: 93.4%; Average loss: 2.4359\nIteration: 3736; Percent complete: 93.4%; Average loss: 2.7024\nIteration: 3737; Percent complete: 93.4%; Average loss: 2.7654\nIteration: 3738; Percent complete: 93.5%; Average loss: 2.8045\nIteration: 3739; Percent complete: 93.5%; Average loss: 2.5862\nIteration: 3740; Percent complete: 93.5%; Average loss: 2.4856\nIteration: 3741; Percent complete: 93.5%; Average loss: 2.7913\nIteration: 3742; Percent complete: 93.5%; Average loss: 2.7353\nIteration: 3743; Percent complete: 93.6%; Average loss: 2.4925\nIteration: 3744; Percent complete: 93.6%; Average loss: 2.8232\nIteration: 3745; Percent complete: 93.6%; Average loss: 2.8488\nIteration: 3746; Percent complete: 93.7%; Average loss: 2.7541\nIteration: 3747; Percent complete: 93.7%; Average loss: 2.7190\nIteration: 3748; Percent complete: 93.7%; Average loss: 2.7566\nIteration: 3749; Percent complete: 93.7%; Average loss: 2.6822\nIteration: 3750; Percent complete: 93.8%; Average loss: 2.5904\nIteration: 3751; Percent complete: 93.8%; Average loss: 2.8972\nIteration: 3752; Percent complete: 93.8%; Average loss: 2.6085\nIteration: 3753; Percent complete: 93.8%; Average loss: 2.5883\nIteration: 3754; Percent complete: 93.8%; Average loss: 2.5596\nIteration: 3755; Percent complete: 93.9%; Average loss: 2.6920\nIteration: 3756; Percent complete: 93.9%; Average loss: 2.5711\nIteration: 3757; Percent complete: 93.9%; Average loss: 2.7629\nIteration: 3758; Percent complete: 94.0%; Average loss: 2.5507\nIteration: 3759; Percent complete: 94.0%; Average loss: 2.6109\nIteration: 3760; Percent complete: 94.0%; Average loss: 2.5869\nIteration: 3761; Percent complete: 94.0%; Average loss: 2.6342\nIteration: 3762; Percent complete: 94.0%; Average loss: 2.3348\nIteration: 3763; Percent complete: 94.1%; Average loss: 2.8609\nIteration: 3764; Percent complete: 94.1%; Average loss: 2.7505\nIteration: 3765; Percent complete: 94.1%; Average loss: 2.4644\nIteration: 3766; Percent complete: 94.2%; Average loss: 2.6692\nIteration: 3767; Percent complete: 94.2%; Average loss: 2.8530\nIteration: 3768; Percent complete: 94.2%; Average loss: 2.8142\nIteration: 3769; Percent complete: 94.2%; Average loss: 2.6350\nIteration: 3770; Percent complete: 94.2%; Average loss: 2.5521\nIteration: 3771; Percent complete: 94.3%; Average loss: 2.4895\nIteration: 3772; Percent complete: 94.3%; Average loss: 2.6630\nIteration: 3773; Percent complete: 94.3%; Average loss: 2.6273\nIteration: 3774; Percent complete: 94.3%; Average loss: 2.5129\nIteration: 3775; Percent complete: 94.4%; Average loss: 2.6165\nIteration: 3776; Percent complete: 94.4%; Average loss: 2.5074\nIteration: 3777; Percent complete: 94.4%; Average loss: 2.6433\nIteration: 3778; Percent complete: 94.5%; Average loss: 2.7176\nIteration: 3779; Percent complete: 94.5%; Average loss: 2.7441\nIteration: 3780; Percent complete: 94.5%; Average loss: 2.6560\nIteration: 3781; Percent complete: 94.5%; Average loss: 2.7618\nIteration: 3782; Percent complete: 94.5%; Average loss: 2.7796\nIteration: 3783; Percent complete: 94.6%; Average loss: 2.6827\nIteration: 3784; Percent complete: 94.6%; Average loss: 2.4268\nIteration: 3785; Percent complete: 94.6%; Average loss: 2.5864\nIteration: 3786; Percent complete: 94.7%; Average loss: 2.3456\nIteration: 3787; Percent complete: 94.7%; Average loss: 2.5766\nIteration: 3788; Percent complete: 94.7%; Average loss: 2.4921\nIteration: 3789; Percent complete: 94.7%; Average loss: 2.5385\nIteration: 3790; Percent complete: 94.8%; Average loss: 2.6013\nIteration: 3791; Percent complete: 94.8%; Average loss: 2.7891\nIteration: 3792; Percent complete: 94.8%; Average loss: 2.2329\nIteration: 3793; Percent complete: 94.8%; Average loss: 2.7225\nIteration: 3794; Percent complete: 94.8%; Average loss: 2.9302\nIteration: 3795; Percent complete: 94.9%; Average loss: 2.5904\nIteration: 3796; Percent complete: 94.9%; Average loss: 2.4428\nIteration: 3797; Percent complete: 94.9%; Average loss: 2.6074\nIteration: 3798; Percent complete: 95.0%; Average loss: 2.2626\nIteration: 3799; Percent complete: 95.0%; Average loss: 2.4694\nIteration: 3800; Percent complete: 95.0%; Average loss: 2.5082\nIteration: 3801; Percent complete: 95.0%; Average loss: 2.7411\nIteration: 3802; Percent complete: 95.0%; Average loss: 2.5996\nIteration: 3803; Percent complete: 95.1%; Average loss: 2.4451\nIteration: 3804; Percent complete: 95.1%; Average loss: 2.7606\nIteration: 3805; Percent complete: 95.1%; Average loss: 2.5571\nIteration: 3806; Percent complete: 95.2%; Average loss: 2.4962\nIteration: 3807; Percent complete: 95.2%; Average loss: 2.7562\nIteration: 3808; Percent complete: 95.2%; Average loss: 2.6337\nIteration: 3809; Percent complete: 95.2%; Average loss: 2.6817\nIteration: 3810; Percent complete: 95.2%; Average loss: 2.6860\nIteration: 3811; Percent complete: 95.3%; Average loss: 2.8549\nIteration: 3812; Percent complete: 95.3%; Average loss: 2.6600\nIteration: 3813; Percent complete: 95.3%; Average loss: 2.4022\nIteration: 3814; Percent complete: 95.3%; Average loss: 2.7867\nIteration: 3815; Percent complete: 95.4%; Average loss: 2.6664\nIteration: 3816; Percent complete: 95.4%; Average loss: 2.4543\nIteration: 3817; Percent complete: 95.4%; Average loss: 2.6305\nIteration: 3818; Percent complete: 95.5%; Average loss: 2.6679\nIteration: 3819; Percent complete: 95.5%; Average loss: 2.7435\nIteration: 3820; Percent complete: 95.5%; Average loss: 2.6850\nIteration: 3821; Percent complete: 95.5%; Average loss: 2.6073\nIteration: 3822; Percent complete: 95.5%; Average loss: 2.6054\nIteration: 3823; Percent complete: 95.6%; Average loss: 2.5593\nIteration: 3824; Percent complete: 95.6%; Average loss: 2.6101\nIteration: 3825; Percent complete: 95.6%; Average loss: 2.6890\nIteration: 3826; Percent complete: 95.7%; Average loss: 2.6007\nIteration: 3827; Percent complete: 95.7%; Average loss: 2.5624\nIteration: 3828; Percent complete: 95.7%; Average loss: 2.7100\nIteration: 3829; Percent complete: 95.7%; Average loss: 2.5628\nIteration: 3830; Percent complete: 95.8%; Average loss: 2.6216\nIteration: 3831; Percent complete: 95.8%; Average loss: 2.5757\nIteration: 3832; Percent complete: 95.8%; Average loss: 2.7810\nIteration: 3833; Percent complete: 95.8%; Average loss: 2.4450\nIteration: 3834; Percent complete: 95.9%; Average loss: 2.7816\nIteration: 3835; Percent complete: 95.9%; Average loss: 2.7703\nIteration: 3836; Percent complete: 95.9%; Average loss: 2.8088\nIteration: 3837; Percent complete: 95.9%; Average loss: 2.5735\nIteration: 3838; Percent complete: 96.0%; Average loss: 2.6421\nIteration: 3839; Percent complete: 96.0%; Average loss: 2.5118\nIteration: 3840; Percent complete: 96.0%; Average loss: 2.7818\nIteration: 3841; Percent complete: 96.0%; Average loss: 2.6869\nIteration: 3842; Percent complete: 96.0%; Average loss: 2.5098\nIteration: 3843; Percent complete: 96.1%; Average loss: 2.6201\nIteration: 3844; Percent complete: 96.1%; Average loss: 2.6979\nIteration: 3845; Percent complete: 96.1%; Average loss: 2.6132\nIteration: 3846; Percent complete: 96.2%; Average loss: 2.8068\nIteration: 3847; Percent complete: 96.2%; Average loss: 2.6106\nIteration: 3848; Percent complete: 96.2%; Average loss: 2.3997\nIteration: 3849; Percent complete: 96.2%; Average loss: 2.7605\nIteration: 3850; Percent complete: 96.2%; Average loss: 2.6554\nIteration: 3851; Percent complete: 96.3%; Average loss: 2.3801\nIteration: 3852; Percent complete: 96.3%; Average loss: 2.7371\nIteration: 3853; Percent complete: 96.3%; Average loss: 2.7901\nIteration: 3854; Percent complete: 96.4%; Average loss: 2.6342\nIteration: 3855; Percent complete: 96.4%; Average loss: 2.7257\nIteration: 3856; Percent complete: 96.4%; Average loss: 2.4971\nIteration: 3857; Percent complete: 96.4%; Average loss: 2.6855\nIteration: 3858; Percent complete: 96.5%; Average loss: 2.4818\nIteration: 3859; Percent complete: 96.5%; Average loss: 2.4423\nIteration: 3860; Percent complete: 96.5%; Average loss: 2.8817\nIteration: 3861; Percent complete: 96.5%; Average loss: 2.5300\nIteration: 3862; Percent complete: 96.5%; Average loss: 2.5531\nIteration: 3863; Percent complete: 96.6%; Average loss: 2.7052\nIteration: 3864; Percent complete: 96.6%; Average loss: 2.6391\nIteration: 3865; Percent complete: 96.6%; Average loss: 2.6104\nIteration: 3866; Percent complete: 96.7%; Average loss: 2.8066\nIteration: 3867; Percent complete: 96.7%; Average loss: 2.8454\nIteration: 3868; Percent complete: 96.7%; Average loss: 2.4708\nIteration: 3869; Percent complete: 96.7%; Average loss: 2.5845\nIteration: 3870; Percent complete: 96.8%; Average loss: 2.8014\nIteration: 3871; Percent complete: 96.8%; Average loss: 2.6356\nIteration: 3872; Percent complete: 96.8%; Average loss: 2.6693\nIteration: 3873; Percent complete: 96.8%; Average loss: 2.7272\nIteration: 3874; Percent complete: 96.9%; Average loss: 2.6699\nIteration: 3875; Percent complete: 96.9%; Average loss: 2.4507\nIteration: 3876; Percent complete: 96.9%; Average loss: 2.6175\nIteration: 3877; Percent complete: 96.9%; Average loss: 2.5820\nIteration: 3878; Percent complete: 97.0%; Average loss: 2.7391\nIteration: 3879; Percent complete: 97.0%; Average loss: 3.0852\nIteration: 3880; Percent complete: 97.0%; Average loss: 2.6479\nIteration: 3881; Percent complete: 97.0%; Average loss: 2.8234\nIteration: 3882; Percent complete: 97.0%; Average loss: 2.6101\nIteration: 3883; Percent complete: 97.1%; Average loss: 2.4807\nIteration: 3884; Percent complete: 97.1%; Average loss: 2.8559\nIteration: 3885; Percent complete: 97.1%; Average loss: 2.5074\nIteration: 3886; Percent complete: 97.2%; Average loss: 2.6826\nIteration: 3887; Percent complete: 97.2%; Average loss: 2.7345\nIteration: 3888; Percent complete: 97.2%; Average loss: 2.6662\nIteration: 3889; Percent complete: 97.2%; Average loss: 2.6814\nIteration: 3890; Percent complete: 97.2%; Average loss: 2.7440\nIteration: 3891; Percent complete: 97.3%; Average loss: 2.4381\nIteration: 3892; Percent complete: 97.3%; Average loss: 2.7038\nIteration: 3893; Percent complete: 97.3%; Average loss: 2.6016\nIteration: 3894; Percent complete: 97.4%; Average loss: 2.5995\nIteration: 3895; Percent complete: 97.4%; Average loss: 2.6037\nIteration: 3896; Percent complete: 97.4%; Average loss: 2.5245\nIteration: 3897; Percent complete: 97.4%; Average loss: 2.4956\nIteration: 3898; Percent complete: 97.5%; Average loss: 2.6990\nIteration: 3899; Percent complete: 97.5%; Average loss: 2.5856\nIteration: 3900; Percent complete: 97.5%; Average loss: 2.6318\nIteration: 3901; Percent complete: 97.5%; Average loss: 2.7275\nIteration: 3902; Percent complete: 97.5%; Average loss: 2.5999\nIteration: 3903; Percent complete: 97.6%; Average loss: 2.6851\nIteration: 3904; Percent complete: 97.6%; Average loss: 2.6065\nIteration: 3905; Percent complete: 97.6%; Average loss: 2.7920\nIteration: 3906; Percent complete: 97.7%; Average loss: 2.5670\nIteration: 3907; Percent complete: 97.7%; Average loss: 2.7529\nIteration: 3908; Percent complete: 97.7%; Average loss: 2.6887\nIteration: 3909; Percent complete: 97.7%; Average loss: 2.8272\nIteration: 3910; Percent complete: 97.8%; Average loss: 2.5402\nIteration: 3911; Percent complete: 97.8%; Average loss: 2.4893\nIteration: 3912; Percent complete: 97.8%; Average loss: 2.4985\nIteration: 3913; Percent complete: 97.8%; Average loss: 2.4258\nIteration: 3914; Percent complete: 97.9%; Average loss: 2.6753\nIteration: 3915; Percent complete: 97.9%; Average loss: 2.2886\nIteration: 3916; Percent complete: 97.9%; Average loss: 2.4678\nIteration: 3917; Percent complete: 97.9%; Average loss: 2.6315\nIteration: 3918; Percent complete: 98.0%; Average loss: 2.8498\nIteration: 3919; Percent complete: 98.0%; Average loss: 2.5685\nIteration: 3920; Percent complete: 98.0%; Average loss: 2.7909\nIteration: 3921; Percent complete: 98.0%; Average loss: 2.4470\nIteration: 3922; Percent complete: 98.0%; Average loss: 2.6524\nIteration: 3923; Percent complete: 98.1%; Average loss: 2.3591\nIteration: 3924; Percent complete: 98.1%; Average loss: 2.6376\nIteration: 3925; Percent complete: 98.1%; Average loss: 2.6596\nIteration: 3926; Percent complete: 98.2%; Average loss: 2.7279\nIteration: 3927; Percent complete: 98.2%; Average loss: 2.9343\nIteration: 3928; Percent complete: 98.2%; Average loss: 2.4957\nIteration: 3929; Percent complete: 98.2%; Average loss: 2.5786\nIteration: 3930; Percent complete: 98.2%; Average loss: 2.5958\nIteration: 3931; Percent complete: 98.3%; Average loss: 2.4616\nIteration: 3932; Percent complete: 98.3%; Average loss: 2.6612\nIteration: 3933; Percent complete: 98.3%; Average loss: 2.8059\nIteration: 3934; Percent complete: 98.4%; Average loss: 2.5671\nIteration: 3935; Percent complete: 98.4%; Average loss: 2.8866\nIteration: 3936; Percent complete: 98.4%; Average loss: 2.6516\nIteration: 3937; Percent complete: 98.4%; Average loss: 2.4936\nIteration: 3938; Percent complete: 98.5%; Average loss: 2.6378\nIteration: 3939; Percent complete: 98.5%; Average loss: 3.0647\nIteration: 3940; Percent complete: 98.5%; Average loss: 2.6255\nIteration: 3941; Percent complete: 98.5%; Average loss: 2.4808\nIteration: 3942; Percent complete: 98.6%; Average loss: 2.7380\nIteration: 3943; Percent complete: 98.6%; Average loss: 2.7029\nIteration: 3944; Percent complete: 98.6%; Average loss: 2.7299\nIteration: 3945; Percent complete: 98.6%; Average loss: 2.4549\nIteration: 3946; Percent complete: 98.7%; Average loss: 2.7582\nIteration: 3947; Percent complete: 98.7%; Average loss: 2.6672\nIteration: 3948; Percent complete: 98.7%; Average loss: 3.0257\nIteration: 3949; Percent complete: 98.7%; Average loss: 2.7312\nIteration: 3950; Percent complete: 98.8%; Average loss: 2.6668\nIteration: 3951; Percent complete: 98.8%; Average loss: 2.7235\nIteration: 3952; Percent complete: 98.8%; Average loss: 2.7313\nIteration: 3953; Percent complete: 98.8%; Average loss: 2.6959\nIteration: 3954; Percent complete: 98.9%; Average loss: 2.5879\nIteration: 3955; Percent complete: 98.9%; Average loss: 2.5201\nIteration: 3956; Percent complete: 98.9%; Average loss: 2.5049\nIteration: 3957; Percent complete: 98.9%; Average loss: 2.5579\nIteration: 3958; Percent complete: 99.0%; Average loss: 2.6588\nIteration: 3959; Percent complete: 99.0%; Average loss: 2.8685\nIteration: 3960; Percent complete: 99.0%; Average loss: 2.6360\nIteration: 3961; Percent complete: 99.0%; Average loss: 2.5147\nIteration: 3962; Percent complete: 99.1%; Average loss: 2.5744\nIteration: 3963; Percent complete: 99.1%; Average loss: 2.4921\nIteration: 3964; Percent complete: 99.1%; Average loss: 2.5656\nIteration: 3965; Percent complete: 99.1%; Average loss: 2.6053\nIteration: 3966; Percent complete: 99.2%; Average loss: 2.6386\nIteration: 3967; Percent complete: 99.2%; Average loss: 2.8323\nIteration: 3968; Percent complete: 99.2%; Average loss: 2.7320\nIteration: 3969; Percent complete: 99.2%; Average loss: 2.7009\nIteration: 3970; Percent complete: 99.2%; Average loss: 2.6720\nIteration: 3971; Percent complete: 99.3%; Average loss: 2.6306\nIteration: 3972; Percent complete: 99.3%; Average loss: 2.5433\nIteration: 3973; Percent complete: 99.3%; Average loss: 2.3170\nIteration: 3974; Percent complete: 99.4%; Average loss: 2.5361\nIteration: 3975; Percent complete: 99.4%; Average loss: 2.6451\nIteration: 3976; Percent complete: 99.4%; Average loss: 2.6958\nIteration: 3977; Percent complete: 99.4%; Average loss: 2.5484\nIteration: 3978; Percent complete: 99.5%; Average loss: 2.5897\nIteration: 3979; Percent complete: 99.5%; Average loss: 2.7198\nIteration: 3980; Percent complete: 99.5%; Average loss: 2.5664\nIteration: 3981; Percent complete: 99.5%; Average loss: 2.4259\nIteration: 3982; Percent complete: 99.6%; Average loss: 2.5914\nIteration: 3983; Percent complete: 99.6%; Average loss: 2.5198\nIteration: 3984; Percent complete: 99.6%; Average loss: 2.6414\nIteration: 3985; Percent complete: 99.6%; Average loss: 2.3692\nIteration: 3986; Percent complete: 99.7%; Average loss: 2.4314\nIteration: 3987; Percent complete: 99.7%; Average loss: 2.4919\nIteration: 3988; Percent complete: 99.7%; Average loss: 2.4943\nIteration: 3989; Percent complete: 99.7%; Average loss: 2.8817\nIteration: 3990; Percent complete: 99.8%; Average loss: 2.4641\nIteration: 3991; Percent complete: 99.8%; Average loss: 2.5646\nIteration: 3992; Percent complete: 99.8%; Average loss: 2.5629\nIteration: 3993; Percent complete: 99.8%; Average loss: 2.5594\nIteration: 3994; Percent complete: 99.9%; Average loss: 2.8016\nIteration: 3995; Percent complete: 99.9%; Average loss: 2.6659\nIteration: 3996; Percent complete: 99.9%; Average loss: 2.5565\nIteration: 3997; Percent complete: 99.9%; Average loss: 2.5439\nIteration: 3998; Percent complete: 100.0%; Average loss: 2.6255\nIteration: 3999; Percent complete: 100.0%; Average loss: 2.4204\nIteration: 4000; Percent complete: 100.0%; Average loss: 2.5053")]),e._v(" "),t("h3",{attrs:{id:"запустить-оценку"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#запустить-оценку"}},[e._v("#")]),e._v(" Запустить оценку"),t("a",{attrs:{href:"https://pytorch.org/tutorials/beginner/chatbot_tutorial.html?highlight=chat%20bot#run-evaluation",target:"_blank",rel:"noopener noreferrer"}},[t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("Чтобы пообщаться с вашей моделью, запустите следующий блок.")]),e._v(" "),t("h1",{attrs:{id:"set-dropout-layers-to-eval-mode"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#set-dropout-layers-to-eval-mode"}},[e._v("#")]),e._v(" Set dropout layers to eval mode")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval",title:"torch.nn.Module.eval",target:"_blank",rel:"noopener noreferrer"}},[e._v("encoder.eval"),t("OutboundLink")],1),e._v("()\n"),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval",title:"torch.nn.Module.eval",target:"_blank",rel:"noopener noreferrer"}},[e._v("decoder.eval"),t("OutboundLink")],1),e._v("()")]),e._v(" "),t("h1",{attrs:{id:"initialize-search-module"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#initialize-search-module"}},[e._v("#")]),e._v(" Initialize search module")]),e._v(" "),t("p",[e._v("searcher = "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module",title:"torch.nn.Module",target:"_blank",rel:"noopener noreferrer"}},[e._v("GreedySearchDecoder"),t("OutboundLink")],1),e._v("(encoder, decoder)")]),e._v(" "),t("h1",{attrs:{id:"begin-chatting-uncomment-and-run-the-following-line-to-begin"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#begin-chatting-uncomment-and-run-the-following-line-to-begin"}},[e._v("#")]),e._v(" Begin chatting (uncomment and run the following line to begin)")]),e._v(" "),t("h1",{attrs:{id:"evaluateinput-encoder-decoder-searcher-voc"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#evaluateinput-encoder-decoder-searcher-voc"}},[e._v("#")]),e._v(" evaluateInput(encoder, decoder, searcher, voc)")]),e._v(" "),t("h2",{attrs:{id:"заключение"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#заключение"}},[e._v("#")]),e._v(" Заключение"),t("a",{attrs:{href:"https://pytorch.org/tutorials/beginner/chatbot_tutorial.html?highlight=chat%20bot#conclusion",target:"_blank",rel:"noopener noreferrer"}},[t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("На этом все, ребята. Поздравляем, теперь вы знаете основы создания модели генеративного чат- бота ! Если вам интересно, вы можете попробовать настроить поведение чат-бота, изменив модель и параметры обучения, а также настроив данные, на которых вы обучаете модель.")]),e._v(" "),t("p",[e._v("Ознакомьтесь с другими руководствами, чтобы узнать о других интересных приложениях для глубокого обучения в PyTorch!")]),e._v(" "),t("p",[t("strong",[e._v("Общее время работы скрипта:")]),e._v(" ( 5 минут 46.009 секунд)")])])}),[],!1,null,null,null);t.default=o.exports}}]);