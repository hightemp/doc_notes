(window.webpackJsonp=window.webpackJsonp||[]).push([[279],{553:function(r,e,t){"use strict";t.r(e);var a=t(14),n=Object(a.a)({},(function(){var r=this,e=r._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":r.$parent.slotKey}},[e("p",[r._v("https://habr.com/ru/articles/516098/")]),r._v(" "),e("p",[r._v("Два года назад я писал на Хабр "),e("a",{attrs:{href:"https://habr.com/ru/post/349864/",target:"_blank",rel:"noopener noreferrer"}},[r._v("статью про Yargy-парсер и библиотеку Natasha"),e("OutboundLink")],1),r._v(", рассказывал про решение задачи NER для русского языка, построенное на правилах. Проект хорошо приняли. Yargy-парсер заменил "),e("a",{attrs:{href:"https://yandex.ru/dev/tomita/",target:"_blank",rel:"noopener noreferrer"}},[r._v("яндексовый Томита-парсер"),e("OutboundLink")],1),r._v(" в крупных проектах внутри Сбера, Интерфакса и РИА Новостей. Библиотека Natasha сейчас встроена в образовательные программы ВШЭ, МФТИ и МГУ.")]),r._v(" "),e("p",[r._v("Проект подрос, библиотека теперь решает все базовые задачи обработки естественного русского языка: сегментация на токены и предложения, морфологический и синтаксический анализ, лемматизация, извлечение именованных сущностей."),e("br"),r._v(" "),e("img",{attrs:{src:"https://habrastorage.org/r/w1560/webt/r7/_x/u-/r7_xu-3lepbdxktr04l41oujfae.png",alt:""}}),e("br"),r._v("\nДля новостных статей качество на всех задачах "),e("a",{attrs:{href:"https://github.com/natasha/natasha#evaluation",target:"_blank",rel:"noopener noreferrer"}},[r._v("сравнимо или превосходит существующие решения"),e("OutboundLink")],1),r._v(". Например с задачей NER Natasha справляется на 1 процентный пункт хуже, чем "),e("a",{attrs:{href:"https://github.com/natasha/naeval#deeppavlov_bert_ner",target:"_blank",rel:"noopener noreferrer"}},[r._v("Deeppavlov BERT NER"),e("OutboundLink")],1),r._v(" (F1 PER 0.97, LOC 0.91, ORG 0.85), модель весит в 75 раз меньше (27МБ), работает на CPU в 2 раза быстрее (25 статей/сек), чем BERT NER на GPU.")]),r._v(" "),e("p",[e("a",{attrs:{href:"https://github.com/natasha",target:"_blank",rel:"noopener noreferrer"}},[r._v("В проекте 9 репозиториев"),e("OutboundLink")],1),r._v(", библиотека Natasha объединяет их под одним интерфейсом. В статье поговорим про новые инструменты, сравним их с существующими решениями: "),e("a",{attrs:{href:"http://deeppavlov.ai/",target:"_blank",rel:"noopener noreferrer"}},[r._v("Deeppavlov"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://spacy.io/",target:"_blank",rel:"noopener noreferrer"}},[r._v("SpaCy"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"http://ufal.mff.cuni.cz/udpipe",target:"_blank",rel:"noopener noreferrer"}},[r._v("UDPipe"),e("OutboundLink")],1),r._v("."),e("br"),r._v(" "),e("img",{attrs:{src:"https://habrastorage.org/r/w1560/webt/pu/63/yp/pu63ypcw3q3pb9f06pufhdynppw.png",alt:""}})]),r._v(" "),e("blockquote",[e("p",[r._v("Этому лонгриду предшествовала серия публикация на сайте "),e("a",{attrs:{href:"http://natasha.github.io/",target:"_blank",rel:"noopener noreferrer"}},[r._v("natasha.github.io"),e("OutboundLink")],1),r._v(":")]),r._v(" "),e("ul",[e("li",[e("a",{attrs:{href:"http://natasha.github.io/ner",target:"_blank",rel:"noopener noreferrer"}},[r._v("Natasha — качественный компактный NER для русского языка"),e("OutboundLink")],1)]),r._v(" "),e("li",[e("a",{attrs:{href:"http://natasha.github.io/navec",target:"_blank",rel:"noopener noreferrer"}},[r._v("Navec — компактные эмбеддинги для русского языка"),e("OutboundLink")],1)]),r._v(" "),e("li",[e("a",{attrs:{href:"http://natasha.github.io/corus",target:"_blank",rel:"noopener noreferrer"}},[r._v("Corus — коллекция русскоязычных NLP-датасетов"),e("OutboundLink")],1)]),r._v(" "),e("li",[e("a",{attrs:{href:"http://natasha.github.io/razdel",target:"_blank",rel:"noopener noreferrer"}},[r._v("Razdel — сегментация русскоязычного текста на токены и предложения"),e("OutboundLink")],1)]),r._v(" "),e("li",[e("a",{attrs:{href:"http://natasha.github.io/naeval",target:"_blank",rel:"noopener noreferrer"}},[r._v("Naeval — количественное сравнение систем для русскоязычного NLP"),e("OutboundLink")],1)]),r._v(" "),e("li",[e("a",{attrs:{href:"http://natasha.github.io/nerus",target:"_blank",rel:"noopener noreferrer"}},[r._v("Nerus — большой синтетический русскоязычный датасет с разметкой морфологии, синтаксиса и именованных сущностей"),e("OutboundLink")],1)])]),r._v(" "),e("p",[r._v("В тексте использованы заметки и обсуждения из чата "),e("a",{attrs:{href:"https://t.me/natural_language_processing",target:"_blank",rel:"noopener noreferrer"}},[r._v("t.me/natural_language_processing"),e("OutboundLink")],1),r._v(", там же в закрепе появляются ссылки на новые материалы:")]),r._v(" "),e("ul",[e("li",[e("a",{attrs:{href:"https://t.me/natural_language_processing/17369",target:"_blank",rel:"noopener noreferrer"}},[r._v("Почему Natasha не использует Transformers. BERT в 100 строк"),e("OutboundLink")],1)]),r._v(" "),e("li",[e("a",{attrs:{href:"https://t.me/natural_language_processing/18186",target:"_blank",rel:"noopener noreferrer"}},[r._v("BERT-модели Slovnet"),e("OutboundLink")],1)]),r._v(" "),e("li",[e("a",{attrs:{href:"https://t.me/natural_language_processing/18673",target:"_blank",rel:"noopener noreferrer"}},[r._v("Ламповый стрим про историю проекта Natasha"),e("OutboundLink")],1)]),r._v(" "),e("li",[e("a",{attrs:{href:"https://t.me/natural_language_processing/19507",target:"_blank",rel:"noopener noreferrer"}},[r._v("Обновлённая документация Yargy"),e("OutboundLink")],1)]),r._v(" "),e("li",[e("a",{attrs:{href:"https://t.me/natural_language_processing/19508",target:"_blank",rel:"noopener noreferrer"}},[r._v("Дополнительные материалы по Yargy-парсеру"),e("OutboundLink")],1)])])]),r._v(" "),e("p",[r._v("Если вас пугает размер текста ниже, посмотрите первые 20 минут лампового стрима про историю проекта Natasha, там короткий пересказ:")]),r._v(" "),e("p",[r._v("Кто больше любит слушать, посмотрите часовой доклад на Datafest 2020, он почти покрывает этот пост:")]),r._v(" "),e("p",[r._v("Содержание:")]),r._v(" "),e("ul",[e("li",[e("a",{attrs:{href:"https://habr.com/ru/articles/516098/#natasha",target:"_blank",rel:"noopener noreferrer"}},[r._v("Natasha — набор качественных открытых инструментов для обработки естественного русского языка. Интерфейс для низкоуровневых библиотек проекта"),e("OutboundLink")],1)]),r._v(" "),e("li",[e("a",{attrs:{href:"https://habr.com/ru/articles/516098/#razdel",target:"_blank",rel:"noopener noreferrer"}},[r._v("Razdel — сегментация русскоязычного текста на токены и предложения"),e("OutboundLink")],1)]),r._v(" "),e("li",[e("a",{attrs:{href:"https://habr.com/ru/articles/516098/#slovnet",target:"_blank",rel:"noopener noreferrer"}},[r._v("Slovnet — deep learning моделирование для обработки естественного русского языка"),e("OutboundLink")],1)]),r._v(" "),e("li",[e("a",{attrs:{href:"https://habr.com/ru/articles/516098/#navec",target:"_blank",rel:"noopener noreferrer"}},[r._v("Navec — компактные эмбеддинги для русского языка"),e("OutboundLink")],1)]),r._v(" "),e("li",[e("a",{attrs:{href:"https://habr.com/ru/articles/516098/#nerus",target:"_blank",rel:"noopener noreferrer"}},[r._v("Nerus — большой синтетический датасет с разметкой морфологии, синтаксиса и именованных сущностей"),e("OutboundLink")],1)]),r._v(" "),e("li",[e("a",{attrs:{href:"https://habr.com/ru/articles/516098/#corus",target:"_blank",rel:"noopener noreferrer"}},[r._v("Corus — коллекция ссылок на публичные русскоязычные датасеты + функции для загрузки"),e("OutboundLink")],1)]),r._v(" "),e("li",[e("a",{attrs:{href:"https://habr.com/ru/articles/516098/#naeval",target:"_blank",rel:"noopener noreferrer"}},[r._v("Naeval — количественное сравнение систем для русскоязычного NLP"),e("OutboundLink")],1)]),r._v(" "),e("li",[e("a",{attrs:{href:"https://habr.com/ru/articles/516098/#yargy",target:"_blank",rel:"noopener noreferrer"}},[r._v("Yargy-парсер — извлечение структурированное информации из текстов на русском языке с помощью грамматик и словарей"),e("OutboundLink")],1)]),r._v(" "),e("li",[e("a",{attrs:{href:"https://habr.com/ru/articles/516098/#ipymarkup",target:"_blank",rel:"noopener noreferrer"}},[r._v("Ipymarkup — визуализация разметки именованных сущностей и синтаксических связей"),e("OutboundLink")],1)])]),r._v(" "),e("h2",{attrs:{id:"natasha-набор-качественных-открытых-инструментов-для-обработки-естественного-русского-языка-интерфеис-для-низкоуровневых-библиотек-проекта"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#natasha-набор-качественных-открытых-инструментов-для-обработки-естественного-русского-языка-интерфеис-для-низкоуровневых-библиотек-проекта"}},[r._v("#")]),r._v(" Natasha — набор качественных открытых инструментов для обработки естественного русского языка. Интерфейс для низкоуровневых библиотек проекта")]),r._v(" "),e("p",[e("img",{attrs:{src:"https://habrastorage.org/r/w1560/webt/rz/dk/l-/rzdkl-9ynag5bfpvd_jtzqpz0iu.png",alt:""}})]),r._v(" "),e("p",[r._v("Раньше библиотека Natasha решала задачу "),e("a",{attrs:{href:"https://habr.com/ru/post/349864/",target:"_blank",rel:"noopener noreferrer"}},[r._v("NER для русского языка, была построена на правилах"),e("OutboundLink")],1),r._v(", показывала среднее качество и производительность. Сейчас Natasha — это целый "),e("a",{attrs:{href:"https://github.com/natasha",target:"_blank",rel:"noopener noreferrer"}},[r._v("большой проект, состоит из 9 репозиториев"),e("OutboundLink")],1),r._v(". "),e("a",{attrs:{href:"https://github.com/natasha/natasha",target:"_blank",rel:"noopener noreferrer"}},[r._v("Библиотека Natasha"),e("OutboundLink")],1),r._v(" объединяет их под одним интерфейсом, решает базовые задачи обработки естественного русского языка: сегментация на токены и предложения, предобученные эмбеддинги, анализ морфологии и синтаксиса, лемматизация, NER. Все решения показывают "),e("a",{attrs:{href:"https://github.com/natasha/natasha#evaluation",target:"_blank",rel:"noopener noreferrer"}},[r._v("топовые результаты в новостной тематике"),e("OutboundLink")],1),r._v(", быстро работают на CPU.")]),r._v(" "),e("p",[r._v("Natasha похожа на другие библиотеки-комбайны: "),e("a",{attrs:{href:"https://spacy.io/",target:"_blank",rel:"noopener noreferrer"}},[r._v("SpaCy"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"http://ufal.mff.cuni.cz/udpipe",target:"_blank",rel:"noopener noreferrer"}},[r._v("UDPipe"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://stanfordnlp.github.io/stanza/",target:"_blank",rel:"noopener noreferrer"}},[r._v("Stanza"),e("OutboundLink")],1),r._v(". SpaCy инициализирует и вызывает модели неявно, пользователь передаёт текст в магическую функцию "),e("code",[r._v("nlp")]),r._v(", получает полностью разобранный документ.")]),r._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[r._v("import spacy\n\n# Внутри load происходит загрузка предобученных эмбеддингов,\n# инициализация компонентов для разбора морфологии, синтаксиса и NER\nnlp = spacy.load('...')\n\n# Пайплайн моделей обрабатывает текст, возвращает разобранный документ\ntext = '...'\ndoc = nlp(text)\n")])])]),e("p",[r._v("Интерфейс Natasha более многословный. Пользователь явно инициализирует компоненты: загружает предобученные эмбеддинги, передаёт их в конструкторы моделей. Сам вызывает методы "),e("code",[r._v("segment")]),r._v(", "),e("code",[r._v("tag_morph")]),r._v(", "),e("code",[r._v("parse_syntax")]),r._v(" для сегментации на токены и предложения, анализа морфологии и синтаксиса.")]),r._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[r._v(">>> from natasha import (\n    Segmenter,\n    \n    NewsEmbedding,\n    NewsMorphTagger,\n    NewsSyntaxParser,\n    \n    Doc\n)\n\n>>> segmenter = Segmenter()\n\n>>> emb = NewsEmbedding()\n>>> morph_tagger = NewsMorphTagger(emb)\n>>> syntax_parser = NewsSyntaxParser(emb)\n\n>>> text = 'Посол Израиля на Украине Йоэль Лион признался, что пришел в шок, узнав о решении властей Львовской области объявить 2019 год годом лидера запрещенной в России Организации украинских националистов (ОУН) Степана Бандеры...'\n>>> doc = Doc(text)\n\n>>> doc.segment(segmenter)\n>>> doc.tag_morph(morph_tagger)\n>>> doc.parse_syntax(syntax_parser)\n\n>>> sent = doc.sents[0]\n>>> sent.morph.print()\n               Посол NOUN|Animacy=Anim|Case=Nom|Gender=Masc|Number=Sing\n             Израиля PROPN|Animacy=Inan|Case=Gen|Gender=Masc|Number=Sing\n                  на ADP\n             Украине PROPN|Animacy=Inan|Case=Loc|Gender=Fem|Number=Sing\n               Йоэль PROPN|Animacy=Anim|Case=Nom|Gender=Masc|Number=Sing\n                Лион PROPN|Animacy=Anim|Case=Nom|Gender=Masc|Number=Sing\n...\n\n>>> sent.syntax.print()\n        ┌──► Посол         nsubj\n        │    Израиля       \n        │ ┌► на            case\n        │ └─ Украине       \n        │ ┌─ Йоэль         \n        │ └► Лион          flat:name\n┌─────┌─└─── признался     \n│     │ ┌──► ,             punct\n│     │ │ ┌► что           mark\n│     └►└─└─ пришел        ccomp\n│     │   ┌► в             case\n│     └──►└─ шок           obl\n...\n")])])]),e("p",[r._v("Модуль извлечения именованных сущностей не зависит от результатов морфологического и синтаксического разбора, его можно использовать отдельно.")]),r._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[r._v(">>> from natasha import NewsNERTagger\n\n>>> ner_tagger = NewsNERTagger(emb)\n>>> doc.tag_ner(ner_tagger)\n>>> doc.ner.print()\nПосол Израиля на Украине Йоэль Лион признался, что пришел в шок, узнав\n      LOC────    LOC──── PER───────                                   \n о решении властей Львовской области объявить 2019 год годом лидера \n                   LOC──────────────                                \nзапрещенной в России Организации украинских националистов (ОУН) \n              LOC─── ORG─────────────────────────────────────── \nСтепана Бандеры...\nPER──────────── \n")])])]),e("p",[r._v("Natasha решает задачу лемматизации, использует "),e("a",{attrs:{href:"https://pymorphy2.readthedocs.io/en/latest/",target:"_blank",rel:"noopener noreferrer"}},[r._v("Pymorphy2"),e("OutboundLink")],1),r._v(" и результаты морфологического разбора.")]),r._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[r._v(">>> from natasha import MorphVocab\n\n>>> morph_vocab = MorphVocab()\n\n>>> for token in doc.tokens:\n>>>     token.lemmatize(morph_vocab)\n\n>>> {_.text: _.lemma for _ in doc.tokens}\n{'Посол': 'посол',\n 'Израиля': 'израиль',\n 'на': 'на',\n 'Украине': 'украина',\n 'Йоэль': 'йоэль',\n 'Лион': 'лион',\n 'признался': 'признаться',\n ',': ',',\n 'что': 'что',\n 'пришел': 'прийти'\n...\n")])])]),e("p",[r._v("Чтобы привести словосочетание к нормальной форме, недостаточно найти леммы отдельных слов, для «МИД России» получится «МИД Россия», для «Организации украинских националистов» — «Организация украинский националист». Natasha использует результаты синтаксического разбора, учитывает связи между словами, нормализует именованные сущности.")]),r._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[r._v(">>> for span in doc.spans:\n>>>    span.normalize(morph_vocab)\n\n>>> {_.text: _.normal for _ in doc.spans}\n{'Израиля': 'Израиль',\n 'Украине': 'Украина',\n 'Йоэль Лион': 'Йоэль Лион',\n 'Львовской области': 'Львовская область',\n 'России': 'Россия',\n 'Организации украинских националистов (ОУН)': 'Организация украинских националистов (ОУН)',\n 'Степана Бандеры': 'Степан Бандера',\n...\n")])])]),e("p",[r._v("Natasha находит в тексте имена, названия организаций и топонимов. Для имён в библиотеке есть набор готовых правил для "),e("a",{attrs:{href:"https://github.com/natasha/yargy",target:"_blank",rel:"noopener noreferrer"}},[r._v("Yargy-парсера"),e("OutboundLink")],1),r._v(", модуль делит нормированные имена на части, из «Виктор Федорович Ющенко» получается "),e("code",[r._v("{first: Виктор, last: Ющенко, middle: Федорович}")]),r._v(".")]),r._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[r._v(">>> from natasha import (\n    PER,\n    NamesExtractor,\n)\n\n>>> names_extractor = NamesExtractor(morph_vocab)\n\n>>> for span in doc.spans:\n>>>    if span.type == PER:\n>>>        span.extract_fact(names_extractor)\n\n>>> {_.normal: _.fact.as_dict for _ in doc.spans if _.type == PER}\n{'Йоэль Лион': {'first': 'Йоэль', 'last': 'Лион'},\n 'Степан Бандера': {'first': 'Степан', 'last': 'Бандера'},\n 'Петр Порошенко': {'first': 'Петр', 'last': 'Порошенко'},\n 'Бандера': {'last': 'Бандера'},\n 'Виктор Ющенко': {'first': 'Виктор', 'last': 'Ющенко'}}\n")])])]),e("p",[r._v("В библиотеке собраны правила для разбора дат, сумм денег и адресов, они описаны в "),e("a",{attrs:{href:"https://nbviewer.jupyter.org/github/natasha/natasha/blob/master/docs.ipynb",target:"_blank",rel:"noopener noreferrer"}},[r._v("документации и справочнике"),e("OutboundLink")],1),r._v(".")]),r._v(" "),e("p",[r._v("Библиотека Natasha хорошо подходит для демонстрации технологий проекта, используется в образовании. Архивы с весами моделей встроены в пакет, после установки не нужно ничего скачивать и настраивать.")]),r._v(" "),e("p",[r._v("Natasha объединяет под одним интерфейсом другие библиотеки проекта. Для решения практических задач стоит использовать их напрямую:")]),r._v(" "),e("ul",[e("li",[e("p",[e("a",{attrs:{href:"https://github.com/natasha/razdel",target:"_blank",rel:"noopener noreferrer"}},[r._v("Razdel"),e("OutboundLink")],1),r._v(" — сегментация текста на предложения и токены;")])]),r._v(" "),e("li",[e("p",[e("a",{attrs:{href:"https://github.com/natasha/navec",target:"_blank",rel:"noopener noreferrer"}},[r._v("Navec"),e("OutboundLink")],1),r._v(" — качественный компактные эмбеддинги;")])]),r._v(" "),e("li",[e("p",[e("a",{attrs:{href:"https://github.com/natasha/slovnet",target:"_blank",rel:"noopener noreferrer"}},[r._v("Slovnet"),e("OutboundLink")],1),r._v(" — современные компактные модели для морфологии, синтаксиса, NER;")])]),r._v(" "),e("li",[e("p",[e("a",{attrs:{href:"https://github.com/natasha/yargy",target:"_blank",rel:"noopener noreferrer"}},[r._v("Yargy"),e("OutboundLink")],1),r._v(" — правила и словари для извлечения структурированной информации;")])]),r._v(" "),e("li",[e("p",[e("a",{attrs:{href:"https://github.com/natasha/ipymarkup",target:"_blank",rel:"noopener noreferrer"}},[r._v("Ipymarkup"),e("OutboundLink")],1),r._v(" — визуализация NER и синтаксической разметки;")])]),r._v(" "),e("li",[e("p",[e("a",{attrs:{href:"https://github.com/natasha/corus",target:"_blank",rel:"noopener noreferrer"}},[r._v("Corus"),e("OutboundLink")],1),r._v(" — коллекция ссылок на публичные русскоязычные датасеты;")])]),r._v(" "),e("li",[e("p",[e("a",{attrs:{href:"https://github.com/natasha/nerus",target:"_blank",rel:"noopener noreferrer"}},[r._v("Nerus"),e("OutboundLink")],1),r._v(" — большой корпус с автоматической разметкой именованных сущностей, морфологии и синтаксиса.")])])]),r._v(" "),e("h2",{attrs:{id:"razdel-сегментация-русскоязычного-текста-на-токены-и-предложения"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#razdel-сегментация-русскоязычного-текста-на-токены-и-предложения"}},[r._v("#")]),r._v(" Razdel — сегментация русскоязычного текста на токены и предложения")]),r._v(" "),e("p",[e("img",{attrs:{src:"https://habrastorage.org/r/w1560/webt/qv/l0/ts/qvl0tsnrvumgt5wb4qczr9na6ci.png",alt:""}})]),r._v(" "),e("p",[e("a",{attrs:{href:"https://github.com/natasha/razdel",target:"_blank",rel:"noopener noreferrer"}},[r._v("Библиотека Razdel"),e("OutboundLink")],1),r._v(" — часть проекта Natasha, делит русскоязычный текст на токены и предложения. "),e("a",{attrs:{href:"https://github.com/natasha/razdel#installation",target:"_blank",rel:"noopener noreferrer"}},[r._v("Инструкция по установке"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/razdel#usage",target:"_blank",rel:"noopener noreferrer"}},[r._v("пример использования"),e("OutboundLink")],1),r._v(" и "),e("a",{attrs:{href:"https://github.com/natasha/razdel#evaluation",target:"_blank",rel:"noopener noreferrer"}},[r._v("замеры производительности"),e("OutboundLink")],1),r._v(" в репозитории Razdel.")]),r._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[r._v(">>> from razdel import tokenize, sentenize\n\n>>> text = 'Кружка-термос на 0.5л (50/64 см³, 516;...)'\n>>> list(tokenize(text))\n[Substring(start=0, stop=13, text='Кружка-термос'),\n Substring(start=14, stop=16, text='на'),\n Substring(start=17, stop=20, text='0.5'),\n Substring(start=20, stop=21, text='л'),\n Substring(start=22, stop=23, text='(')\n ...]\n\n>>> text = '''\n... - \"Так в чем же дело?\" - \"Не ра-ду-ют\".\n... И т. д. и т. п. В общем, вся газета\n... '''\n>>> list(sentenize(text))\n[Substring(start=1, stop=23, text='- \"Так в чем же дело?\"'),\n Substring(start=24, stop=40, text='- \"Не ра-ду-ют\".'),\n Substring(start=41, stop=56, text='И т. д. и т. п.'),\n Substring(start=57, stop=76, text='В общем, вся газета')]\n")])])]),e("p",[r._v("Современные модели часто не заморачиваются на счёт сегментации, используют "),e("a",{attrs:{href:"https://dyakonov.org/2019/11/29/%D1%82%D0%BE%D0%BA%D0%B5%D0%BD%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F-%D0%BD%D0%B0-%D0%BF%D0%BE%D0%B4%D1%81%D0%BB%D0%BE%D0%B2%D0%B0-subword-tokenization/",target:"_blank",rel:"noopener noreferrer"}},[r._v("BPE"),e("OutboundLink")],1),r._v(", показывают замечательные результаты, вспомним все версии "),e("a",{attrs:{href:"https://openai.com/blog/better-language-models/",target:"_blank",rel:"noopener noreferrer"}},[r._v("GPT"),e("OutboundLink")],1),r._v(" и зоопарк "),e("a",{attrs:{href:"https://arxiv.org/abs/1810.04805",target:"_blank",rel:"noopener noreferrer"}},[r._v("BERT"),e("OutboundLink")],1),r._v("ов. Natasha решает задачи разбора морфологии и синтаксиса, они имеют смысл только для отдельных слов внутри одного предложения. Поэтому мы ответственно подходим к этапу сегментации, стараемся повторить разметку из популярных открытых датасетов: "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_ud_syntag",target:"_blank",rel:"noopener noreferrer"}},[r._v("SynTagRus"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_morphoru_corpora",target:"_blank",rel:"noopener noreferrer"}},[r._v("OpenCorpora"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_morphoru_gicrya",target:"_blank",rel:"noopener noreferrer"}},[r._v("GICRYA"),e("OutboundLink")],1),r._v(".")]),r._v(" "),e("p",[r._v("Скорость и качество Razdel сопоставимы или выше, чем у других открытых решений для русского языка.")]),r._v(" "),e("p",[r._v("Решения для сегментации на токены")]),r._v(" "),e("p",[r._v("Ошибки на 1000 токенов")]),r._v(" "),e("p",[r._v("Время обработки, секунды")]),r._v(" "),e("p",[r._v("Regexp-baseline")]),r._v(" "),e("p",[r._v("19")]),r._v(" "),e("p",[e("strong",[r._v("0.5")])]),r._v(" "),e("p",[e("a",{attrs:{href:"https://github.com/natasha/naeval#spacy",target:"_blank",rel:"noopener noreferrer"}},[r._v("SpaCy"),e("OutboundLink")],1)]),r._v(" "),e("p",[r._v("17")]),r._v(" "),e("p",[r._v("5.4")]),r._v(" "),e("p",[e("a",{attrs:{href:"https://github.com/natasha/naeval#nltk",target:"_blank",rel:"noopener noreferrer"}},[r._v("NLTK"),e("OutboundLink")],1)]),r._v(" "),e("p",[r._v("130")]),r._v(" "),e("p",[r._v("3.1")]),r._v(" "),e("p",[e("a",{attrs:{href:"https://github.com/natasha/naeval#mystem",target:"_blank",rel:"noopener noreferrer"}},[r._v("MyStem"),e("OutboundLink")],1)]),r._v(" "),e("p",[r._v("19")]),r._v(" "),e("p",[r._v("4.5")]),r._v(" "),e("p",[e("a",{attrs:{href:"https://github.com/natasha/naeval#moses",target:"_blank",rel:"noopener noreferrer"}},[r._v("Moses"),e("OutboundLink")],1)]),r._v(" "),e("p",[e("strong",[r._v("11")])]),r._v(" "),e("p",[e("strong",[r._v("1.9")])]),r._v(" "),e("p",[e("a",{attrs:{href:"https://github.com/natasha/naeval#segtok",target:"_blank",rel:"noopener noreferrer"}},[r._v("SegTok"),e("OutboundLink")],1)]),r._v(" "),e("p",[r._v("12")]),r._v(" "),e("p",[r._v("2.1")]),r._v(" "),e("p",[e("a",{attrs:{href:"https://github.com/natasha/naeval#spacy_russian_tokenizer",target:"_blank",rel:"noopener noreferrer"}},[r._v("SpaCy Russian Tokenizer"),e("OutboundLink")],1)]),r._v(" "),e("p",[e("strong",[r._v("8")])]),r._v(" "),e("p",[r._v("46.4")]),r._v(" "),e("p",[e("a",{attrs:{href:"https://github.com/natasha/naeval#rutokenizer",target:"_blank",rel:"noopener noreferrer"}},[r._v("RuTokenizer"),e("OutboundLink")],1)]),r._v(" "),e("p",[r._v("15")]),r._v(" "),e("p",[e("strong",[r._v("1.0")])]),r._v(" "),e("p",[r._v("Razdel")]),r._v(" "),e("p",[e("strong",[r._v("7")])]),r._v(" "),e("p",[r._v("2.6")]),r._v(" "),e("p",[r._v("Решения для сегментации на предложения")]),r._v(" "),e("p",[r._v("Ошибки на 1000 предложений")]),r._v(" "),e("p",[r._v("Время обработки, секунды")]),r._v(" "),e("p",[r._v("Regexp-baseline")]),r._v(" "),e("p",[r._v("76")]),r._v(" "),e("p",[e("strong",[r._v("0.7")])]),r._v(" "),e("p",[e("a",{attrs:{href:"https://github.com/natasha/naeval#segtok",target:"_blank",rel:"noopener noreferrer"}},[r._v("SegTok"),e("OutboundLink")],1)]),r._v(" "),e("p",[r._v("381")]),r._v(" "),e("p",[r._v("10.8")]),r._v(" "),e("p",[e("a",{attrs:{href:"https://github.com/natasha/naeval#moses",target:"_blank",rel:"noopener noreferrer"}},[r._v("Moses"),e("OutboundLink")],1)]),r._v(" "),e("p",[r._v("166")]),r._v(" "),e("p",[e("strong",[r._v("7.0")])]),r._v(" "),e("p",[e("a",{attrs:{href:"https://github.com/natasha/naeval#nltk",target:"_blank",rel:"noopener noreferrer"}},[r._v("NLTK"),e("OutboundLink")],1)]),r._v(" "),e("p",[e("strong",[r._v("57")])]),r._v(" "),e("p",[r._v("7.1")]),r._v(" "),e("p",[e("a",{attrs:{href:"https://github.com/natasha/naeval#rusenttokenizer",target:"_blank",rel:"noopener noreferrer"}},[r._v("DeepPavlov"),e("OutboundLink")],1)]),r._v(" "),e("p",[e("strong",[r._v("41")])]),r._v(" "),e("p",[r._v("8.5")]),r._v(" "),e("p",[r._v("Razdel")]),r._v(" "),e("p",[e("strong",[r._v("43")])]),r._v(" "),e("p",[e("strong",[r._v("4.8")])]),r._v(" "),e("p",[r._v("_Число ошибок среднее по 4 датасетам: "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_ud_syntag",target:"_blank",rel:"noopener noreferrer"}},[r._v("SynTagRus"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_morphoru_corpora",target:"_blank",rel:"noopener noreferrer"}},[r._v("OpenCorpora"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_morphoru_gicrya",target:"_blank",rel:"noopener noreferrer"}},[r._v("GICRYA"),e("OutboundLink")],1),r._v(" and "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_morphoru_rnc",target:"_blank",rel:"noopener noreferrer"}},[r._v("RNC"),e("OutboundLink")],1),r._v(". Подробнее в "),e("a",{attrs:{href:"https://github.com/natasha/razdel#evaluation",target:"_blank",rel:"noopener noreferrer"}},[r._v("репозитории Razdel"),e("OutboundLink")],1),r._v("."),e("br"),r._v("\n_"),e("br"),r._v("\nЗачем вообще нужен Razdel, если похожее качество даёт baseline с регулярочкой и для русского языка есть куча готовых решений? На самом деле, Razdel это не просто токенизатор, а небольшой сегментационный движок на правилах. Сегментация базовая задача, часто встречается на практике. Например, есть судебный акт, нужно выделить в нём резолютивную часть и поделить её на параграфы. Естественно, готовые решения так не умеют. Как писать свои правила "),e("a",{attrs:{href:"https://github.com/natasha/razdel/blob/master/razdel/segmenters/sentenize.py",target:"_blank",rel:"noopener noreferrer"}},[r._v("читайте в исходниках"),e("OutboundLink")],1),r._v(". Дальше речь о том, как упороться и сделать на нашем движке топовое решение для токенов и предложений.")]),r._v(" "),e("h3",{attrs:{id:"в-чем-сложность"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#в-чем-сложность"}},[r._v("#")]),r._v(" В чём сложность?")]),r._v(" "),e("p",[r._v("В русском языке предложения обычно заканчиваются точкой, вопросительным или восклицательным знаком. Просто разделим текст регулярным выражением "),e("code",[r._v("[.?!]\\s+")]),r._v(". Такое решение даст 76 ошибок на 1000 предложений. Типы и примеры ошибок:")]),r._v(" "),e("p",[r._v("Cокращения"),e("br"),r._v("\n… любая площадка с аудиторией от 3 тыс.▒человек является блогером."),e("br"),r._v("\n… над ними с конца XVII в.▒стоял бей;"),e("br"),r._v("\n… в Камерном музыкальном театре им.▒Б.А. Покровского.")]),r._v(" "),e("p",[r._v("Инициалы"),e("br"),r._v("\nВ след за операми «Идоменей» В.А.▒Моцарта – Р.▒Штрауса …")]),r._v(" "),e("p",[r._v("Списки"),e("br"),r._v("\n2.▒думал будет в финское консульство красивая длинная очередь …"),e("br"),r._v("\nг.▒билеты на поезда российских железных дорог …")]),r._v(" "),e("p",[r._v("В конце предложения смайлик или типографское многоточие"),e("br"),r._v("\nКто предложит способ избавления от минусов — тому спасибо :)▒Посмотрел, призадумался…▒Вот это уже более неприятно, поскольку содержательность нарушится.")]),r._v(" "),e("p",[r._v("Цитаты, прямая речь, в конце предложения кавычка"),e("br"),r._v("\n— невесты у вас в городе есть?»▒«Кому и кобыла невеста»."),e("br"),r._v("\n«Как хорошо, что я не такой!»▒Сейчас при переводе сделал фрейдстскую ошибку:«идология».")]),r._v(" "),e("p",[r._v("Razdel учитывает эти нюансы, сокращает число ошибок c 76 до 43 на 1000 предложений.")]),r._v(" "),e("p",[r._v("С токенами аналогичная ситуация. Хорошее базовое решение даёт регулярное выражение "),e("code",[r._v("[а-яё-]+|[0-9]+|[^а-яё0-9 ]")]),r._v(", оно делает 19 ошибок на 1000 токенов. Примеры:")]),r._v(" "),e("p",[r._v("Дробные числа, сложная пунктуация"),e("br"),r._v("\n… В конце 1980▒-х — начале 1990▒-х"),e("br"),r._v("\n… БС-▒3 можно отметить слегка меньшую массу (3▒,▒6 т)"),e("br"),r._v("\n— да и умерла.▒.▒. Понял ли девку, сокол?▒!")]),r._v(" "),e("p",[r._v("Razdel сокращает число ошибок до 7 на 1000 токенов.")]),r._v(" "),e("h3",{attrs:{id:"принцип-работы"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#принцип-работы"}},[r._v("#")]),r._v(" Принцип работы")]),r._v(" "),e("p",[r._v("Система построена на правилах. Принцип сегментации на токены и предложения одинаковый.")]),r._v(" "),e("h4",{attrs:{id:"сбор-кандидатов"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#сбор-кандидатов"}},[r._v("#")]),r._v(" Сбор кандидатов")]),r._v(" "),e("p",[r._v("Находим в тексте всех кандидатов на конец предложения: точки, многоточия, скобки, кавычки.")]),r._v(" "),e("p",[r._v("6.▒Наиболее частый и при этом высоко оцененный вариант ответов «я рада»▒(13 высказываний, 25 баллов)▒– ситуации получения одобрения и поощрения.▒7.▒Примечательно, что в ответе «я знаю»▒оценен как максимально стереотипный, но лишь раз встречается ответ «я женщина»▒;▒присутствуют высказывания «один брак – это всё, что меня ждет в этой жизни»▒и «рано или поздно придется рожать»▒.▒Составители: В.▒П.▒Головин, Ф.▒В.▒Заничев, А.▒Л.▒Расторгуев, Р.▒В.▒Савко, И.▒И.▒Тучков.")]),r._v(" "),e("p",[r._v("Для токенов дробим текст на атомы. Внутри атома точно не проходит граница токена.")]),r._v(" "),e("p",[r._v("В▒конце▒1980▒-▒х▒-▒начале▒1990▒-▒х▒"),e("br"),r._v("\nБС▒-▒3▒можно▒отметить▒слегка▒меньшую▒массу▒(▒3▒,▒6▒т▒)▒"),e("br"),r._v("\n▒—▒да▒и▒умерла▒.▒.▒.▒Понял▒ли▒девку▒,▒сокол▒?▒!")]),r._v(" "),e("h4",{attrs:{id:"объединение"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#объединение"}},[r._v("#")]),r._v(" Объединение")]),r._v(" "),e("p",[r._v("Последовательно обходим кандидатов на разделение, убираем лишние. Используем список эвристик.")]),r._v(" "),e("p",[r._v("Элемент списка. Разделитель — точка или скобка, слева число или буква"),e("br"),r._v("\n6.▒Наиболее частый и при этом высоко оцененный вариант ответов «я рада» (13 высказываний, 25 баллов) – ситуации получения одобрения и поощрения. 7.▒Примечательно, что в ответе «я знаю» …")]),r._v(" "),e("p",[r._v("Инициалы. Разделитель — точка, слева одна заглавная буква"),e("br"),r._v("\n… Составители: В.▒П.▒Головин, Ф.▒В.▒Заничев, А.▒Л.▒Расторгуев, Р.▒В.▒Савко, И.▒И.▒Тучков.")]),r._v(" "),e("p",[r._v("Справа от разделителя нет пробела"),e("br"),r._v("\n… но лишь раз встречается ответ «я женщина»▒; присутствуют высказывания «один брак – это всё, что меня ждет в этой жизни» и «рано или поздно придется рожать»▒.")]),r._v(" "),e("p",[r._v("Перед закрывающей кавычкой или скобкой нет знака конца предложения, это не цитата и не прямая речь"),e("br"),r._v("\n6. Наиболее частый и при этом высоко оцененный вариант ответов «я рада»▒(13 высказываний, 25 баллов)▒– ситуации получения одобрения и поощрения. … «один брак – это всё, что меня ждет в этой жизни»▒и «рано или поздно придется рожать».")]),r._v(" "),e("p",[r._v("В результате остаётся два разделителя, считаем их концами предложений.")]),r._v(" "),e("ol",{attrs:{start:"6"}},[e("li",[r._v("Наиболее частый и при этом высоко оцененный вариант ответов «я рада» (13 высказываний, 25 баллов) – ситуации получения одобрения и поощрения.▒7. Примечательно, что в ответе «я знаю» оценен как максимально стереотипный, но лишь раз встречается ответ «я женщина»; присутствуют высказывания «один брак – это всё, что меня ждет в этой жизни» и «рано или поздно придется рожать».▒Составители: В. П. Головин, Ф. В. Заничев, А. Л. Расторгуев, Р. В. Савко, И. И. Тучков.")])]),r._v(" "),e("p",[r._v("Для токенов процедура аналогичная, правила другие.")]),r._v(" "),e("p",[r._v("Дробь или рациональное число"),e("br"),r._v("\n… (3▒,▒6 т) …")]),r._v(" "),e("p",[r._v("Сложная пунктуация"),e("br"),r._v("\n— да и умерла.▒.▒. Понял ли девку, сокол?▒!")]),r._v(" "),e("p",[r._v("Вокруг дефиса нет пробелов, это не начало прямой речи"),e("br"),r._v("\nВ конце 1980▒-▒х — начале 1990▒-▒х"),e("br"),r._v("\nБС▒-▒3 можно отметить …")]),r._v(" "),e("p",[r._v("Всё что осталось считаем границами токенов.")]),r._v(" "),e("p",[r._v("В▒конце▒1980-х▒-▒начале▒1990-х▒"),e("br"),r._v("\nБС-3▒можно▒отметить▒слегка▒меньшую▒массу▒(▒3,6▒т▒)▒"),e("br"),r._v("\n▒—▒да▒и▒умерла▒...▒Понял▒ли▒девку▒,▒сокол▒?!")]),r._v(" "),e("h3",{attrs:{id:"ограничения"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#ограничения"}},[r._v("#")]),r._v(" Ограничения")]),r._v(" "),e("p",[r._v("Правила в Razdel оптимизированы для аккуратно написанных текстов с правильной пунктуацией. Решение хорошо работает с новостными статьями, художественными текстами. На постах из социальных сетей, расшифровках телефонных разговоров качество ниже. Если между предложениями нет пробела или в конце нет точки или предложение начинается с маленькой буквы, Razdel сделает ошибку.")]),r._v(" "),e("p",[r._v("Как писать правила под свои задачи "),e("a",{attrs:{href:"https://github.com/natasha/razdel/blob/master/razdel/segmenters/sentenize.py",target:"_blank",rel:"noopener noreferrer"}},[r._v("читайте в исходниках"),e("OutboundLink")],1),r._v(", в документации эта тема пока не раскрыта.")]),r._v(" "),e("h2",{attrs:{id:"slovnet-deep-learning-моделирование-для-обработки-естественного-русского-языка"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#slovnet-deep-learning-моделирование-для-обработки-естественного-русского-языка"}},[r._v("#")]),r._v(" Slovnet — deep learning моделирование для обработки естественного русского языка")]),r._v(" "),e("p",[e("img",{attrs:{src:"https://habrastorage.org/r/w1560/webt/jq/k-/fv/jqk-fvsz6fbzbxakuhekbj7b1xw.png",alt:""}})]),r._v(" "),e("p",[r._v("В проекте Natasha "),e("a",{attrs:{href:"https://github.com/natasha/slovnet",target:"_blank",rel:"noopener noreferrer"}},[r._v("Slovnet"),e("OutboundLink")],1),r._v(" занимается обучением и инференсом современных моделей для русскоязычного NLP. В библиотеке собраны качественные компактные модели для извлечения именованных сущностей, разбора морфологии и синтаксиса. "),e("a",{attrs:{href:"https://github.com/natasha/slovnet#evaluation",target:"_blank",rel:"noopener noreferrer"}},[r._v("Качество на всех задачах сравнимо или превосходит"),e("OutboundLink")],1),r._v(" другие отрытые решения для русского языка на текстах новостной тематики. "),e("a",{attrs:{href:"https://github.com/natasha/slovnet#install",target:"_blank",rel:"noopener noreferrer"}},[r._v("Инструкция по установке"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/slovnet#usage",target:"_blank",rel:"noopener noreferrer"}},[r._v("примеры использования"),e("OutboundLink")],1),r._v(" — в "),e("a",{attrs:{href:"https://github.com/natasha/slovnet",target:"_blank",rel:"noopener noreferrer"}},[r._v("репозитории Slovnet"),e("OutboundLink")],1),r._v(". Подробно разберёмся, как устроено решение для задачи NER, для морфологии и синтаксиса всё по аналогии.")]),r._v(" "),e("p",[r._v("В конце 2018 года после "),e("a",{attrs:{href:"https://arxiv.org/abs/1810.04805",target:"_blank",rel:"noopener noreferrer"}},[r._v("статьи от Google про BERT"),e("OutboundLink")],1),r._v(" в англоязычном NLP случился большой прогресс. В 2019 ребята из "),e("a",{attrs:{href:"https://deeppavlov.ai/",target:"_blank",rel:"noopener noreferrer"}},[r._v("проекта DeepPavlov"),e("OutboundLink")],1),r._v(" адаптировали мультиязычный BERT для русского, появился "),e("a",{attrs:{href:"http://docs.deeppavlov.ai/en/master/features/models/bert.html",target:"_blank",rel:"noopener noreferrer"}},[r._v("RuBERT"),e("OutboundLink")],1),r._v(". Поверх обучили "),e("a",{attrs:{href:"http://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf",target:"_blank",rel:"noopener noreferrer"}},[r._v("CRF-голову"),e("OutboundLink")],1),r._v(", получился "),e("a",{attrs:{href:"https://www.youtube.com/watch?v=eKTA8i8s-zs",target:"_blank",rel:"noopener noreferrer"}},[r._v("DeepPavlov BERT NER"),e("OutboundLink")],1),r._v(" — SOTA для русского языка. У модели великолепное качество, в 2 раза меньше ошибок, чем у ближайшего преследователя "),e("a",{attrs:{href:"https://github.com/deepmipt/ner",target:"_blank",rel:"noopener noreferrer"}},[r._v("DeepPavlov NER"),e("OutboundLink")],1),r._v(", но размер и производительность пугают: 6ГБ — потребление GPU RAM, 2ГБ — размер модели, 13 статей в секунду — производительность на хорошей GPU.")]),r._v(" "),e("p",[r._v("В 2020 году в проекте Natasha нам удалось вплотную приблизится по качеству к DeepPavlov BERT NER, размер модели получился в 75 раз меньше (27МБ), потребление памяти в 30 раз меньше (205МБ), скорость в 2 раза больше на CPU (25 статей в секунду).")]),r._v(" "),e("p",[r._v("Natasha, Slovnet NER")]),r._v(" "),e("p",[r._v("DeepPavlov BERT NER")]),r._v(" "),e("p",[r._v("PER/LOC/ORG F1 по токенам, среднее по Collection5, factRuEval-2016, BSNLP-2019, Gareev")]),r._v(" "),e("p",[r._v("0.97/0.91/0.85")]),r._v(" "),e("p",[r._v("0.98/0.92/0.86")]),r._v(" "),e("p",[r._v("Размер модели")]),r._v(" "),e("p",[r._v("27МБ")]),r._v(" "),e("p",[r._v("2ГБ")]),r._v(" "),e("p",[r._v("Потребление памяти")]),r._v(" "),e("p",[r._v("205МБ")]),r._v(" "),e("p",[r._v("6ГБ (GPU)")]),r._v(" "),e("p",[r._v("Производительность, новостных статей в секунду (1 статья ≈ 1КБ)")]),r._v(" "),e("p",[r._v("25 на CPU (Core i5)")]),r._v(" "),e("p",[r._v("13 на GPU (RTX 2080 Ti), 1 на CPU")]),r._v(" "),e("p",[r._v("Время инициализации, секунд")]),r._v(" "),e("p",[r._v("1")]),r._v(" "),e("p",[r._v("35")]),r._v(" "),e("p",[r._v("Библиотека поддерживает")]),r._v(" "),e("p",[r._v("Python 3.5+, PyPy3")]),r._v(" "),e("p",[r._v("Python 3.6+")]),r._v(" "),e("p",[r._v("Зависимости")]),r._v(" "),e("p",[r._v("NumPy")]),r._v(" "),e("p",[r._v("TensorFlow")]),r._v(" "),e("p",[e("em",[r._v("Качество Slovnet NER на 1 процентный пункт ниже, чем у SOTA DeepPavlov BERT NER, размер модели в 75 раз меньше, потребление памяти в 30 раз меньше, скорость в 2 раза больше на CPU. Сравнение со SpaCy, PullEnti и другими решениями для русскоязычного NER в "),e("a",{attrs:{href:"https://github.com/natasha/slovnet#ner-1",target:"_blank",rel:"noopener noreferrer"}},[r._v("репозитории Slovnet"),e("OutboundLink")],1),r._v(".")])]),r._v(" "),e("p",[r._v("Как получить такой результат? Короткий рецепт:")]),r._v(" "),e("blockquote",[e("p",[e("a",{attrs:{href:"https://github.com/natasha/slovnet#ner",target:"_blank",rel:"noopener noreferrer"}},[r._v("Slovnet NER"),e("OutboundLink")],1),r._v(" = "),e("a",{attrs:{href:"https://github.com/natasha/slovnet/blob/master/scripts/02_bert_ner/main.ipynb",target:"_blank",rel:"noopener noreferrer"}},[r._v("Slovnet BERT NER"),e("OutboundLink")],1),r._v(" — аналог DeepPavlov BERT NER + дистилляция через синтетическую разметку ("),e("a",{attrs:{href:"https://github.com/natasha/nerus",target:"_blank",rel:"noopener noreferrer"}},[r._v("Nerus"),e("OutboundLink")],1),r._v(") в WordCNN-CRF c квантованными эмбеддингами ("),e("a",{attrs:{href:"https://github.com/natasha/navec",target:"_blank",rel:"noopener noreferrer"}},[r._v("Navec"),e("OutboundLink")],1),r._v(") + движок для инференса на NumPy.")])]),r._v(" "),e("p",[r._v("Теперь по порядку. План такой: обучим тяжёлую модель c BERT-архитектурой на небольшом вручную аннотированном датасете. Разметим ей корпус новостей, получится большой грязный синтетический тренировочный датасет. Обучим на нём компактную примитивную модель. Этот процесс называется дистилляцией: тяжёлая модель — учитель, компактная — ученик. Рассчитываем, что BERT-архитектура избыточна для задачи NER, компактная модель несильно проиграет по качеству тяжёлой.")]),r._v(" "),e("h3",{attrs:{id:"модель-учитель"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#модель-учитель"}},[r._v("#")]),r._v(" Модель-учитель")]),r._v(" "),e("p",[r._v("DeepPavlov BERT NER состоит из RuBERT-энкодера и CRF-головы. Наша тяжёлая модель-учитель повторяет эту архитектуру с небольшими улучшениями.")]),r._v(" "),e("p",[r._v("Все бенчмарки измеряют качество NER на текстах новостей. Дообучим RuBERT на новостях. В репозитории "),e("a",{attrs:{href:"https://github.com/natasha/corus",target:"_blank",rel:"noopener noreferrer"}},[r._v("Corus"),e("OutboundLink")],1),r._v(" собраны ссылки на публичные русскоязычные новостные корпуса, в сумме 12 ГБ текстов. Используем техники из "),e("a",{attrs:{href:"https://arxiv.org/abs/1907.11692",target:"_blank",rel:"noopener noreferrer"}},[r._v("статьи от Facebook про RoBERTa"),e("OutboundLink")],1),r._v(": большие агрегированные батчи, динамическая маска, отказ от предсказания следующего предложения (NSP). RuBERT использует огромный словарь на 120 000 сабтокенов — наследие мультиязычного BERT от Google. Сократим размер до 50 000 самых частотных для новостей, покрытие уменьшится на 5%. Получим "),e("a",{attrs:{href:"https://github.com/natasha/slovnet/blob/master/scripts/01_bert_news/main.ipynb",target:"_blank",rel:"noopener noreferrer"}},[r._v("NewsRuBERT"),e("OutboundLink")],1),r._v(", модель предсказывает замаскированные сабтокены в новостях на 5 процентных пунктов лучше RuBERT (63% в топ-1).")]),r._v(" "),e("p",[r._v("Обучим NewsRuBERT-энкодер и CRF-голову на 1000 статей из "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_ne5",target:"_blank",rel:"noopener noreferrer"}},[r._v("Collection5"),e("OutboundLink")],1),r._v(". Получим "),e("a",{attrs:{href:"https://github.com/natasha/slovnet/blob/master/scripts/02_bert_ner/main.ipynb",target:"_blank",rel:"noopener noreferrer"}},[r._v("Slovnet BERT NER"),e("OutboundLink")],1),r._v(", качество на 0.5 процентных пункта лучше, чем у DeepPavlov BERT NER, размер модели меньше в 4 раза (473МБ), работает в 3 раза быстрее (40 статей в секунду).")]),r._v(" "),e("blockquote",[e("p",[r._v("NewsRuBERT = RuBERT + 12ГБ новостей + техники из RoBERTa + 50K-словарь."),e("br"),r._v("\nSlovnet BERT NER (аналог DeepPavlov BERT NER) = NewsRuBERT + CRF-голова + Collection5.")])]),r._v(" "),e("p",[r._v("Сейчас, для обучения моделей с BERT-like архитектурой, принято использовать "),e("a",{attrs:{href:"https://huggingface.co/transformers/",target:"_blank",rel:"noopener noreferrer"}},[r._v("Transformers"),e("OutboundLink")],1),r._v(" от Hugging Face. Transformers — это 100 000 строк кода на Python. Когда взорвётся loss или на инференсе мусор, тяжело разобраться, что пошло не так. Ладно, там много кода дублируется. Пускай мы тренируем RoBERTa, довольно быстро локализуем проблему до ~3000 строк кода, но это тоже немало. С современным PyTorch, библиотека Transformers не так актуальна. С "),e("code",[r._v("[torch.nn.TransformerEncoderLayer](https://pytorch.org/docs/stable/nn.html#transformer-layers)")]),r._v(" код RoBERTa-like модели занимает 100 строк:")]),r._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[r._v("class BERTEmbedding(nn.Module):\n    def __init__(self, vocab_size, seq_len, emb_dim, dropout=0.1, norm_eps=1e-12):\n        super(BERTEmbedding, self).__init__()\n        self.word = nn.Embedding(vocab_size, emb_dim)\n        self.position = nn.Embedding(seq_len, emb_dim)\n        self.norm = nn.LayerNorm(emb_dim, eps=norm_eps)\n        self.drop = nn.Dropout(dropout)\n\n    def forward(self, input):\n        batch_size, seq_len = input.shape\n        position = torch.arange(seq_len).expand_as(input).to(input.device)\n\n        emb = self.word(input) + self.position(position)\n        emb = self.norm(emb)\n        return self.drop(emb)\n      \n\ndef BERTLayer(emb_dim, heads_num, hidden_dim, dropout=0.1, norm_eps=1e-12):\n    layer = nn.TransformerEncoderLayer(\n        d_model=emb_dim,\n        nhead=heads_num,\n        dim_feedforward=hidden_dim,\n        dropout=dropout,\n        activation='gelu'\n    )\n    layer.norm1.eps = norm_eps\n    layer.norm2.eps = norm_eps\n    return layer\n\n\nclass BERTEncoder(nn.Module):\n    def __init__(self, layers_num, emb_dim, heads_num, hidden_dim,\n                 dropout=0.1, norm_eps=1e-12):\n        super(BERTEncoder, self).__init__()\n        self.layers = nn.ModuleList([\n            BERTLayer(\n                emb_dim, heads_num, hidden_dim,\n                dropout, norm_eps\n            )\n            for _ in range(layers_num)\n        ])\n\n    def forward(self, input, pad_mask=None):\n        input = input.transpose(0, 1)  # torch expects seq x batch x emb\n        for layer in self.layers:\n            input = layer(input, src_key_padding_mask=pad_mask)\n        return input.transpose(0, 1)  # restore\n          \n          \nclass BERTMLMHead(nn.Module):\n    def __init__(self, emb_dim, vocab_size, norm_eps=1e-12):\n        super(BERTMLMHead, self).__init__()\n        self.linear1 = nn.Linear(emb_dim, emb_dim)\n        self.norm = nn.LayerNorm(emb_dim, eps=norm_eps)\n        self.linear2 = nn.Linear(emb_dim, vocab_size)\n\n    def forward(self, input):\n        x = self.linear1(input)\n        x = F.gelu(x)\n        x = self.norm(x)\n        return self.linear2(x)\n\n\nclass BERTMLM(nn.Module):\n    def __init__(self, emb, encoder, head):\n        super(BERTMLM, self).__init__()\n        self.emb = emb\n        self.encoder = encoder\n        self.head = head\n\n    def forward(self, input):\n        x = self.emb(input)\n        x = self.encoder(x)\n        return self.head(x)\n")])])]),e("p",[r._v("Это не прототип, код скопирован из "),e("a",{attrs:{href:"https://github.com/natasha/slovnet/blob/master/slovnet/model/bert.py",target:"_blank",rel:"noopener noreferrer"}},[r._v("репозитория Slovnet"),e("OutboundLink")],1),r._v(". Transformers полезно читать, они делают большую работу, набивают код для статей с Arxiv, часто исходники на Python понятнее, чем объяснение в научной статье.")]),r._v(" "),e("h3",{attrs:{id:"синтетическии-датасет"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#синтетическии-датасет"}},[r._v("#")]),r._v(" Синтетический датасет")]),r._v(" "),e("p",[r._v("Разметим 700 000 статей из "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_lenta",target:"_blank",rel:"noopener noreferrer"}},[r._v("корпуса Lenta.ru"),e("OutboundLink")],1),r._v(" тяжёлой моделью. Получим огромный синтетический обучающий датасет. Архив доступен в репозитории "),e("a",{attrs:{href:"https://github.com/natasha/nerus",target:"_blank",rel:"noopener noreferrer"}},[r._v("Nerus"),e("OutboundLink")],1),r._v(" проекта Natasha. Разметка очень качественная, оценки F1 по токенам: PER — 99.7%, LOC — 98.6%, ORG — 97.2%. Редкие примеры ошибок:")]),r._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[r._v("Выборы Верховного совета Аджарской автономной республики\n       ORG────────────── LOC────────────────────────────\nназначены в соответствии с 241-ой статьей и 4-м пунктом 10-й\nстатьи Конституционного закона Грузии <О статусе Аджарской\n                               LOC───            LOC──────\nавтономной республики>.\n───────────~~~~~~~~~~~\n\nСледственное управление при прокуратуре требует наказать\nORG────────────────────~~~~~~~~~~~~~~~~\nпремьера Якутии.\nLOC───\n\nНачальник полигона <Игумново> в Нижегородской области осужден\n                    ~~~~~~~~    LOC──────────────────\nза загрязнение атмосферы и грунтовых вод.\n\nСтраны Азии и Африки поддержали позицию России в конфликте с\n       ~~~~   ~~~~~~                    LOC───\nГрузией.\nLOC────\n\nУ Владимира Стржалковского появится помощник - специалист по\n  PER─────────────────────\nпроведению сделок M&A.\n                  ~~~\n\nПостоянный Секретариат ОССНАА в Каире в пятницу заявил: Когда\n           ~~~~~~~~~~~~ORG───   LOC──\nСаакашвили стал президентом Грузии, он проявил стремление\nPER───────                  LOC───\nвступить в НАТО, Европейский Союз и установить более близкие\n           ORG─  LOC─────────────\nотношения с США.\n            LOC\n")])])]),e("h3",{attrs:{id:"модель-ученик"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#модель-ученик"}},[r._v("#")]),r._v(" Модель-ученик")]),r._v(" "),e("p",[r._v("С выбором архитектуры тяжёлой модели-учителя проблем не возникло, вариант один — трансформеры. С компактной моделью-учеником сложнее, вариантов много. С 2013 до 2018 год с появления word2vec до статьи про BERT, человечество придумало кучу нейросетевых архитектур для решения задачи NER. У всех общая схема:")]),r._v(" "),e("p",[e("img",{attrs:{src:"https://habrastorage.org/r/w1560/webt/uu/gm/_u/uugm_uuuruwcdrs8ycekgfpwufm.png",alt:""}}),e("br"),r._v(" "),e("em",[r._v("Схема нейросетевых архитектур для задачи NER: энкодер токенов, энкодер контекста, декодер тегов. Расшифровка сокращений в "),e("a",{attrs:{href:"https://arxiv.org/pdf/1806.05626.pdf",target:"_blank",rel:"noopener noreferrer"}},[r._v("обзорной статье Yang (2018)"),e("OutboundLink")],1),r._v(".")])]),r._v(" "),e("p",[r._v("Комбинаций архитектур много. Какую выбрать? Например, (CharCNN + Embedding)-WordBiLSTM-CRF — схема модели из "),e("a",{attrs:{href:"https://arxiv.org/pdf/1709.09686.pdf",target:"_blank",rel:"noopener noreferrer"}},[r._v("статьи про DeepPavlov NER"),e("OutboundLink")],1),r._v(", SOTA для русского языка до 2019 года.")]),r._v(" "),e("p",[r._v('Варианты с CharCNN, CharRNN пропускаем, запускать маленькую нейронную сеть по символам на каждом токене — не наш путь, слишком медленно. WordRNN тоже хотелось бы избежать, решение должно работать на CPU, перемножать матрицы на каждом токене медленно. Для NER выбор между Linear и CRF условный. Мы используем BIO-кодировку, порядок тегов важен. Приходится терпеть жуткие тормоза, использовать CRF. Остаётся один вариант — Embedding-WordCNN-CRF. Такая модель не учитывает регистр токенов, для NER это важно, «надежда» — просто слово, «Надежда» — возможно, имя. Добавим ShapeEmbedding — эмбеддинг с очертаниями токенов, например: «NER» — EN_XX, «Вайнович» — RU_Xx, "!" — PUNCT_!, «и» — RU_x, «5.1» — NUM, «Нью-Йорк» — RU_Xx-Xx. Схема Slovnet NER — (WordEmbedding + ShapeEmbedding)-WordCNN-CRF.')]),r._v(" "),e("h3",{attrs:{id:"дистилляция"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#дистилляция"}},[r._v("#")]),r._v(" Дистилляция")]),r._v(" "),e("p",[r._v("Обучим Slovnet NER на огромном синтетическом датасете. Сравним результат с тяжёлой моделью-учителем Slovnet BERT NER. Качество считаем и усредняем по размеченным вручную Collection5, Gareev, factRuEval-2016, BSNLP-2019. Размер обучающей выборки очень важен: на 250 новостных статьях (размер factRuEval-2016) средний по PER, LOC, LOG F1 — 0.64, на 1000 (аналог Collection5) — 0.81, на всём датасете — 0.91, качество Slovnet BERT NER — 0.92.")]),r._v(" "),e("p",[e("img",{attrs:{src:"https://habrastorage.org/r/w1560/webt/er/mf/p_/ermfp_dwuzfmo94zoe3wjdyg-ce.png",alt:""}}),e("br"),r._v(" "),e("em",[r._v("Качество Slovnet NER, зависимость от числа синтетических обучающих примеров. Серая линия — качество Slovnet BERT NER. Slovnet NER не видит размеченных вручную примеров, обучается только на синтетических данных.")])]),r._v(" "),e("p",[r._v("Примитивная модель-ученик на 1 процентный пункт хуже тяжёлой модели-учителя. Это замечательный результат. Напрашивается универсальный рецепт:")]),r._v(" "),e("blockquote",[e("p",[r._v("Вручную размечаем немного данных. Обучаем тяжёлый трансформер. Генерируем много синтетических данных. Обучаем простую модель на большой выборке. Получаем качество трансформера, размер и производительность простой модели.")])]),r._v(" "),e("p",[r._v("В библиотеке Slovnet есть ещё две модели обученные по этому рецепту: "),e("a",{attrs:{href:"https://github.com/natasha/slovnet#morphology",target:"_blank",rel:"noopener noreferrer"}},[r._v("Slovnet Morph"),e("OutboundLink")],1),r._v(" — морфологический теггер, "),e("a",{attrs:{href:"https://github.com/natasha/slovnet#syntax",target:"_blank",rel:"noopener noreferrer"}},[r._v("Slovnet Syntax"),e("OutboundLink")],1),r._v(" — синтаксический парсер. Slovnet Morph отстаёт от тяжёлой модели-учителя "),e("a",{attrs:{href:"https://github.com/natasha/slovnet#morphology-1",target:"_blank",rel:"noopener noreferrer"}},[r._v("на 2 процентных пункта"),e("OutboundLink")],1),r._v(", Slovnet Syntax — "),e("a",{attrs:{href:"https://github.com/natasha/slovnet#syntax-1",target:"_blank",rel:"noopener noreferrer"}},[r._v("на 5"),e("OutboundLink")],1),r._v(". У обеих моделей качество и производительность выше существующих решений для русского на новостных статьях.")]),r._v(" "),e("h3",{attrs:{id:"квантизация"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#квантизация"}},[r._v("#")]),r._v(" Квантизация")]),r._v(" "),e("p",[r._v("Размер Slovnet NER — 289МБ. 287МБ занимает таблица с эмбеддингами. Модель использует большой словарь на 250 000 строк, он покрывает 98% слов в новостных текстах. Используем "),e("a",{attrs:{href:"http://mccormickml.com/2017/10/13/product-quantizer-tutorial-part-1/",target:"_blank",rel:"noopener noreferrer"}},[r._v("квантизацию"),e("OutboundLink")],1),r._v(", заменим 300-мерные float-вектора на 100-мерные 8-битные. Размер модели уменьшится в 10 раз (27МБ), качество не изменится. "),e("a",{attrs:{href:"https://github.com/natasha/navec",target:"_blank",rel:"noopener noreferrer"}},[r._v("Библиотека Navec"),e("OutboundLink")],1),r._v(" — часть проекта Natasha, коллекция квантованных предобученных эмбеддингов. "),e("a",{attrs:{href:"https://github.com/natasha/navec#hudlit",target:"_blank",rel:"noopener noreferrer"}},[r._v("Веса обученные на художественной литературе"),e("OutboundLink")],1),r._v(" занимают 50МБ, обходят по "),e("a",{attrs:{href:"https://github.com/natasha/navec/#evaluation",target:"_blank",rel:"noopener noreferrer"}},[r._v("синтетическим оценкам"),e("OutboundLink")],1),r._v(" все "),e("a",{attrs:{href:"https://rusvectores.org/ru/models/",target:"_blank",rel:"noopener noreferrer"}},[r._v("статические модели RusVectores"),e("OutboundLink")],1),r._v(".")]),r._v(" "),e("h3",{attrs:{id:"инференс"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#инференс"}},[r._v("#")]),r._v(" Инференс")]),r._v(" "),e("p",[r._v("Slovnet NER использует PyTorch для обучения. Пакет PyTorch весит 700МБ, не хочется тянуть его в продакшн для инференса. Ещё PyTorch "),e("a",{attrs:{href:"https://github.com/pytorch/pytorch/issues/17835",target:"_blank",rel:"noopener noreferrer"}},[r._v("не работает с интерпретатором PyPy"),e("OutboundLink")],1),r._v(". Slovnet используется в связке с "),e("a",{attrs:{href:"https://github.com/natasha/yargy",target:"_blank",rel:"noopener noreferrer"}},[r._v("Yargy-парсером"),e("OutboundLink")],1),r._v(" аналогом "),e("a",{attrs:{href:"https://yandex.ru/dev/tomita/",target:"_blank",rel:"noopener noreferrer"}},[r._v("яндексового Tomita-парсера"),e("OutboundLink")],1),r._v(". С PyPy Yargy работает в 2-10 раз быстрее, зависит от сложности грамматик. Не хочется терять скорость из-за зависимости от PyTorch.")]),r._v(" "),e("p",[r._v("Стандартное решение — использовать "),e("a",{attrs:{href:"https://pytorch.org/docs/stable/jit.html",target:"_blank",rel:"noopener noreferrer"}},[r._v("TorchScript"),e("OutboundLink")],1),r._v(" или "),e("a",{attrs:{href:"https://pytorch.org/docs/stable/onnx.html",target:"_blank",rel:"noopener noreferrer"}},[r._v("сконвертировать модель в ONNX"),e("OutboundLink")],1),r._v(", инференс делать в "),e("a",{attrs:{href:"https://github.com/microsoft/onnxruntime",target:"_blank",rel:"noopener noreferrer"}},[r._v("ONNXRuntime"),e("OutboundLink")],1),r._v(". Slovnet NER использует нестандартные блоки: квантованные эмбеддинги, CRF-декодер. TorchScript и ONNXRuntime не поддерживают PyPy.")]),r._v(" "),e("p",[r._v("Slovnet NER — простая модель, "),e("a",{attrs:{href:"https://github.com/natasha/slovnet/blob/master/slovnet/exec/model.py",target:"_blank",rel:"noopener noreferrer"}},[r._v("вручную реализуем все блоки на NumPy"),e("OutboundLink")],1),r._v(", используем веса, посчитанные PyTorch. Применим немного NumPy-магии, аккуратно реализуем "),e("a",{attrs:{href:"https://github.com/natasha/slovnet/blob/master/slovnet/exec/model.py#L82-L112",target:"_blank",rel:"noopener noreferrer"}},[r._v("блок CNN"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/slovnet/blob/master/slovnet/exec/model.py#L154-L184",target:"_blank",rel:"noopener noreferrer"}},[r._v("CRF-декодер"),e("OutboundLink")],1),r._v(", распаковка квантованного эмбеддинга "),e("a",{attrs:{href:"https://github.com/natasha/slovnet/blob/master/slovnet/exec/model.py#L229-L234",target:"_blank",rel:"noopener noreferrer"}},[r._v("занимает 5 строк"),e("OutboundLink")],1),r._v(". Скорость инференса на CPU такая же как с ONNXRuntime и PyTorch, 25 новостных статей в секунду на Core i5.")]),r._v(" "),e("p",[r._v("Техника работает на более сложных моделях: Slovnet Morph и Slovnet Syntax тоже реализованы на NumPy. Slovnet NER, Morph и Syntax используют общую таблицу эмбеддингов. Вынесем веса в отдельный файл, таблица не дублируется в памяти и на диске:")]),r._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[r._v(">>> navec = Navec.load('navec_news_v1_1B.tar')  # 25MB\n>>> morph = Morph.load('slovnet_morph_news_v1.tar')  # 2MB\n>>> syntax = Syntax.load('slovnet_syntax_news_v1.tar')  # 3MB\n>>> ner = NER.load('slovnet_ner_news_v1.tar')  # 2MB\n\n# 25 + 2 + 3 + 2 вместо 25+2 + 25+3 + 25+2\n>>> morph.navec(navec)\n>>> syntax.navec(navec)\n>>> ner.navec(navec)\n")])])]),e("h3",{attrs:{id:"ограничения-2"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#ограничения-2"}},[r._v("#")]),r._v(" Ограничения")]),r._v(" "),e("p",[r._v("Natasha извлекает стандартные сущности: имена, названия топонимов и организаций. Решение показывает хорошее качество на новостях. Как работать с другими сущностями и типами текстов? Нужно обучить новую модель. Сделать это непросто. За компактный размер и скорость работы мы платим сложностью подготовки модели. "),e("a",{attrs:{href:"https://github.com/natasha/slovnet/blob/master/scripts/02_bert_ner/main.ipynb",target:"_blank",rel:"noopener noreferrer"}},[r._v("Скрипт-ноутбук для подготовки тяжёлой модели учителя"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/slovnet/blob/master/scripts/05_ner/main.ipynb",target:"_blank",rel:"noopener noreferrer"}},[r._v("скрипт-ноутбук для модели-ученика"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/navec#development",target:"_blank",rel:"noopener noreferrer"}},[r._v("инструкции по подготовке квантованных эмбеддингов"),e("OutboundLink")],1),r._v(".")]),r._v(" "),e("h2",{attrs:{id:"navec-компактные-эмбеддинги-для-русского-языка"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#navec-компактные-эмбеддинги-для-русского-языка"}},[r._v("#")]),r._v(" Navec — компактные эмбеддинги для русского языка")]),r._v(" "),e("p",[e("img",{attrs:{src:"https://habrastorage.org/r/w1560/webt/to/gc/c8/togcc85ufq3b9rmshkwxvmqdnqo.png",alt:""}})]),r._v(" "),e("p",[r._v("С компактными моделями удобно работать. Они быстро запускаются, используют мало памяти, на один инстанст помещается больше параллельных процессов.")]),r._v(" "),e("p",[r._v("В NLP 80-90% весов модели приходится на таблицу с эмбеддингами. "),e("a",{attrs:{href:"https://github.com/natasha/navec",target:"_blank",rel:"noopener noreferrer"}},[r._v("Библиотека Navec"),e("OutboundLink")],1),r._v(" — часть проекта Natasha, коллекция предобученных эмбеддингов для русского языка. По intrinsic-метрикам качества они чуть-чуть не дотягивают по топовых решений "),e("a",{attrs:{href:"https://rusvectores.org/",target:"_blank",rel:"noopener noreferrer"}},[r._v("RusVectores"),e("OutboundLink")],1),r._v(", зато размер архива с весами в 5-6 раз меньше (51МБ), словарь в 2-3 раза больше (500К слов).")]),r._v(" "),e("p",[r._v("Качество*")]),r._v(" "),e("p",[r._v("Размер модели, МБ")]),r._v(" "),e("p",[r._v("Размер словаря, ×103")]),r._v(" "),e("p",[r._v("Navec")]),r._v(" "),e("p",[r._v("0.719")]),r._v(" "),e("p",[r._v("50.6")]),r._v(" "),e("p",[r._v("500")]),r._v(" "),e("p",[r._v("RusVectores")]),r._v(" "),e("p",[r._v("0.638–0.726")]),r._v(" "),e("p",[r._v("220.6–290.7")]),r._v(" "),e("p",[r._v("189–249")]),r._v(" "),e("p",[e("em",[r._v("* Качество на задаче определения семантической близости. Усреднённая оценка по шести датасетам: "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_simlex",target:"_blank",rel:"noopener noreferrer"}},[r._v("SimLex965"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_toloka_lrwc",target:"_blank",rel:"noopener noreferrer"}},[r._v("LRWC"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_russe_hj",target:"_blank",rel:"noopener noreferrer"}},[r._v("HJ"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_russe_rt",target:"_blank",rel:"noopener noreferrer"}},[r._v("RT"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_russe_ae",target:"_blank",rel:"noopener noreferrer"}},[r._v("AE"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_russe_ae",target:"_blank",rel:"noopener noreferrer"}},[r._v("AE2"),e("OutboundLink")],1)])]),r._v(" "),e("p",[r._v("Речь пойдёт про "),e("a",{attrs:{href:"https://lena-voita.github.io/nlp_course/word_embeddings.html",target:"_blank",rel:"noopener noreferrer"}},[r._v("старые добрые пословные эмбеддинги"),e("OutboundLink")],1),r._v(", совершившие революцию в NLP в 2013 году. Технология актуальна до сих пор. В проекте Natasha модели для "),e("a",{attrs:{href:"https://github.com/natasha/slovnet#morphology",target:"_blank",rel:"noopener noreferrer"}},[r._v("разбора морфологии"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/slovnet#syntax",target:"_blank",rel:"noopener noreferrer"}},[r._v("синтаксиса"),e("OutboundLink")],1),r._v(" и "),e("a",{attrs:{href:"https://github.com/natasha/slovnet#ner",target:"_blank",rel:"noopener noreferrer"}},[r._v("извлечения именованных сущностей"),e("OutboundLink")],1),r._v(" работают на пословных Navec-эмбеддингах, "),e("a",{attrs:{href:"https://github.com/natasha/slovnet#evaluation",target:"_blank",rel:"noopener noreferrer"}},[r._v("показывают качество выше других открытых решений"),e("OutboundLink")],1),r._v(".")]),r._v(" "),e("h3",{attrs:{id:"rusvectores"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#rusvectores"}},[r._v("#")]),r._v(" RusVectores")]),r._v(" "),e("p",[r._v("Для русского языка принято использовать "),e("a",{attrs:{href:"https://rusvectores.org/ru/models/",target:"_blank",rel:"noopener noreferrer"}},[r._v("предобученные эмбеддинги от RusVectores"),e("OutboundLink")],1),r._v(", у них есть неприятная особенность: в таблице записаны не слова, а пары «слово_POS-тег». Идея хорошая, для пары «печь_VERB» ожидаем вектор, похожий на «готовить_VERB», «варить_VERB», а для «печь_NOUN» — «изба_NOUN», «топка_NOUN».")]),r._v(" "),e("p",[r._v("На практике использовать такие эмбеддинги неудобно. Недостаточно разделить текст на токены, для каждого нужно как-то определить POS-тег. Таблица эмбеддингов разбухает. Вместо одного слова «стать», мы храним 6: 2 разумных «стать_VERB», «стать_NOUN» и 4 странных «стать_ADV», «стать_PROPN», «стать_NUM», «стать_ADJ». В таблице на 250 000 записей 195 000 уникальных слов.")]),r._v(" "),e("h3",{attrs:{id:"качество"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#качество"}},[r._v("#")]),r._v(" Качество")]),r._v(" "),e("p",[r._v("Оценим качество эмбеддингов на задаче семантической близости. Возьмём пару слов, для каждого найдём вектор-эмбеддинг, посчитаем косинусное сходство. Navec для похожих слов «чашка» и «кувшин» возвращет 0.49, для «фрукт» и «печь» — −0.0047. Соберём много пар с эталонными метками похожести, посчитаем корреляцию Спирмена с нашими ответами.")]),r._v(" "),e("p",[r._v("Авторы RusVectores используют небольшой "),e("a",{attrs:{href:"https://arxiv.org/abs/1801.06407",target:"_blank",rel:"noopener noreferrer"}},[r._v("аккуратно проверенный и исправленный"),e("OutboundLink")],1),r._v(" тестовый список пар "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_simlex",target:"_blank",rel:"noopener noreferrer"}},[r._v("SimLex965"),e("OutboundLink")],1),r._v(". Добавим свежий яндексовый "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_toloka_lrwc",target:"_blank",rel:"noopener noreferrer"}},[r._v("LRWC"),e("OutboundLink")],1),r._v(" и датасеты из "),e("a",{attrs:{href:"https://russe.nlpub.org/downloads/",target:"_blank",rel:"noopener noreferrer"}},[r._v("проекта RUSSE"),e("OutboundLink")],1),r._v(": "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_russe_hj",target:"_blank",rel:"noopener noreferrer"}},[r._v("HJ"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_russe_rt",target:"_blank",rel:"noopener noreferrer"}},[r._v("RT"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_russe_ae",target:"_blank",rel:"noopener noreferrer"}},[r._v("AE"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_russe_ae",target:"_blank",rel:"noopener noreferrer"}},[r._v("AE2"),e("OutboundLink")],1),r._v(":")]),r._v(" "),e("p",[r._v("Среднее качество на 6 датасетах")]),r._v(" "),e("p",[r._v("Время загрузки, секунды")]),r._v(" "),e("p",[r._v("Размер модели, МБ")]),r._v(" "),e("p",[r._v("Размер словаря, ×103")]),r._v(" "),e("p",[r._v("Navec")]),r._v(" "),e("p",[e("code",[r._v("hudlit_12B_500K_300d_100q")])]),r._v(" "),e("p",[e("strong",[r._v("0.719")])]),r._v(" "),e("p",[e("strong",[r._v("1.0")])]),r._v(" "),e("p",[e("strong",[r._v("50.6")])]),r._v(" "),e("p",[e("strong",[r._v("500")])]),r._v(" "),e("p",[e("code",[r._v("news_1B_250K_300d_100q")])]),r._v(" "),e("p",[r._v("0.653")]),r._v(" "),e("p",[e("strong",[r._v("0.5")])]),r._v(" "),e("p",[e("strong",[r._v("25.4")])]),r._v(" "),e("p",[e("strong",[r._v("250")])]),r._v(" "),e("p",[r._v("RusVectores")]),r._v(" "),e("p",[e("code",[r._v("ruscorpora_upos_cbow_300_20_2019")])]),r._v(" "),e("p",[e("strong",[r._v("0.692")])]),r._v(" "),e("p",[e("strong",[r._v("3.3")])]),r._v(" "),e("p",[e("strong",[r._v("220.6")])]),r._v(" "),e("p",[r._v("189")]),r._v(" "),e("p",[e("code",[r._v("ruwikiruscorpora_upos_skipgram_300_2_2019")])]),r._v(" "),e("p",[r._v("0.691")]),r._v(" "),e("p",[r._v("5.0")]),r._v(" "),e("p",[r._v("290.0")]),r._v(" "),e("p",[r._v("248")]),r._v(" "),e("p",[e("code",[r._v("tayga_upos_skipgram_300_2_2019")])]),r._v(" "),e("p",[e("strong",[r._v("0.726")])]),r._v(" "),e("p",[r._v("5.2")]),r._v(" "),e("p",[r._v("290.7")]),r._v(" "),e("p",[e("strong",[r._v("249")])]),r._v(" "),e("p",[e("code",[r._v("tayga_none_fasttextcbow_300_10_2019")])]),r._v(" "),e("p",[r._v("0.638")]),r._v(" "),e("p",[r._v("8.0")]),r._v(" "),e("p",[r._v("2741.9")]),r._v(" "),e("p",[r._v("192")]),r._v(" "),e("p",[e("code",[r._v("araneum_none_fasttextcbow_300_5_2018")])]),r._v(" "),e("p",[r._v("0.664")]),r._v(" "),e("p",[r._v("16.4")]),r._v(" "),e("p",[r._v("2752.1")]),r._v(" "),e("p",[r._v("195")]),r._v(" "),e("p",[e("em",[e("a",{attrs:{href:"https://github.com/natasha/navec#evaluation",target:"_blank",rel:"noopener noreferrer"}},[r._v("Таблица с разбивкой по датасетам"),e("OutboundLink")],1),r._v(" в репозитории Navec.")])]),r._v(" "),e("p",[r._v("Качество "),e("code",[r._v("hudlit_12B_500K_300d_100q")]),r._v(" сравнимо или лучше, чем у решений RusVectores, словарь больше в 2–3 раза, размер модели меньше в 5–6 раз. Как удалось получить такое качество и размер?")]),r._v(" "),e("h3",{attrs:{id:"принцип-работы-2"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#принцип-работы-2"}},[r._v("#")]),r._v(" Принцип работы")]),r._v(" "),e("p",[e("code",[r._v("hudlit_12B_500K_300d_100q")]),r._v(" — "),e("a",{attrs:{href:"https://nlp.stanford.edu/projects/glove/",target:"_blank",rel:"noopener noreferrer"}},[r._v("GloVe-эмбеддинги"),e("OutboundLink")],1),r._v(" обученные на "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_librusec",target:"_blank",rel:"noopener noreferrer"}},[r._v("145ГБ художественной литературы"),e("OutboundLink")],1),r._v(". Архив с текстами возьмём из "),e("a",{attrs:{href:"https://russe.nlpub.org/downloads/",target:"_blank",rel:"noopener noreferrer"}},[r._v("проекта RUSSE"),e("OutboundLink")],1),r._v(". Используем "),e("a",{attrs:{href:"https://github.com/stanfordnlp/GloVe",target:"_blank",rel:"noopener noreferrer"}},[r._v("оригинальную реализацию GloVe на C"),e("OutboundLink")],1),r._v(", обернём её в "),e("a",{attrs:{href:"https://github.com/natasha/navec/blob/master/navec/train/glove.py",target:"_blank",rel:"noopener noreferrer"}},[r._v("удобный Python-интерфейс"),e("OutboundLink")],1),r._v(".")]),r._v(" "),e("p",[r._v("Почему не word2vec? Эксперименты на большом датасете быстрее с GloVe. Один раз считаем матрицу коллокаций, по ней готовим эмбеддинги разных размерностей, выбираем оптимальный вариант.")]),r._v(" "),e("p",[r._v("Почему не fastText? В проекте Natasha мы работаем с текстами новостей. В них мало опечаток, проблему OOV-токенов решает большой словарь. 250 000 строк в таблице "),e("code",[r._v("news_1B_250K_300d_100q")]),r._v(" покрывают 98% слов в новостных статьях.")]),r._v(" "),e("p",[r._v("Размер словаря "),e("code",[r._v("hudlit_12B_500K_300d_100q")]),r._v(" — 500 000 записей, он покрывает 98% слов в художественных текстах. Оптимальная размерность векторов — 300. Таблица 500 000 × 300 из float-чисел занимает 578МБ, размер архива с весами "),e("code",[r._v("hudlit_12B_500K_300d_100q")]),r._v(" в 12 раз меньше (48МБ). Дело в квантизации.")]),r._v(" "),e("h3",{attrs:{id:"квантизация-2"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#квантизация-2"}},[r._v("#")]),r._v(" Квантизация")]),r._v(" "),e("p",[r._v("Заменим 32-битные float-числа на 8-битные коды: [−∞, −0.86) — код 0, [−0.86, -0.79) — код 1, [-0.79, -0.74) — 2, …, [0.86, ∞) — 255. Размер таблицы уменьшится в 4 раз (143МБ).")]),r._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[r._v("Было:\n-0.220 -0.071  0.320 -0.279  0.376  0.409  0.340 -0.329  0.400\n 0.046  0.870 -0.163  0.075  0.198 -0.357 -0.279  0.267  0.239\n 0.111  0.057  0.746 -0.240 -0.254  0.504  0.202  0.212  0.570\n 0.529  0.088  0.444 -0.005 -0.003 -0.350 -0.001  0.472  0.635\n                     ────── ──────\n-0.170  0.677  0.212  0.202 -0.030  0.279  0.229 -0.475 -0.031\n                            ──────                      ──────\nСтало:\n    63    105    215     49    225    230    219     39    228\n   143    255     78    152    187     34     49    204    198\n   163    146    253     58     55    240    188    191    246\n   243    155    234    127    127     35    128    237    249\n                        ───    ───\n    76    251    191    188    118    207    195     18    118\n                               ───                         ───\n")])])]),e("p",[e("em",[r._v("Данные огрубляются, разные значения -0.005 и -0.003 заменяет один код 127, -0.030 и -0.031 — 118")])]),r._v(" "),e("p",[r._v("Заменим кодом не одно, а 3 числа. Кластеризуем все тройки чисел из таблицы эмбеддингов "),e("a",{attrs:{href:"https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_k-%D1%81%D1%80%D0%B5%D0%B4%D0%BD%D0%B8%D1%85",target:"_blank",rel:"noopener noreferrer"}},[r._v("алгоритмом k-means"),e("OutboundLink")],1),r._v(" на 256 кластеров, вместо каждой тройки будем хранить код от 0 до 255. Таблица уменьшится ещё в 3 раза (48МБ). Navec использует "),e("a",{attrs:{href:"http://yusukematsui.me/project/pqkmeans/pqkmeans.html",target:"_blank",rel:"noopener noreferrer"}},[r._v("библиотеку PQk-means"),e("OutboundLink")],1),r._v(", она разбивает матрицу на 100 колонок, каждую кластеризует отдельно, качество на синтетических тестах падёт на 1 процентный пункт. Понятно про квантизацию в статье "),e("a",{attrs:{href:"http://mccormickml.com/2017/10/13/product-quantizer-tutorial-part-1/",target:"_blank",rel:"noopener noreferrer"}},[r._v("Product Quantizers for k-NN"),e("OutboundLink")],1),r._v(".")]),r._v(" "),e("p",[r._v("Квантованные эмбеддинги проигрывают обычным по скорости. Сжатый вектор перед использованием нужно распаковать. Аккуратно реализуем процедуру, "),e("a",{attrs:{href:"https://github.com/natasha/navec/blob/master/navec/pq.py#L40-L43",target:"_blank",rel:"noopener noreferrer"}},[r._v("применим Numpy-магию"),e("OutboundLink")],1),r._v(", в PyTorch "),e("a",{attrs:{href:"https://github.com/natasha/slovnet/blob/master/slovnet/model/emb.py#L29-L39",target:"_blank",rel:"noopener noreferrer"}},[r._v("используем torch.gather"),e("OutboundLink")],1),r._v(". В Slovnet NER доступ к таблице эмбеддингов занимает 0.1% от общего времени вычислений.")]),r._v(" "),e("p",[r._v("Модуль "),e("code",[r._v("NavecEmbedding")]),r._v(" из "),e("a",{attrs:{href:"https://github.com/natasha/slovnet",target:"_blank",rel:"noopener noreferrer"}},[r._v("библиотеки Slovnet"),e("OutboundLink")],1),r._v(" интегрирует Navec в PyTorch-модели:")]),r._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[r._v(">>> import torch\n\n>>> from navec import Navec\n>>> from slovnet.model.emb import NavecEmbedding\n\n>>> path = 'hudlit_12B_500K_300d_100q.tar'  # 51MB\n>>> navec = Navec.load(path)  # ~1 sec, ~100MB RAM\n\n>>> words = ['навек', '<unk>', '<pad>']\n>>> ids = [navec.vocab[_] for _ in words]\n\n>>> emb = NavecEmbedding(navec)\n>>> input = torch.tensor(ids)\n\n>>> emb(input)  # 3 x 300\ntensor([[ 4.2000e-01,  3.6666e-01,  1.7728e-01,\n        [ 1.6954e-01, -4.6063e-01,  5.4519e-01,\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,\n\t  ...\n")])])]),e("h2",{attrs:{id:"nerus-большои-синтетическии-датасет-с-разметкои-морфологии-синтаксиса-и-именованных-сущностеи"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#nerus-большои-синтетическии-датасет-с-разметкои-морфологии-синтаксиса-и-именованных-сущностеи"}},[r._v("#")]),r._v(" Nerus — большой синтетический датасет с разметкой морфологии, синтаксиса и именованных сущностей")]),r._v(" "),e("p",[e("img",{attrs:{src:"https://habrastorage.org/r/w1560/webt/qz/q7/ma/qzq7mafha7lfwywqjrtllntksko.png",alt:""}})]),r._v(" "),e("p",[r._v("В проекте Natasha анализ морфологии, синтаксиса и извлечение именованных сущностей делают 3 компактные модели: "),e("a",{attrs:{href:"https://github.com/natasha/slovnet#ner",target:"_blank",rel:"noopener noreferrer"}},[r._v("Slovnet NER"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/slovnet#morphology",target:"_blank",rel:"noopener noreferrer"}},[r._v("Slovnet Morph"),e("OutboundLink")],1),r._v(" и "),e("a",{attrs:{href:"https://github.com/natasha/slovnet#syntax",target:"_blank",rel:"noopener noreferrer"}},[r._v("Slovnet Syntax"),e("OutboundLink")],1),r._v(". "),e("a",{attrs:{href:"https://github.com/natasha/slovnet#evaluation",target:"_blank",rel:"noopener noreferrer"}},[r._v("Качество решений"),e("OutboundLink")],1),r._v(" на 1–5 процентных пунктов хуже, чем у тяжёлых аналогов c BERT-архитектурой, размер в 50-75 раз меньше, скорость на CPU в 2 раза больше. Модели обучены на огромном синтетическом "),e("a",{attrs:{href:"https://github.com/natasha/nerus",target:"_blank",rel:"noopener noreferrer"}},[r._v("датасете Nerus"),e("OutboundLink")],1),r._v(", в архиве 700 000 новостных статей с "),e("a",{attrs:{href:"https://universaldependencies.org/format.html",target:"_blank",rel:"noopener noreferrer"}},[r._v("CoNLL-U"),e("OutboundLink")],1),r._v("-разметкой морфологии, синтаксиса и именованных сущностей:")]),r._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[r._v("# newdoc id = 0\n# sent_id = 0_0\n# text = Вице-премьер по социальным вопросам Татьяна Голикова рассказала, в каких регионах России зафиксирована ...\n1    Вице-премьер   _   NOUN   _   Animacy=Anim|C...   7   nsubj     _   Tag=O\n2    по             _   ADP    _   _                     4   case      _   Tag=O\n3    социальным     _   ADJ    _   Case=Dat|Degre...   4   amod      _   Tag=O\n4    вопросам       _   NOUN   _   Animacy=Inan|C...   1   nmod      _   Tag=O\n5    Татьяна        _   PROPN  _   Animacy=Anim|C...   1   appos     _   Tag=B-PER\n6    Голикова       _   PROPN  _   Animacy=Anim|C...   5   flat:name _   Tag=I-PER\n7    рассказала     _   VERB   _   Aspect=Perf|Ge...   0   root      _   Tag=O\n8    ,              _   PUNCT  _   _                   13  punct     _   Tag=O\n9    в              _   ADP    _   _                   11  case      _   Tag=O\n10   каких          _   DET    _   Case=Loc|Numbe...   11  det       _   Tag=O\n11   регионах       _   NOUN   _   Animacy=Inan|C...   13  obl       _   Tag=O\n12   России         _   PROPN  _   Animacy=Inan|C...   11  nmod      _   Tag=B-LOC\n13   зафиксирована  _   VERB   _   Aspect=Perf|Ge...   7   ccomp     _   Tag=O\n14   наиболее       _   ADV    _   Degree=Pos          15  advmod    _   Tag=O\n15   высокая        _   ADJ    _   Case=Nom|Degre...   16  amod      _   Tag=O\n16   смертность     _   NOUN   _   Animacy=Inan|C...   13  nsubj     _   Tag=O\n17   от             _   ADP    _   _                   18  case      _   Tag=O\n18   рака           _   NOUN   _   Animacy=Inan|C...   16  nmod      _   Tag=O\n19   ,              _   PUNCT  _   _                   20  punct     _   Tag=O\n20   сообщает       _   VERB   _   Aspect=Imp|Moo...   0   root      _   Tag=O\n21   РИА            _   PROPN  _   Animacy=Inan|C...   20  nsubj     _   Tag=B-ORG\n22   Новости        _   PROPN  _   Animacy=Inan|C...   21  appos     _   Tag=I-ORG\n23   .              _   PUNCT  _   _                   20  punct     _   Tag=O\n\n# sent_id = 0_1\n# text = По словам Голиковой, чаще всего онкологические заболевания становились причиной смерти в Псковской, Тверской, ...\n1   По              _   ADP    _   _                   2   case      _   Tag=O\n2   словам          _   NOUN   _   Animacy=Inan|C...   9   parataxis _   Tag=O\n...\n")])])]),e("p",[r._v("Slovnet NER, Morph, Syntax — примитивные модели. Когда в обучающей выборке 1000 примеров, Slovnet NER отстаёт от тяжёлого BERT-аналога на 11 процентных пунктов, когда примеров 10 000 — на 3 пункта, когда 500 000 — на 1.")]),r._v(" "),e("p",[r._v("Nerus — результат работы, тяжёлых моделей с BERT-архитектурой: "),e("a",{attrs:{href:"https://github.com/natasha/slovnet/blob/master/scripts/02_bert_ner/main.ipynb",target:"_blank",rel:"noopener noreferrer"}},[r._v("Slovnet BERT NER"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/slovnet/blob/master/scripts/03_bert_morph/main.ipynb",target:"_blank",rel:"noopener noreferrer"}},[r._v("Slovnet BERT Morph"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/slovnet/blob/master/scripts/04_bert_syntax/main.ipynb",target:"_blank",rel:"noopener noreferrer"}},[r._v("Slovnet BERT Syntax"),e("OutboundLink")],1),r._v(". Обработка 700 000 новостных статей занимает 20 часов на Tesla V100. Мы экономим время других исследователей, выкладываем готовый архив в открытый доступ. В "),e("a",{attrs:{href:"https://github.com/buriy/spacy-ru",target:"_blank",rel:"noopener noreferrer"}},[r._v("SpaCy-Ru"),e("OutboundLink")],1),r._v(" обучают на Nerus качественные русскоязычные модели для SpaCy, готовят патч в официальный репозиторий.")]),r._v(" "),e("p",[e("a",{attrs:{href:"https://storage.yandexcloud.net/natasha-nerus/data/nerus_lenta.conllu.gz",target:"_blank",rel:"noopener noreferrer"}},[e("img",{attrs:{src:"https://habrastorage.org/r/w1560/webt/7_/g-/2h/7_g-2hzeucgt0wvyrpmapcsy9o4.png",alt:""}}),e("OutboundLink")],1)]),r._v(" "),e("p",[r._v("У синтетической разметки высокое качество: точность определения морфологических тегов — 98%, синтаксических связей — 96%. Для NER оценки F1 по токенам: PER — 99%, LOC — 98%, ORG — 97%. Для оценки качества мы размечаем "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_ud_syntag",target:"_blank",rel:"noopener noreferrer"}},[r._v("SynTagRus"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_ne5",target:"_blank",rel:"noopener noreferrer"}},[r._v("Collection5"),e("OutboundLink")],1),r._v(" и новостной срез "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_gramru",target:"_blank",rel:"noopener noreferrer"}},[r._v("GramEval2020"),e("OutboundLink")],1),r._v(", сравниваем эталонную разметку с нашей, подробнее в "),e("a",{attrs:{href:"https://github.com/natasha/nerus#evaluation",target:"_blank",rel:"noopener noreferrer"}},[r._v("репозитории Nerus"),e("OutboundLink")],1),r._v(". Из-за ошибок в разметке синтаксиса встречаются циклы и множественные корни, POS-теги иногда не соответствуют синтаксическим рёбрам. Полезно использовать "),e("a",{attrs:{href:"https://github.com/UniversalDependencies/tools/blob/master/validate.py",target:"_blank",rel:"noopener noreferrer"}},[r._v("валидатор от Universal Dependencies"),e("OutboundLink")],1),r._v(", пропускать такие примеры.")]),r._v(" "),e("p",[r._v("Python-пакет Nerus организует удобный интерфейс для загрузки и визуализации разметки:")]),r._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[r._v(">>> from nerus import load_nerus\n\n>>> docs = load_nerus('nerus_lenta.conllu.gz')\n>>> doc = next(docs)\n>>> doc\n\nNerusDoc(\n    id='0',\n    sents=[NerusSent(\n         id='0_0',\n         text='Вице-премьер по социальным вопросам Татьяна Голикова рассказала, в каких регионах России ...',\n         tokens=[NerusToken(\n              id='1',\n              text='Вице-премьер',\n              pos='NOUN',\n              feats={'Animacy': 'Anim',\n               'Case': 'Nom',\n               'Gender': 'Masc',\n               'Number': 'Sing'},\n              head_id='7',\n              rel='nsubj',\n              tag='O'\n          ),\n          NerusToken(\n              id='2',\n              text='по',\n              pos='ADP',\n...\n\n>>> doc.ner.print()\nВице-премьер по социальным вопросам Татьяна Голикова рассказала, в каких регионах России зафиксирована наиболее \n                                    PER─────────────                              LOC───                     \nвысокая смертность от  рака, сообщает РИА Новости. По словам Голиковой, чаще всего онкологические заболевания\n                                      ORG────────            PER──────             \n...\n\n​\n>>> sent = doc.sents[0]\n>>> sent.morph.print()\n        Вице-премьер  NOUN|Animacy=Anim|Case=Nom|Gender=Masc|Number=Sing\n                  по  ADP\n          социальным  ADJ|Case=Dat|Degree=Pos|Number=Plur\n            вопросам  NOUN|Animacy=Inan|Case=Dat|Gender=Masc|Number=Plur\n             Татьяна  PROPN|Animacy=Anim|Case=Nom|Gender=Fem|Number=Sing\n            Голикова  PROPN|Animacy=Anim|Case=Nom|Gender=Fem|Number=Sing\n          рассказала  VERB|Aspect=Perf|Gender=Fem|Mood=Ind|Number=Sing\n...\n\t\t\t\t   \n>>> sent.syntax.print()\n  ┌►┌─┌───── Вице-премьер  nsubj\n  │ │ │ ┌──► по            case\n  │ │ │ │ ┌► социальным    amod\n  │ │ └►└─└─ вопросам      nmod\n  │ └────►┌─ Татьяна       appos\n  │       └► Голикова      flat:name\n┌─└───────── рассказала    \n│   ┌──────► ,             punct\n│   │   ┌──► в             case\n│   │   │ ┌► каких         det\n│   │ ┌►└─└─ регионах      obl\n│   │ │ └──► России        nmod\n└──►└─└───── зафиксирована ccomp\n    │     ┌► наиболее      advmod\n    │   ┌►└─ высокая       amod\n    └►┌─└─── смертность    nsubj:pass\n      │   ┌► от            case\n      └──►└─ рака          nmod\n          ┌► ,             punct\n      ┌─┌─└─ сообщает      \n      │ └►┌─ РИА           nsubj\n      │   └► Новости       appos\n      └────► .             punct\n")])])]),e("p",[e("a",{attrs:{href:"https://github.com/natasha/nerus#usage",target:"_blank",rel:"noopener noreferrer"}},[r._v("Инструкция по установке, примеры использования"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/nerus#evaluation",target:"_blank",rel:"noopener noreferrer"}},[r._v("оценки качества"),e("OutboundLink")],1),r._v(" в репозитории Nerus.")]),r._v(" "),e("h2",{attrs:{id:"corus-коллекция-ссылок-на-публичные-русскоязычные-датасеты-функции-для-загрузки"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#corus-коллекция-ссылок-на-публичные-русскоязычные-датасеты-функции-для-загрузки"}},[r._v("#")]),r._v(" Corus — коллекция ссылок на публичные русскоязычные датасеты + функции для загрузки")]),r._v(" "),e("p",[e("img",{attrs:{src:"https://habrastorage.org/r/w1560/webt/1q/m2/qg/1qm2qgkuf6krpvpcobq5zkwtba8.png",alt:""}}),e("br"),r._v(" "),e("a",{attrs:{href:"https://github.com/natasha/corus",target:"_blank",rel:"noopener noreferrer"}},[r._v("Библиотека Corus"),e("OutboundLink")],1),r._v(" — часть проекта Natasha, коллекция ссылок на публичные русскоязычные NLP-датасеты + Python-пакет с функциями-загрузчиками. "),e("a",{attrs:{href:"https://github.com/natasha/corus#reference",target:"_blank",rel:"noopener noreferrer"}},[r._v("Список ссылок на источники"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus#install",target:"_blank",rel:"noopener noreferrer"}},[r._v("инструкция по установке"),e("OutboundLink")],1),r._v(" и "),e("a",{attrs:{href:"https://nbviewer.jupyter.org/github/natasha/corus/blob/master/docs.ipynb",target:"_blank",rel:"noopener noreferrer"}},[r._v("примеры использования"),e("OutboundLink")],1),r._v(" в репозитории Corus.")]),r._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[r._v(">>> from corus import load_lenta\n\n# Находим в реестре Corus ссылку на Lenta.ru, загружаем:\n# wget https://github.com/yutkin/Lenta.Ru-News-Dataset/...\n\n>>> path = 'lenta-ru-news.csv.gz'\n>>> records = load_lenta(path)  # 2ГБ, 750 000 статей\n>>> next(records)\nLentaRecord(\n    url='https://lenta.ru/news/2018/12/14/cancer/',\n    title='Названы регионы России с\\xa0самой высокой ...',\n    text='Вице-премьер по социальным вопросам Татьяна ...',\n    topic='Россия',\n    tags='Общество'\n)\n")])])]),e("p",[r._v("Полезные открытые датасеты для русского языка так хорошо спрятаны, что мало людей про них знает.")]),r._v(" "),e("h3",{attrs:{id:"примеры"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#примеры"}},[r._v("#")]),r._v(" Примеры")]),r._v(" "),e("h4",{attrs:{id:"корпус-новостных-статеи"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#корпус-новостных-статеи"}},[r._v("#")]),r._v(" Корпус новостных статей")]),r._v(" "),e("p",[r._v("Хотим обучить языковую модель на новостных статьях, нужно много текстов. Первым приходит в голову новостной срез "),e("a",{attrs:{href:"https://tatianashavrina.github.io/taiga_site/",target:"_blank",rel:"noopener noreferrer"}},[r._v("датасета Taiga"),e("OutboundLink")],1),r._v(" (~1ГБ). Многие знают про "),e("a",{attrs:{href:"https://github.com/yutkin/Lenta.Ru-News-Dataset",target:"_blank",rel:"noopener noreferrer"}},[r._v("дамп Lenta.ru"),e("OutboundLink")],1),r._v(" (2ГБ). Остальные источники найти сложнее. В 2019 году на Диалоге проходил "),e("a",{attrs:{href:"https://vk.com/headline_gen",target:"_blank",rel:"noopener noreferrer"}},[r._v("конкурс про генерацию заголовков"),e("OutboundLink")],1),r._v(", организаторы подготовили "),e("a",{attrs:{href:"https://github.com/RossiyaSegodnya/ria_news_dataset",target:"_blank",rel:"noopener noreferrer"}},[r._v("дамп РИА Новостей"),e("OutboundLink")],1),r._v(" за 4 года (3.7ГБ). В 2018 году Юрий Бабуров опубликовал "),e("a",{attrs:{href:"https://github.com/buriy/russian-nlp-datasets/releases/tag/r4",target:"_blank",rel:"noopener noreferrer"}},[r._v("выгрузку с 40 русскоязычных новостных ресурсов"),e("OutboundLink")],1),r._v(" (7.5ГБ). Волонтёры из "),e("a",{attrs:{href:"http://ods.ai/",target:"_blank",rel:"noopener noreferrer"}},[r._v("ODS"),e("OutboundLink")],1),r._v(" "),e("a",{attrs:{href:"https://github.com/ods-ai-ml4sg/proj_news_viz/releases/tag/data",target:"_blank",rel:"noopener noreferrer"}},[r._v("делятся архивами"),e("OutboundLink")],1),r._v(" (7ГБ), собранными для "),e("a",{attrs:{href:"https://proj-news-viz-flask.herokuapp.com/",target:"_blank",rel:"noopener noreferrer"}},[r._v("проекта про анализ новостной повестки"),e("OutboundLink")],1),r._v(".")]),r._v(" "),e("p",[r._v("В "),e("a",{attrs:{href:"https://github.com/natasha/corus#reference",target:"_blank",rel:"noopener noreferrer"}},[r._v("реестре Corus"),e("OutboundLink")],1),r._v(" ссылки на эти датасеты помечены тегом «news», для всех источников есть функции-загрузчики: "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_taiga_arzamas",target:"_blank",rel:"noopener noreferrer"}},[e("code",[r._v("load_taiga_*")]),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_lenta",target:"_blank",rel:"noopener noreferrer"}},[e("code",[r._v("load_lenta")]),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_ria",target:"_blank",rel:"noopener noreferrer"}},[e("code",[r._v("load_ria")]),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_buriy_lenta",target:"_blank",rel:"noopener noreferrer"}},[e("code",[r._v("load_buriy_*")]),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_ods_interfax",target:"_blank",rel:"noopener noreferrer"}},[e("code",[r._v("load_ods_*")]),e("OutboundLink")],1),r._v(".")]),r._v(" "),e("h4",{attrs:{id:"ner"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#ner"}},[r._v("#")]),r._v(" NER")]),r._v(" "),e("p",[r._v("Хотим обучить NER для русского языка, нужны аннотированные тексты. Первым делом вспоминаем про "),e("a",{attrs:{href:"https://github.com/dialogue-evaluation/factRuEval-2016/",target:"_blank",rel:"noopener noreferrer"}},[r._v("данные конкурса factRuEval-2016"),e("OutboundLink")],1),r._v(". У разметки есть недостатки: свой сложный формат, спаны сущностей пересекаются, есть неоднозначная категориям «LocOrg». Не все знают про "),e("a",{attrs:{href:"http://labinform.ru/pub/named_entities/descr_ne.htm",target:"_blank",rel:"noopener noreferrer"}},[r._v("коллекцию Named Entities 5"),e("OutboundLink")],1),r._v(" наследницу "),e("a",{attrs:{href:"http://ai-center.botik.ru/Airec/index.php/ru/collections/28-persons-1000",target:"_blank",rel:"noopener noreferrer"}},[r._v("Persons-1000"),e("OutboundLink")],1),r._v(". Разметка в "),e("a",{attrs:{href:"https://brat.nlplab.org/standoff.html",target:"_blank",rel:"noopener noreferrer"}},[r._v("стандартном формате"),e("OutboundLink")],1),r._v(", спаны не пересекаются, красота! Остальные три источника известны только самым преданным фанатам русскоязычного NER. Напишем на почту Ринату Гарееву, приложим ссылку на его "),e("a",{attrs:{href:"https://www.researchgate.net/publication/262203599_Introducing_Baselines_for_Russian_Named_Entity_Recognition",target:"_blank",rel:"noopener noreferrer"}},[r._v("статью 2013 года"),e("OutboundLink")],1),r._v(", в ответ получим 250 новостных статей с помеченными именами и организациями. В 2019 году проводился "),e("a",{attrs:{href:"http://bsnlp.cs.helsinki.fi/shared_task.html",target:"_blank",rel:"noopener noreferrer"}},[r._v("конкурс BSNLP-2019"),e("OutboundLink")],1),r._v(" про NER для славянских языков, напишем организаторам, получим ещё 450 размеченных текстов. В проекте WiNER "),e("a",{attrs:{href:"https://www.aclweb.org/anthology/I17-1042/",target:"_blank",rel:"noopener noreferrer"}},[r._v("придумали делать полуавтоматическую разметку NER из дампов Wikipedia"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/dice-group/FOX/tree/master/input/Wikiner",target:"_blank",rel:"noopener noreferrer"}},[r._v("большая выгрузка для русского доступна на Github"),e("OutboundLink")],1),r._v(".")]),r._v(" "),e("p",[r._v("Ссылки и функции для загрузки в реестре Corus: "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_factru",target:"_blank",rel:"noopener noreferrer"}},[e("code",[r._v("load_factru")]),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_ne5",target:"_blank",rel:"noopener noreferrer"}},[e("code",[r._v("load_ne5")]),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_gareev",target:"_blank",rel:"noopener noreferrer"}},[e("code",[r._v("load_gareev")]),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_bsnlp",target:"_blank",rel:"noopener noreferrer"}},[e("code",[r._v("load_bsnlp")]),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_wikiner",target:"_blank",rel:"noopener noreferrer"}},[e("code",[r._v("load_wikiner")]),e("OutboundLink")],1),r._v(".")]),r._v(" "),e("h3",{attrs:{id:"коллекция-ссылок"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#коллекция-ссылок"}},[r._v("#")]),r._v(" Коллекция ссылок")]),r._v(" "),e("p",[r._v("Перед тем как обзавестить загрузчиком и попасть в реестр, ссылки на источники копятся в "),e("a",{attrs:{href:"https://github.com/natasha/corus/issues",target:"_blank",rel:"noopener noreferrer"}},[r._v("разделе с Тикетами"),e("OutboundLink")],1),r._v(". В коллекции 30 датасетов: "),e("a",{attrs:{href:"https://github.com/natasha/corus/issues/30",target:"_blank",rel:"noopener noreferrer"}},[r._v("новая версия Taiga"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus/issues/26",target:"_blank",rel:"noopener noreferrer"}},[r._v("568ГБ русского текста из Common Crawl"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus/issues/32",target:"_blank",rel:"noopener noreferrer"}},[r._v("отзывы c Banki.ru"),e("OutboundLink")],1),r._v(" и "),e("a",{attrs:{href:"https://github.com/natasha/corus/issues/16",target:"_blank",rel:"noopener noreferrer"}},[r._v("Auto.ru"),e("OutboundLink")],1),r._v(". Приглашаем делиться находками, заводить тикеты со ссылками.")]),r._v(" "),e("h3",{attrs:{id:"функции-загрузчики"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#функции-загрузчики"}},[r._v("#")]),r._v(" Функции-загрузчики")]),r._v(" "),e("p",[r._v("Код для простого датасета легко написать самому. "),e("a",{attrs:{href:"https://github.com/yutkin/Lenta.Ru-News-Dataset",target:"_blank",rel:"noopener noreferrer"}},[r._v("Дамп Lenta.ru"),e("OutboundLink")],1),r._v(" оформлен грамотно, "),e("a",{attrs:{href:"https://github.com/natasha/corus/blob/master/corus/sources/lenta.py#L28-L30",target:"_blank",rel:"noopener noreferrer"}},[r._v("реализация простая"),e("OutboundLink")],1),r._v(". "),e("a",{attrs:{href:"https://tatianashavrina.github.io/taiga_site/",target:"_blank",rel:"noopener noreferrer"}},[r._v("Taiga"),e("OutboundLink")],1),r._v(" состоит из ~15 миллионов "),e("a",{attrs:{href:"https://universaldependencies.org/format.html",target:"_blank",rel:"noopener noreferrer"}},[r._v("CoNLL-U"),e("OutboundLink")],1),r._v("-файлов, запакованных в zip-архивы. Чтобы загрузка работала быстро, не использовала много памяти и не угробила файловую систему, нужно заморочиться, аккуратно на низком уровне "),e("a",{attrs:{href:"https://github.com/natasha/corus/blob/master/corus/zip.py",target:"_blank",rel:"noopener noreferrer"}},[r._v("реализовать работу с zip-файлами"),e("OutboundLink")],1),r._v(".")]),r._v(" "),e("p",[r._v("Для 35 источников в Python-пакете Corus есть функции-загрузчики. Интерфейс доступа к Taiga не сложнее, чем к дампу Lenta.ru:")]),r._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[r._v(">>> from corus import load_taiga_proza_metas, load_taiga_proza\n\n>>> path = 'taiga/proza_ru.zip'\n>>> metas = load_taiga_proza_metas(path)\n>>> records = load_taiga_proza(path, metas)\n>>> next(records)\n\nTaigaRecord(\n    id='20151231005',\n    meta=Meta(\n        id='20151231005',\n        timestamp=datetime.datetime(2015, 12, 31, 23, 40),\n        genre='Малые формы',\n        topic='миниатюры',\n        author=Author(\n            name='Кальб',\n            readers=7973,\n            texts=92681,\n            url='http://www.proza.ru/avtor/sadshoot'\n        ),\n        title='С Новым Годом!',\n        url='http://www.proza.ru/2015/12/31/1875'\n    ),\n    text='...Искры улыбок...\\n... затмят фейерверки..\\n...\n)\n")])])]),e("p",[r._v("Приглашаем пользователей делать пулл-реквесты, присылать свои функции-загрузчики, "),e("a",{attrs:{href:"https://github.com/natasha/corus#development",target:"_blank",rel:"noopener noreferrer"}},[r._v("короткая инструкция"),e("OutboundLink")],1),r._v(" в репозитории Corus.")]),r._v(" "),e("h2",{attrs:{id:"naeval-количественное-сравнение-систем-для-русскоязычного-nlp"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#naeval-количественное-сравнение-систем-для-русскоязычного-nlp"}},[r._v("#")]),r._v(" Naeval — количественное сравнение систем для русскоязычного NLP")]),r._v(" "),e("p",[e("img",{attrs:{src:"https://habrastorage.org/r/w1560/webt/v4/ah/kq/v4ahkqyhsuxsp5m8jjaqoflwq_c.png",alt:""}}),e("br"),r._v("\nNatasha — не научный проект, нет цели побить SOTA, но важно проверить качество на публичных бенчмарках, постараться занять высокое место, сильно не проиграв в производительности. Как делают в академии: измеряют качество, получают число, берут таблички из других статей, сравнивают эти числа со своими. У такой схемы две проблемы:")]),r._v(" "),e("ol",[e("li",[r._v("Забывают про производительность. Не сравнивают размер модели, скорость работы. Упор только на качество.")]),r._v(" "),e("li",[r._v("Не публикуют код. В расчёте метрики качества обычно миллион нюансов. Как именно считали в других статьях? Неизвестно.")])]),r._v(" "),e("p",[e("a",{attrs:{href:"https://github.com/natasha/naeval",target:"_blank",rel:"noopener noreferrer"}},[r._v("Naeval"),e("OutboundLink")],1),r._v(" — часть проекта Natasha, набор скриптов для оценки качества и скорости работы открытых инструментов для обработки естественного русского языка:")]),r._v(" "),e("p",[r._v("Задача")]),r._v(" "),e("p",[r._v("Датасеты")]),r._v(" "),e("p",[r._v("Решения")]),r._v(" "),e("p",[r._v("Токенизация")]),r._v(" "),e("p",[e("a",{attrs:{href:"https://github.com/natasha/corus#load_ud_syntag",target:"_blank",rel:"noopener noreferrer"}},[r._v("SynTagRus"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_morphoru_corpora",target:"_blank",rel:"noopener noreferrer"}},[r._v("OpenCorpora"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_morphoru_gicrya",target:"_blank",rel:"noopener noreferrer"}},[r._v("GICRYA"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_morphoru_rnc",target:"_blank",rel:"noopener noreferrer"}},[r._v("RNC"),e("OutboundLink")],1)]),r._v(" "),e("p",[e("a",{attrs:{href:"https://github.com/natasha/naeval#spacy",target:"_blank",rel:"noopener noreferrer"}},[r._v("SpaCy"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/naeval#nltk",target:"_blank",rel:"noopener noreferrer"}},[r._v("NLTK"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/naeval#mystem",target:"_blank",rel:"noopener noreferrer"}},[r._v("MyStem"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/naeval#moses",target:"_blank",rel:"noopener noreferrer"}},[r._v("Moses"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/naeval#segtok",target:"_blank",rel:"noopener noreferrer"}},[r._v("SegTok"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/naeval#spacy_russian_tokenizer",target:"_blank",rel:"noopener noreferrer"}},[r._v("SpaCy Russian Tokenizer"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/naeval#rutokenizer",target:"_blank",rel:"noopener noreferrer"}},[r._v("RuTokenizer"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/naeval#razdel",target:"_blank",rel:"noopener noreferrer"}},[r._v("Razdel"),e("OutboundLink")],1)]),r._v(" "),e("p",[r._v("Сегментация предложений")]),r._v(" "),e("p",[e("a",{attrs:{href:"https://github.com/natasha/corus#load_ud_syntag",target:"_blank",rel:"noopener noreferrer"}},[r._v("SynTagRus"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_morphoru_corpora",target:"_blank",rel:"noopener noreferrer"}},[r._v("OpenCorpora"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_morphoru_gicrya",target:"_blank",rel:"noopener noreferrer"}},[r._v("GICRYA"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_morphoru_rnc",target:"_blank",rel:"noopener noreferrer"}},[r._v("RNC"),e("OutboundLink")],1)]),r._v(" "),e("p",[e("a",{attrs:{href:"https://github.com/natasha/naeval#segtok",target:"_blank",rel:"noopener noreferrer"}},[r._v("SegTok"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/naeval#moses",target:"_blank",rel:"noopener noreferrer"}},[r._v("Moses"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/naeval#nltk",target:"_blank",rel:"noopener noreferrer"}},[r._v("NLTK"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/naeval#rutokenizer",target:"_blank",rel:"noopener noreferrer"}},[r._v("RuSentTokenizer"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/naeval#razdel",target:"_blank",rel:"noopener noreferrer"}},[r._v("Razdel"),e("OutboundLink")],1)]),r._v(" "),e("p",[r._v("Эмбеддинги")]),r._v(" "),e("p",[e("a",{attrs:{href:"https://github.com/natasha/corus#load_simlex",target:"_blank",rel:"noopener noreferrer"}},[r._v("SimLex965"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_russe_hj",target:"_blank",rel:"noopener noreferrer"}},[r._v("HJ"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_toloka_lrwc",target:"_blank",rel:"noopener noreferrer"}},[r._v("LRWC"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_russe_rt",target:"_blank",rel:"noopener noreferrer"}},[r._v("RT"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_russe_ae",target:"_blank",rel:"noopener noreferrer"}},[r._v("AE"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_russe_ae",target:"_blank",rel:"noopener noreferrer"}},[r._v("AE2"),e("OutboundLink")],1)]),r._v(" "),e("p",[e("a",{attrs:{href:"https://rusvectores.org/ru/models/",target:"_blank",rel:"noopener noreferrer"}},[r._v("RusVectores"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/navec",target:"_blank",rel:"noopener noreferrer"}},[r._v("Navec"),e("OutboundLink")],1)]),r._v(" "),e("p",[r._v("Анализ морфологии")]),r._v(" "),e("p",[e("a",{attrs:{href:"https://github.com/natasha/corus#load_gramru",target:"_blank",rel:"noopener noreferrer"}},[r._v("GramRuEval2020"),e("OutboundLink")],1),r._v(" (SynTagRus, GSD, Lenta.ru, Taiga)")]),r._v(" "),e("p",[e("a",{attrs:{href:"https://github.com/natasha/naeval#deeppavlov_morph",target:"_blank",rel:"noopener noreferrer"}},[r._v("DeepPavlov Morph"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/naeval#deeppavlov_bert_morph",target:"_blank",rel:"noopener noreferrer"}},[r._v("DeepPavlov BERT Morph"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/naeval#rupostagger",target:"_blank",rel:"noopener noreferrer"}},[r._v("RuPosTagger"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/naeval#rnnmorph",target:"_blank",rel:"noopener noreferrer"}},[r._v("RNNMorph"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/naeval#maru",target:"_blank",rel:"noopener noreferrer"}},[r._v("Maru"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/naeval#udpipe",target:"_blank",rel:"noopener noreferrer"}},[r._v("UDPipe"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/naeval#spacy",target:"_blank",rel:"noopener noreferrer"}},[r._v("SpaCy"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/naeval#stanza",target:"_blank",rel:"noopener noreferrer"}},[r._v("Stanza"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/naeval#slovnet_morph",target:"_blank",rel:"noopener noreferrer"}},[r._v("Slovnet Morph"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/naeval#slovnet_bert_morph",target:"_blank",rel:"noopener noreferrer"}},[r._v("Slovnet BERT Morph"),e("OutboundLink")],1)]),r._v(" "),e("p",[r._v("Анализ синтаксиса")]),r._v(" "),e("p",[e("a",{attrs:{href:"https://github.com/natasha/corus#load_gramru",target:"_blank",rel:"noopener noreferrer"}},[r._v("GramRuEval2020"),e("OutboundLink")],1),r._v(" (SynTagRus, GSD, Lenta.ru, Taiga)")]),r._v(" "),e("p",[e("a",{attrs:{href:"https://github.com/natasha/naeval#deeppavlov_bert_syntax",target:"_blank",rel:"noopener noreferrer"}},[r._v("DeepPavlov BERT Syntax"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/naeval#udpipe",target:"_blank",rel:"noopener noreferrer"}},[r._v("UDPipe"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/naeval#spacy",target:"_blank",rel:"noopener noreferrer"}},[r._v("SpaCy"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/naeval#stanza",target:"_blank",rel:"noopener noreferrer"}},[r._v("Stanza"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/naeval#slovnet_syntax",target:"_blank",rel:"noopener noreferrer"}},[r._v("Slovnet Syntax"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/naeval#slovnet_bert_syntax",target:"_blank",rel:"noopener noreferrer"}},[r._v("Slovnet BERT Syntax"),e("OutboundLink")],1)]),r._v(" "),e("p",[r._v("NER")]),r._v(" "),e("p",[e("a",{attrs:{href:"https://github.com/natasha/corus#load_factru",target:"_blank",rel:"noopener noreferrer"}},[r._v("factRuEval-2016"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_ne5",target:"_blank",rel:"noopener noreferrer"}},[r._v("Collection5"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_gareev",target:"_blank",rel:"noopener noreferrer"}},[r._v("Gareev"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_bsnlp",target:"_blank",rel:"noopener noreferrer"}},[r._v("BSNLP-2019"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_wikiner",target:"_blank",rel:"noopener noreferrer"}},[r._v("WiNER"),e("OutboundLink")],1)]),r._v(" "),e("p",[e("a",{attrs:{href:"https://github.com/natasha/naeval#deeppavlov_ner",target:"_blank",rel:"noopener noreferrer"}},[r._v("DeepPavlov NER"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/naeval#deeppavlov_bert_ner",target:"_blank",rel:"noopener noreferrer"}},[r._v("DeepPavlov BERT NER"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/naeval#deeppavlov_slavic_bert_ner",target:"_blank",rel:"noopener noreferrer"}},[r._v("DeepPavlov Slavic BERT NER"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/naeval#pullenti",target:"_blank",rel:"noopener noreferrer"}},[r._v("PullEnti"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/naeval#spacy",target:"_blank",rel:"noopener noreferrer"}},[r._v("SpaCy"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/naeval#stanza",target:"_blank",rel:"noopener noreferrer"}},[r._v("Stanza"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/naeval#texterra",target:"_blank",rel:"noopener noreferrer"}},[r._v("Texterra"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/naeval#tomita",target:"_blank",rel:"noopener noreferrer"}},[r._v("Tomita"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/naeval#mitie",target:"_blank",rel:"noopener noreferrer"}},[r._v("MITIE"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/naeval#slovnet_ner",target:"_blank",rel:"noopener noreferrer"}},[r._v("Slovnet NER"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/naeval#slovnet_bert_ner",target:"_blank",rel:"noopener noreferrer"}},[r._v("Slovnet BERT NER"),e("OutboundLink")],1)]),r._v(" "),e("p",[e("em",[r._v("Сетка решений и тестовых датасетов из "),e("a",{attrs:{href:"https://github.com/natasha/naeval",target:"_blank",rel:"noopener noreferrer"}},[r._v("репозитория Naeval"),e("OutboundLink")],1),r._v(". Инструменты проекта Natasha: "),e("a",{attrs:{href:"https://github.com/natasha/razdel",target:"_blank",rel:"noopener noreferrer"}},[r._v("Razdel"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/navec",target:"_blank",rel:"noopener noreferrer"}},[r._v("Navec"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/slovnet",target:"_blank",rel:"noopener noreferrer"}},[r._v("Slovnet"),e("OutboundLink")],1),r._v(".")])]),r._v(" "),e("p",[r._v("Дальше подробнее рассмотрим задачу NER.")]),r._v(" "),e("h3",{attrs:{id:"датасеты"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#датасеты"}},[r._v("#")]),r._v(" Датасеты")]),r._v(" "),e("p",[r._v("Для русскоязычного NER существует 5 публичных бенчмарков: "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_factru",target:"_blank",rel:"noopener noreferrer"}},[r._v("factRuEval-2016"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_ne5",target:"_blank",rel:"noopener noreferrer"}},[r._v("Collection5"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_gareev",target:"_blank",rel:"noopener noreferrer"}},[r._v("Gareev"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_bsnlp",target:"_blank",rel:"noopener noreferrer"}},[r._v("BSNLP-2019"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/corus#load_wikiner",target:"_blank",rel:"noopener noreferrer"}},[r._v("WiNER"),e("OutboundLink")],1),r._v(". Ссылки на источники собраны в "),e("a",{attrs:{href:"https://github.com/natasha/corus",target:"_blank",rel:"noopener noreferrer"}},[r._v("реестре Corus"),e("OutboundLink")],1),r._v(". Все датасеты состоят из новостных статей, в текстах отмечены подстроки с именами, названиями организаций и топонимов. Что может быть проще?")]),r._v(" "),e("p",[r._v("У всех источников разный формат разметки. Collection5 использует "),e("a",{attrs:{href:"https://brat.nlplab.org/standoff.html",target:"_blank",rel:"noopener noreferrer"}},[r._v("Standoff-формат"),e("OutboundLink")],1),r._v(" утилиты "),e("a",{attrs:{href:"https://brat.nlplab.org/index.html",target:"_blank",rel:"noopener noreferrer"}},[r._v("Brat"),e("OutboundLink")],1),r._v(", Gareev и WiNER — разные диалекты "),e("a",{attrs:{href:"https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)",target:"_blank",rel:"noopener noreferrer"}},[r._v("BIO-разметки"),e("OutboundLink")],1),r._v(", у BSNLP-2019 "),e("a",{attrs:{href:"http://bsnlp.cs.helsinki.fi/Guidelines_20190122.pdf",target:"_blank",rel:"noopener noreferrer"}},[r._v("свой формат"),e("OutboundLink")],1),r._v(", у factRuEval-2016 тоже "),e("a",{attrs:{href:"https://github.com/dialogue-evaluation/factRuEval-2016#%D1%84%D0%BE%D1%80%D0%BC%D0%B0%D1%82-%D0%B4%D0%B5%D0%BC%D0%BE%D0%BD%D1%81%D1%82%D1%80%D0%B0%D1%86%D0%B8%D0%BE%D0%BD%D0%BD%D0%BE%D0%B9-%D1%80%D0%B0%D0%B7%D0%BC%D0%B5%D1%82%D0%BA%D0%B8",target:"_blank",rel:"noopener noreferrer"}},[r._v("своя нетривиальная спецификация"),e("OutboundLink")],1),r._v(". Naeval приводит все источники к общему формату. Разметка состоит из спанов. Спан — тройка: тип сущности, начало и конец подстроки.")]),r._v(" "),e("p",[r._v("Типы сущностей. factRuEval-2016 и Collection5 отдельно помечают полутопонимы-полуорганизации: «Кремль», «ЕС», «СССР». BSNLP-2019 и WiNER выделяют названия событий: «Чемпионат России», «Брексит». Naeval адаптирует и удаляет часть меток, оставляет эталонные метки PER, LOC, ORG: имена людей, названия топонимов и организаций.")]),r._v(" "),e("p",[r._v("Вложенные спаны. В factRuEval-2016 спаны пересекаются. Naeval упрощает разметку:")]),r._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[r._v('Было:\nТеперь, как утверждают в Х5 Retail Group, куда входят \n                         org_name───────              \n                         Org────────────              \nсети магазинов "Пятерочка", "Перекресток" и "Карусель",\norg_descr─────  org_name─    org_name───     org_name  \nOrg──────────────────────                              \norg_descr─────                                         \nOrg─────────────────────────────────────               \norg_descr─────                                         \nOrg──────────────────────────────────────────────────  \nо повышении цен сообщили два поставщика рыбы и \nморепродуктов и компания, поставляющая овощи и фрукты.\n\nСтало:\nТеперь, как утверждают в Х5 Retail Group, куда входят\n                         ORG────────────\nсети магазинов "Пятерочка", "Перекресток" и "Карусель",\n                ORG──────    ORG────────     ORG─────     \n\nо повышении цен сообщили два поставщика рыбы и\nморепродуктов и компания, поставляющая овощи и фрукты.\n')])])]),e("h3",{attrs:{id:"модели"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#модели"}},[r._v("#")]),r._v(" Модели")]),r._v(" "),e("p",[r._v("Naeval сравнивает 12 открытых решений задачи NER для русского языка. Все инструменты "),e("a",{attrs:{href:"https://github.com/natasha/naeval/tree/master/docker",target:"_blank",rel:"noopener noreferrer"}},[r._v("завёрнуты в Docker-контейнеры"),e("OutboundLink")],1),r._v(" с веб-интерфейсом:")]),r._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[r._v('$ docker run -p 8080:8080 natasha/tomita-algfio\n2020-07-02 11:09:19 BIN: \'tomita-linux64\', CONFIG: \'algfio\'\n2020-07-02 11:09:19 Listening http://0.0.0.0:8080\t    \n\n$ curl -X POST http://localhost:8080 --data \\\n  \'Глава государства Дмитрий Медведев и Председатель \\\n  Правительства РФ Владимир Путин выразили глубочайшие \\\n  соболезнования семье актрисы\'\n\n<document url="" di="5" bi="-1" date="2020-07-02">\n   <facts>\n      <Person pos="18" len="16" sn="0" fw="2" lw="3">\n         <Name_Surname val="МЕДВЕДЕВ" />\n         <Name_FirstName val="ДМИТРИЙ" />\n         <Name_SurnameIsDictionary val="1" />\n      </Person>\n      <Person pos="67" len="14" sn="0" fw="8" lw="9">\n         <Name_Surname val="ПУТИН" />\n         <Name_FirstName val="ВЛАДИМИР" />\n         <Name_SurnameIsDictionary val="1" />\n      </Person>\n   </facts>\n</document>\n')])])]),e("p",[r._v("Некоторые решения так тяжело запустить и настроить, что мало людей ими пользуется. "),e("a",{attrs:{href:"http://pullenti.ru/",target:"_blank",rel:"noopener noreferrer"}},[r._v("PullEnti"),e("OutboundLink")],1),r._v(" — сложная система, построенная на правилах, заняла первой место на конкурсе factRuEval в 2016 году. Инструмент распространяется в виде SDK для C#. Работа над Naeval вылилась в "),e("a",{attrs:{href:"http://github.com/pullenti",target:"_blank",rel:"noopener noreferrer"}},[r._v("отдельный проект"),e("OutboundLink")],1),r._v(" с набором обёрток для PullEnti: "),e("a",{attrs:{href:"https://github.com/pullenti/PullentiServer",target:"_blank",rel:"noopener noreferrer"}},[r._v("PullentiServer"),e("OutboundLink")],1),r._v(" — веб-сервер на С#, "),e("a",{attrs:{href:"https://github.com/pullenti/pullenti-client",target:"_blank",rel:"noopener noreferrer"}},[r._v("pullenti-client"),e("OutboundLink")],1),r._v(" — Python-клиент для PullentiServer:")]),r._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[r._v("$ docker run -p 8080:8080 pullenti/pullenti-server\n2020-07-02 11:42:02 [INFO] Init Pullenti v3.21 ...\n2020-07-02 11:42:02 [INFO] Load lang: ru, en\n2020-07-02 11:42:03 [INFO] Load analyzer: geo, org, person\n2020-07-02 11:42:05 [INFO] Listen prefix: http://*:8080/\n\n>>> from pullenti_client import Client\n\n>>> client = Client('localhost', 8080)\n>>> text = 'Глава государства Дмитрий Медведев и ' \\\n...  'Председатель Правительства РФ Владимир Путин ' \\\n...  'выразили глубочайшие соболезнования семье актрисы'\n>>> result = client(text)\n>>> result.graph\n")])])]),e("p",[e("img",{attrs:{src:"https://habrastorage.org/r/w1560/webt/nr/lt/c_/nrltc_h9ecxeodnar5r1wwzgcbc.png",alt:""}}),e("br"),r._v("\nФормат разметки у всех инструментов немного отличается. Naeval загружает результаты, адаптирует типы сущностей, упрощает структуру спанов:")]),r._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[r._v("Было (PullEnti):\nНапомним, парламент Южной Осетии на состоявшемся 19 декабря \n          ORGANIZATION──────────                            \n                    GEO─────────                            \nзаседании одобрил представление президента Республики \n                                PERSON────────────────\n                                PERSONPROPERTY─────── \nЛеонида Тибилова об отставке председателя Верховного суда \n────────────────             PERSON───────────────────────\n                             PERSONPROPERTY────────────── \n                                          ORGANIZATION─── \nАцамаза Биченова.\n──────────────── \n\nСтало:\nНапомним, парламент Южной Осетии на состоявшемся 19 декабря \n          ORG────── LOC─────────\nзаседании одобрил представление президента Республики \nЛеонида Тибилова об отставке председателя Верховного суда \nPER─────────────                          ORG────────────\nАцамаза Биченова.\nPER─────────────\n")])])]),e("p",[e("em",[r._v("Результат работы PullEnti сложнее адаптировать, чем разметку factRuEval-2016. Алгоритм убирает тег PERSONPROPERTY, разбивает вложенные PERSON, ORGANIZATION и GEO на непересекающиеся PER, LOC, ORG.")])]),r._v(" "),e("h3",{attrs:{id:"сравнение"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#сравнение"}},[r._v("#")]),r._v(" Сравнение")]),r._v(" "),e("p",[r._v("Для каждой пары «модель, датасет» Naeval вычисляет "),e("a",{attrs:{href:"https://github.com/natasha/naeval/blob/master/naeval/ner/score.py",target:"_blank",rel:"noopener noreferrer"}},[r._v("F1-меру по токенам"),e("OutboundLink")],1),r._v(", публикует "),e("a",{attrs:{href:"https://github.com/natasha/naeval#ner",target:"_blank",rel:"noopener noreferrer"}},[r._v("таблицу с оценками качества"),e("OutboundLink")],1),r._v(".")]),r._v(" "),e("p",[r._v("Natasha — не научный проект, для нас важна практичность решения. Naeval измеряет время старта, скорость работы, размер модели и потребление RAM. "),e("a",{attrs:{href:"https://github.com/natasha/naeval#ner",target:"_blank",rel:"noopener noreferrer"}},[r._v("Таблица с результатами в репозитории"),e("OutboundLink")],1),r._v(".")]),r._v(" "),e("p",[r._v("Мы подготовили датасеты, завернули 20 систем в Docker-контейнеры и посчитали метрики для 5 других задач русскоязычного NLP, результаты в репозитории Naeval: "),e("a",{attrs:{href:"https://github.com/natasha/naeval#tokenization",target:"_blank",rel:"noopener noreferrer"}},[r._v("токенизация"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/naeval#sentence-segmentation",target:"_blank",rel:"noopener noreferrer"}},[r._v("сегментация на предложения"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/naeval#pretrained-embeddings",target:"_blank",rel:"noopener noreferrer"}},[r._v("эмбеддинги"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/naeval#morphology-taggers",target:"_blank",rel:"noopener noreferrer"}},[r._v("анализ морфологии"),e("OutboundLink")],1),r._v(" и "),e("a",{attrs:{href:"https://github.com/natasha/naeval#syntax-parser",target:"_blank",rel:"noopener noreferrer"}},[r._v("синтаксиса"),e("OutboundLink")],1),r._v(".")]),r._v(" "),e("h2",{attrs:{id:"yargy-парсер-извлечение-структурированное-информации-из-текстов-на-русском-языке-с-помощью-грамматик-и-словареи"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#yargy-парсер-извлечение-структурированное-информации-из-текстов-на-русском-языке-с-помощью-грамматик-и-словареи"}},[r._v("#")]),r._v(" Yargy-парсер — извлечение структурированное информации из текстов на русском языке с помощью грамматик и словарей")]),r._v(" "),e("p",[e("img",{attrs:{src:"https://habrastorage.org/r/w1560/webt/d_/xt/dp/d_xtdp-kcw4iayyjkro-kdv7sli.png",alt:""}})]),r._v(" "),e("p",[e("a",{attrs:{href:"https://github.com/natasha/yargy",target:"_blank",rel:"noopener noreferrer"}},[r._v("Yargy-парсер"),e("OutboundLink")],1),r._v(" — аналог яндексового "),e("a",{attrs:{href:"https://tech.yandex.ru/tomita/",target:"_blank",rel:"noopener noreferrer"}},[r._v("Томита-парсера"),e("OutboundLink")],1),r._v(" для Python. "),e("a",{attrs:{href:"https://github.com/natasha/yargy#install",target:"_blank",rel:"noopener noreferrer"}},[r._v("Инструкция по установке"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/yargy#usage",target:"_blank",rel:"noopener noreferrer"}},[r._v("пример использования"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/yargy#documentation",target:"_blank",rel:"noopener noreferrer"}},[r._v("документация"),e("OutboundLink")],1),r._v(" в репозитории Yargy. Правила для извлечения сущностей описываются с помощью "),e("a",{attrs:{href:"https://ru.wikipedia.org/wiki/%D0%9A%D0%BE%D0%BD%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%BD%D0%BE-%D1%81%D0%B2%D0%BE%D0%B1%D0%BE%D0%B4%D0%BD%D0%B0%D1%8F_%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%B0%D1%82%D0%B8%D0%BA%D0%B0",target:"_blank",rel:"noopener noreferrer"}},[r._v("контекстно-свободных грамматик"),e("OutboundLink")],1),r._v(" и словарей. Два года назад я писал на Хабр "),e("a",{attrs:{href:"https://habr.com/ru/post/349864/",target:"_blank",rel:"noopener noreferrer"}},[r._v("статью про Yargy и библиотеку Natasha"),e("OutboundLink")],1),r._v(", рассказывал про решение задачи NER для русского языка. Проект хорошо приняли. Yargy-парсер заменил Томиту в крупных проектах внутри Сбера, Интерфакса и РИА Новостей. Появилось много образовательных материалов. Большое видео с воркшопа в Яндексе, полтора часа про процесс разработки грамматик на примерах:")]),r._v(" "),e("p",[r._v("Обновилась документация, я причесал "),e("a",{attrs:{href:"https://nbviewer.jupyter.org/github/natasha/yargy/blob/master/docs/index.ipynb",target:"_blank",rel:"noopener noreferrer"}},[r._v("вводный раздел"),e("OutboundLink")],1),r._v(" и "),e("a",{attrs:{href:"https://nbviewer.jupyter.org/github/natasha/yargy/blob/master/docs/ref.ipynb",target:"_blank",rel:"noopener noreferrer"}},[r._v("справочник"),e("OutboundLink")],1),r._v(". Главное, появился "),e("a",{attrs:{href:"https://nbviewer.jupyter.org/github/natasha/yargy/blob/master/docs/cookbook.ipynb",target:"_blank",rel:"noopener noreferrer"}},[r._v("Cookbook"),e("OutboundLink")],1),r._v(" — раздел с полезными практиками. Там собраны ответы на самые частые вопросы из "),e("a",{attrs:{href:"https://t.me/natural_language_processing",target:"_blank",rel:"noopener noreferrer"}},[r._v("t.me/natural_language_processing"),e("OutboundLink")],1),r._v(":")]),r._v(" "),e("ul",[e("li",[e("a",{attrs:{href:"https://nbviewer.jupyter.org/github/natasha/yargy/blob/master/docs/cookbook.ipynb#%D0%9F%D1%80%D0%BE%D0%BF%D1%83%D1%81%D1%82%D0%B8%D1%82%D1%8C-%D1%87%D0%B0%D1%81%D1%82%D1%8C-%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%B0",target:"_blank",rel:"noopener noreferrer"}},[r._v("как пропустить часть текста"),e("OutboundLink")],1),r._v(";")]),r._v(" "),e("li",[e("a",{attrs:{href:"https://nbviewer.jupyter.org/github/natasha/yargy/blob/master/docs/cookbook.ipynb#IdTokenizer",target:"_blank",rel:"noopener noreferrer"}},[r._v("как подать на вход токены, а не текст"),e("OutboundLink")],1),r._v(";")]),r._v(" "),e("li",[e("a",{attrs:{href:"https://nbviewer.jupyter.org/github/natasha/yargy/blob/master/docs/cookbook.ipynb#CappedParser",target:"_blank",rel:"noopener noreferrer"}},[r._v("что делать, если парсер тормозит"),e("OutboundLink")],1),r._v(".")])]),r._v(" "),e("p",[r._v("Yargy-парсер — сложный инструмент. В Cookbook описаны неочевидные моменты, который всплывают при работе с большими наборами правил:")]),r._v(" "),e("ul",[e("li",[e("a",{attrs:{href:"https://nbviewer.jupyter.org/github/natasha/yargy/blob/master/docs/cookbook.ipynb#%D0%9F%D0%BE%D1%80%D1%8F%D0%B4%D0%BE%D0%BA-%D0%B0%D1%80%D0%B3%D1%83%D0%BC%D0%B5%D0%BD%D1%82%D0%BE%D0%B2-%D0%B2-or_-%D0%B8%D0%BC%D0%B5%D0%B5%D1%82-%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D0%B5",target:"_blank",rel:"noopener noreferrer"}},[r._v("порядок аргументов в or_"),e("OutboundLink")],1),r._v(";")]),r._v(" "),e("li",[e("a",{attrs:{href:"https://nbviewer.jupyter.org/github/natasha/yargy/blob/master/docs/cookbook.ipynb#%D0%9D%D0%B5%D0%BE%D0%B4%D0%BD%D0%BE%D0%B7%D0%BD%D0%B0%D1%8F-%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%B0%D1%82%D0%B8%D0%BA%D0%B0",target:"_blank",rel:"noopener noreferrer"}},[r._v("неоднозначные грамматики"),e("OutboundLink")],1),r._v(";")]),r._v(" "),e("li",[e("a",{attrs:{href:"https://nbviewer.jupyter.org/github/natasha/yargy/blob/master/docs/cookbook.ipynb#%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5-%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5-%D0%B8-Yargy",target:"_blank",rel:"noopener noreferrer"}},[r._v("зачем аргумент tagger в Parser"),e("OutboundLink")],1),r._v(".")])]),r._v(" "),e("p",[r._v("У нас в лабе на Yargy работает несколько крупных сервисов. Перечитал код, собрал в Cookbook паттерны, которые не описаны в паблике:")]),r._v(" "),e("ul",[e("li",[e("a",{attrs:{href:"https://nbviewer.jupyter.org/github/natasha/yargy/blob/master/docs/cookbook.ipynb#%D0%93%D0%B5%D0%BD%D0%B5%D1%80%D0%B0%D1%86%D0%B8%D1%8F-%D0%BF%D1%80%D0%B0%D0%B2%D0%B8%D0%BB",target:"_blank",rel:"noopener noreferrer"}},[r._v("генерация правил"),e("OutboundLink")],1),r._v(";")]),r._v(" "),e("li",[e("a",{attrs:{href:"https://nbviewer.jupyter.org/github/natasha/yargy/blob/master/docs/cookbook.ipynb#%D0%9D%D0%B0%D1%81%D0%BB%D0%B5%D0%B4%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5-fact",target:"_blank",rel:"noopener noreferrer"}},[r._v("наследование fact"),e("OutboundLink")],1),r._v(" (особенно полезно, ни одно решение на практике без этого приёма не обходится).")])]),r._v(" "),e("p",[r._v("После прочтения документации полезно посмотреть "),e("a",{attrs:{href:"https://github.com/natasha/yargy-examples",target:"_blank",rel:"noopener noreferrer"}},[r._v("репозиторий с примерами"),e("OutboundLink")],1),r._v(":")]),r._v(" "),e("ul",[e("li",[e("a",{attrs:{href:"https://github.com/natasha/yargy-examples/blob/master/02_console/notes.ipynb",target:"_blank",rel:"noopener noreferrer"}},[r._v("парсинг объявлений с Авито"),e("OutboundLink")],1),r._v(";")]),r._v(" "),e("li",[e("a",{attrs:{href:"https://github.com/natasha/yargy-examples/blob/master/04_food/notes.ipynb",target:"_blank",rel:"noopener noreferrer"}},[r._v("разбор рецептов из ВК"),e("OutboundLink")],1),r._v(".")])]),r._v(" "),e("p",[r._v("Ещё в проекте Natasha есть репозиторий "),e("a",{attrs:{href:"https://github.com/natasha/natasha-usage",target:"_blank",rel:"noopener noreferrer"}},[r._v("natasha-usage"),e("OutboundLink")],1),r._v(". Туда попадает код пользователей Yargy-парсера, опубликованный на Github. 80% ссылок учебные проекты, но есть и содержательные примеры:")]),r._v(" "),e("ul",[e("li",[e("a",{attrs:{href:"https://github.com/xamgore/spbmetro",target:"_blank",rel:"noopener noreferrer"}},[r._v("разбор фида о работе метро в Спб"),e("OutboundLink")],1),r._v(";")]),r._v(" "),e("li",[e("a",{attrs:{href:"https://github.com/AlexSkrn/yargy_flats_parser/blob/master/yargy_flats_parser.ipynb",target:"_blank",rel:"noopener noreferrer"}},[r._v("парсинг объявлений о сдаче жилья в соцсетях"),e("OutboundLink")],1),r._v(";")]),r._v(" "),e("li",[e("a",{attrs:{href:"https://github.com/rokku3kpvc/yargy-tires/blob/master/tires_parser.py",target:"_blank",rel:"noopener noreferrer"}},[r._v("извлечение атрибутов из названий авто покрышек"),e("OutboundLink")],1),r._v(";")]),r._v(" "),e("li",[e("a",{attrs:{href:"https://github.com/AndreyKolomiets/ods_jobs_analytics/blob/master/extractors/position_extractor.py",target:"_blank",rel:"noopener noreferrer"}},[r._v("парсинг вакансий из канала jobs чата ODS"),e("OutboundLink")],1),r._v(";")])]),r._v(" "),e("p",[r._v("Самые интересные кейсы использования Yargy-парсера, конечно, не публикуют открыто на Github. Напишите в личку, если компания использует Yargy и, если не против, добавим ваше лого на "),e("a",{attrs:{href:"http://natasha.github.io/",target:"_blank",rel:"noopener noreferrer"}},[r._v("natasha.github.io"),e("OutboundLink")],1),r._v(".")]),r._v(" "),e("h2",{attrs:{id:"ipymarkup-визуализация-разметки-именованных-сущностеи-и-синтаксических-связеи"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#ipymarkup-визуализация-разметки-именованных-сущностеи-и-синтаксических-связеи"}},[r._v("#")]),r._v(" Ipymarkup — визуализация разметки именованных сущностей и синтаксических связей")]),r._v(" "),e("p",[e("img",{attrs:{src:"https://habrastorage.org/r/w1560/webt/gw/p9/lp/gwp9lpwboykhneqqwinufvivdju.png",alt:""}}),e("br"),r._v(" "),e("a",{attrs:{href:"https://github.com/natasha/ipymarkup",target:"_blank",rel:"noopener noreferrer"}},[r._v("Ipymarkup"),e("OutboundLink")],1),r._v(" — примитивная библиотека, нужна для подсветки подстрок в тексте, визуализации NER. "),e("a",{attrs:{href:"https://github.com/natasha/ipymarkup#install",target:"_blank",rel:"noopener noreferrer"}},[r._v("Инструкция по установке"),e("OutboundLink")],1),r._v(", "),e("a",{attrs:{href:"https://github.com/natasha/ipymarkup#usage",target:"_blank",rel:"noopener noreferrer"}},[r._v("пример использования"),e("OutboundLink")],1),r._v(" в репозитории Ipymarkup. Библиотека похожа на "),e("a",{attrs:{href:"https://explosion.ai/demos/displacy",target:"_blank",rel:"noopener noreferrer"}},[r._v("displaCy"),e("OutboundLink")],1),r._v(" и "),e("a",{attrs:{href:"https://explosion.ai/demos/displacy-ent",target:"_blank",rel:"noopener noreferrer"}},[r._v("displaCy ENT"),e("OutboundLink")],1),r._v(", бесценна при отладке грамматик для Yargy-парсера.")]),r._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[r._v(">>> from yargy import Parser\n>>> from ipymarkup import show_span_box_markup as show_markup\n\n>>> parser = Parser(...)\n>>> text = '...'\n>>> matches = parser.findall(text)\n>>> spans = [_.span for _ in matches]\n>>> show_markup(text, spans)\n")])])]),e("p",[e("img",{attrs:{src:"https://habrastorage.org/r/w1560/webt/2c/vq/zn/2cvqznnj3s9gr2mauhfuxx6drai.png",alt:""}})]),r._v(" "),e("p",[r._v("В проекте Natasha появилось "),e("a",{attrs:{href:"https://github.com/natasha/slovnet#syntax",target:"_blank",rel:"noopener noreferrer"}},[r._v("решение задачи синтаксического разбора"),e("OutboundLink")],1),r._v(". Понадобилось не только выделять слова в тексте, но и рисовать между ними стрелочки. Существует масса готовых решений, есть даже "),e("a",{attrs:{href:"https://www.aclweb.org/anthology/L18-1091.pdf",target:"_blank",rel:"noopener noreferrer"}},[r._v("научная статья по теме"),e("OutboundLink")],1),r._v(".")]),r._v(" "),e("p",[e("img",{attrs:{src:"https://habrastorage.org/r/w1560/webt/dm/pt/ym/dmptym0rlzlligd6uuidnpxplke.png",alt:""}})]),r._v(" "),e("p",[r._v("Конечно, ничего из существующего не подошло, и однажды я конкретно заморочился, применил всю известную магию CSS и HTML, добавил новую визуализацию в Ipymarkup. "),e("a",{attrs:{href:"https://nbviewer.jupyter.org/github/natasha/ipymarkup/blob/master/docs.ipynb#Syntax-tree",target:"_blank",rel:"noopener noreferrer"}},[r._v("Инструкция по использованию"),e("OutboundLink")],1),r._v(" в доке.")]),r._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[r._v(">>> from ipymarkup import show_dep_markup\n\n>>> words = ['В', 'советский', 'период', 'времени', 'число', 'ИТ', '-', 'специалистов', 'в', 'Армении', 'составляло', 'около', 'десяти', 'тысяч', '.']\n>>> deps = [(2, 0, 'case'), (2, 1, 'amod'), (10, 2, 'obl'), (2, 3, 'nmod'), (10, 4, 'obj'), (7, 5, 'compound'), (5, 6, 'punct'), (4, 7, 'nmod'), (9, 8, 'case'), (4, 9, 'nmod'), (13, 11, 'case'), (13, 12, 'nummod'), (10, 13, 'nsubj'), (10, 14, 'punct')]\n>>> show_dep_markup(words, deps)\n")])])]),e("p",[e("img",{attrs:{src:"https://habrastorage.org/r/w1560/webt/0z/lz/wm/0zlzwmyl-zdfrkg5xaumhb-3b8o.png",alt:""}})]),r._v(" "),e("p",[r._v("Теперь в "),e("a",{attrs:{href:"https://github.com/natasha/natasha",target:"_blank",rel:"noopener noreferrer"}},[r._v("Natasha"),e("OutboundLink")],1),r._v(" и "),e("a",{attrs:{href:"https://github.com/natasha/nerus",target:"_blank",rel:"noopener noreferrer"}},[r._v("Nerus"),e("OutboundLink")],1),r._v(" удобно смотреть результаты синтаксического разбора.")]),r._v(" "),e("p",[e("img",{attrs:{src:"https://habrastorage.org/r/w1560/webt/u2/kd/qf/u2kdqfgbxcioedy6d7bcpgyz3kg.png",alt:""}})])])}),[],!1,null,null,null);e.default=n.exports}}]);