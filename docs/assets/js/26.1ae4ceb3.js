(window.webpackJsonp=window.webpackJsonp||[]).push([[26],{298:function(t,e,a){"use strict";a.r(e);var o=a(14),r=Object(o.a)({},(function(){var t=this,e=t._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("p",[t._v("https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model#4-bit-mode")]),t._v(" "),e("p",[t._v("LLaMA is a Large Language Model developed by Meta AI.")]),t._v(" "),e("p",[t._v("It was trained on more tokens than previous models. The result is that the smallest version with 7 billion parameters has similar performance to GPT-3 with 175 billion parameters.")]),t._v(" "),e("h2",{attrs:{id:"_4-bit-mode"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_4-bit-mode"}},[t._v("#")]),t._v(" "),e("a",{attrs:{href:"https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model#4-bit-mode",target:"_blank",rel:"noopener noreferrer"}},[e("OutboundLink")],1),t._v("4-bit mode")]),t._v(" "),e("p",[t._v("In 4-bit mode, the LLaMA models are loaded with just 25% of their regular VRAM usage. So LLaMA-7B fits into a 6GB GPU, and LLaMA-30B fits into a 24GB GPU.")]),t._v(" "),e("p",[t._v("This is possible thanks to "),e("a",{attrs:{href:"https://github.com/qwopqwop200/GPTQ-for-LLaMa",target:"_blank",rel:"noopener noreferrer"}},[t._v("@qwopqwop200"),e("OutboundLink")],1),t._v("'s adaptation of the GPTQ algorithm for LLaMA: "),e("a",{attrs:{href:"https://github.com/qwopqwop200/GPTQ-for-LLaMa",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://github.com/qwopqwop200/GPTQ-for-LLaMa"),e("OutboundLink")],1)]),t._v(" "),e("p",[t._v("GPTQ is a clever quantization algorithm that lightly reoptimizes the weights during quantization so that the accuracy loss is compensated relative to a round-to-nearest quantization. See the paper for more details: "),e("a",{attrs:{href:"https://arxiv.org/abs/2210.17323",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://arxiv.org/abs/2210.17323"),e("OutboundLink")],1)]),t._v(" "),e("h3",{attrs:{id:"installation"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#installation"}},[t._v("#")]),t._v(" "),e("a",{attrs:{href:"https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model#installation",target:"_blank",rel:"noopener noreferrer"}},[e("OutboundLink")],1),t._v("Installation")]),t._v(" "),e("h4",{attrs:{id:"step-0-install-nvcc"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#step-0-install-nvcc"}},[t._v("#")]),t._v(" "),e("a",{attrs:{href:"https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model#step-0-install-nvcc",target:"_blank",rel:"noopener noreferrer"}},[e("OutboundLink")],1),t._v("Step 0: install nvcc")]),t._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("conda activate textgen\nconda install -c conda-forge cudatoolkit-dev\n")])])]),e("p",[t._v("The command above takes some 10 minutes to run and shows no progress bar or updates along the way.")]),t._v(" "),e("p",[t._v("See this issue for more details: "),e("a",{attrs:{href:"https://github.com/oobabooga/text-generation-webui/issues/416#issuecomment-1475078571",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://github.com/oobabooga/text-generation-webui/issues/416#issuecomment-1475078571"),e("OutboundLink")],1)]),t._v(" "),e("h4",{attrs:{id:"step-1-install-gptq-for-llama"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#step-1-install-gptq-for-llama"}},[t._v("#")]),t._v(" "),e("a",{attrs:{href:"https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model#step-1-install-gptq-for-llama",target:"_blank",rel:"noopener noreferrer"}},[e("OutboundLink")],1),t._v("Step 1: install GPTQ-for-LLaMa")]),t._v(" "),e("p",[t._v("Clone "),e("a",{attrs:{href:"https://github.com/qwopqwop200/GPTQ-for-LLaMa",target:"_blank",rel:"noopener noreferrer"}},[t._v("GPTQ-for-LLaMa"),e("OutboundLink")],1),t._v(" into the "),e("code",[t._v("text-generation-webui/repositories")]),t._v(" subfolder and install it:")]),t._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("mkdir repositories\ncd repositories\ngit clone https://github.com/qwopqwop200/GPTQ-for-LLaMa\ncd GPTQ-for-LLaMa\npython setup_cuda.py install\n")])])]),e("p",[t._v("You are going to need to have a C++ compiler installed into your system for the last command. On Linux, "),e("code",[t._v("sudo apt install build-essential")]),t._v(" or equivalent is enough.")]),t._v(" "),e("h4",{attrs:{id:"step-2-get-the-pre-converted-weights"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#step-2-get-the-pre-converted-weights"}},[t._v("#")]),t._v(" "),e("a",{attrs:{href:"https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model#step-2-get-the-pre-converted-weights",target:"_blank",rel:"noopener noreferrer"}},[e("OutboundLink")],1),t._v("Step 2: get the pre-converted weights")]),t._v(" "),e("ul",[e("li",[t._v("Converted without "),e("code",[t._v("group-size")]),t._v(" (better for the 7b model): "),e("a",{attrs:{href:"https://github.com/oobabooga/text-generation-webui/pull/530#issuecomment-1483891617",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://github.com/oobabooga/text-generation-webui/pull/530#issuecomment-1483891617"),e("OutboundLink")],1)]),t._v(" "),e("li",[t._v("Converted with "),e("code",[t._v("group-size")]),t._v(" (better from 13b upwards): "),e("a",{attrs:{href:"https://github.com/oobabooga/text-generation-webui/pull/530#issuecomment-1483941105",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://github.com/oobabooga/text-generation-webui/pull/530#issuecomment-1483941105"),e("OutboundLink")],1)])]),t._v(" "),e("h4",{attrs:{id:"step-3-start-the-web-ui"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#step-3-start-the-web-ui"}},[t._v("#")]),t._v(" "),e("a",{attrs:{href:"https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model#step-3-start-the-web-ui",target:"_blank",rel:"noopener noreferrer"}},[e("OutboundLink")],1),t._v("Step 3: Start the web UI:")]),t._v(" "),e("p",[t._v("For the models converted without "),e("code",[t._v("group-size")]),t._v(":")]),t._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("python server.py --model llama-7b-4bit --wbits 4 \n")])])]),e("p",[t._v("For the models converted with "),e("code",[t._v("group-size")]),t._v(":")]),t._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("python server.py --model llama-13b-4bit-128g --wbits 4 --groupsize 128 \n")])])]),e("h3",{attrs:{id:"cpu-offloading"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#cpu-offloading"}},[t._v("#")]),t._v(" "),e("a",{attrs:{href:"https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model#cpu-offloading",target:"_blank",rel:"noopener noreferrer"}},[e("OutboundLink")],1),t._v("CPU offloading")]),t._v(" "),e("p",[t._v("It is possible to offload part of the layers of the 4-bit model to the CPU with the "),e("code",[t._v("--pre_layer")]),t._v(" flag. The higher the number after "),e("code",[t._v("--pre_layer")]),t._v(", the more layers will be allocated to the GPU.")]),t._v(" "),e("p",[t._v("With this command, I can run llama-7b with 4GB VRAM:")]),t._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("python server.py --model llama-7b-4bit --wbits 4 --pre_layer 20\n")])])]),e("p",[t._v("This is the performance:")]),t._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("Output generated in 123.79 seconds (1.61 tokens/s, 199 tokens)\n")])])]),e("h2",{attrs:{id:"hugging-face-format-weights"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#hugging-face-format-weights"}},[t._v("#")]),t._v(" "),e("a",{attrs:{href:"https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model#hugging-face-format-weights",target:"_blank",rel:"noopener noreferrer"}},[e("OutboundLink")],1),t._v("Hugging Face format weights")]),t._v(" "),e("p",[t._v("For regular 16-bit/8-bit inference.")]),t._v(" "),e("h3",{attrs:{id:"getting-the-weights"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#getting-the-weights"}},[t._v("#")]),t._v(" "),e("a",{attrs:{href:"https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model#getting-the-weights",target:"_blank",rel:"noopener noreferrer"}},[e("OutboundLink")],1),t._v("Getting the weights")]),t._v(" "),e("h4",{attrs:{id:"option-1-from-decapoda-research-easy-way"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#option-1-from-decapoda-research-easy-way"}},[t._v("#")]),t._v(" "),e("a",{attrs:{href:"https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model#option-1-from-decapoda-research-easy-way",target:"_blank",rel:"noopener noreferrer"}},[e("OutboundLink")],1),t._v("Option 1: from Decapoda Research (easy way)")]),t._v(" "),e("p",[t._v("Simply run one of these commands to get the pre-converted weights:")]),t._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("python download-model.py decapoda-research/llama-7b-hf\npython download-model.py decapoda-research/llama-13b-hf\n")])])]),e("blockquote",[e("p",[e("strong",[t._v("Note")])]),t._v(" "),e("p",[t._v("You need to manually replace "),e("code",[t._v("LLaMATokenizer")]),t._v(" with "),e("code",[t._v("LlamaTokenizer")]),t._v(" in the "),e("code",[t._v("tokenizer_config.json")]),t._v(" file that will be downloaded.")])]),t._v(" "),e("h4",{attrs:{id:"option-2-convert-the-weights-yourself"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#option-2-convert-the-weights-yourself"}},[t._v("#")]),t._v(" "),e("a",{attrs:{href:"https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model#option-2-convert-the-weights-yourself",target:"_blank",rel:"noopener noreferrer"}},[e("OutboundLink")],1),t._v("Option 2: convert the weights yourself")]),t._v(" "),e("ol",[e("li",[t._v("Use the script below to convert the model in "),e("code",[t._v(".pth")]),t._v(" format that you, a fellow academic, downloaded using Meta's official link:")])]),t._v(" "),e("h3",{attrs:{id:"convert-llama-weights-to-hf-py"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#convert-llama-weights-to-hf-py"}},[t._v("#")]),t._v(" "),e("a",{attrs:{href:"https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model#convert_llama_weights_to_hfpy",target:"_blank",rel:"noopener noreferrer"}},[e("OutboundLink")],1),e("a",{attrs:{href:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py",target:"_blank",rel:"noopener noreferrer"}},[t._v("convert_llama_weights_to_hf.py"),e("OutboundLink")],1)]),t._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("python convert_llama_weights_to_hf.py --input_dir /path/to/LLaMA --model_size 7B --output_dir /path/to/outputs\n")])])]),e("p",[t._v("Two folders will be created at the end:")]),t._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("/path/to/outputs/llama-7b\n/path/to/outputs/tokenizer\n")])])]),e("ol",{attrs:{start:"2"}},[e("li",[t._v("Move the files inside "),e("code",[t._v("/path/to/outputs/tokenizer")]),t._v(" to "),e("code",[t._v("/path/to/outputs/llama-7b")]),t._v(":")])]),t._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("mv /path/to/outputs/tokenizer/* /path/to/outputs/llama-7b\n")])])]),e("ol",{attrs:{start:"3"}},[e("li",[t._v("Move the "),e("code",[t._v("llama-7b")]),t._v(" folder inside your "),e("code",[t._v("text-generation-webui/models")]),t._v(" folder.")])]),t._v(" "),e("h3",{attrs:{id:"starting-the-web-ui"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#starting-the-web-ui"}},[t._v("#")]),t._v(" "),e("a",{attrs:{href:"https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model#starting-the-web-ui",target:"_blank",rel:"noopener noreferrer"}},[e("OutboundLink")],1),t._v("Starting the web UI")]),t._v(" "),e("p",[t._v("python server.py --model llama-7b # use llama-7b-hf if you downloaded from decapoda")])])}),[],!1,null,null,null);e.default=r.exports}}]);