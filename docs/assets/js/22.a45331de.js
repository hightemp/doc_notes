(window.webpackJsonp=window.webpackJsonp||[]).push([[22],{296:function(e,t,o){"use strict";o.r(t);var a=o(14),n=Object(a.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("p",[e._v("https://www.reddit.com/r/LocalLLaMA/comments/11o6o3f/how_to_install_llama_8bit_and_4bit/")]),e._v(" "),t("p",[e._v("FAQ")]),e._v(" "),t("p",[t("strong",[e._v("Q:")]),e._v(" What is LLaMA?")]),e._v(" "),t("p",[t("strong",[e._v("A:")]),e._v(" LLaMA (Large Language Model Meta AI) is a foundational large language model designed primarily for researchers. Like other large language models, LLaMA works by taking a sequence of words as an input and predicts a next word to recursively generate text.")]),e._v(" "),t("p",[t("strong",[e._v("Q:")]),e._v(" Is LLaMA like ChatGPT?")]),e._v(" "),t("p",[t("strong",[e._v("A:")]),e._v(" No. LLaMA models are not finetuned for question answering. They should be prompted so that the expected answer is the natural continuation of the prompt. Nonetheless, it is possible to chat with LLaMA models in a way similar to ChatGPT but not near the same quality.")]),e._v(" "),t("p",[t("strong",[e._v("Q:")]),e._v(" What languages does LLaMA support?")]),e._v(" "),t("p",[t("strong",[e._v("A:")]),e._v(" Primarily English, but it should have limited capabilities for the following languages: bg, ca, cs, da, de, es, fr, hr, hu, it, nl, pl, pt, ro, ru, sl, sr, sv, uk.")]),e._v(" "),t("p",[t("strong",[e._v("Q:")]),e._v(" I've heard about Alpaca. What is that?")]),e._v(" "),t("p",[t("strong",[e._v("A:")]),e._v(" That refers to the "),t("a",{attrs:{href:"https://crfm.stanford.edu/2023/03/13/alpaca.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("Stanford Alpaca"),t("OutboundLink")],1),e._v(" project, an effort to build an instruction-following LLaMA model from the standard 7B LLaMA model. It has been shown to produce results similar to OpenAI's text-davinci-003. This guide contains instructions on trying out Alpaca using a few different methods.")]),e._v(" "),t("p",[e._v("8-bit Model Requirements")]),e._v(" "),t("p",[e._v("Model")]),e._v(" "),t("p",[e._v("VRAM Used")]),e._v(" "),t("p",[e._v("Minimum Total VRAM")]),e._v(" "),t("p",[e._v("Card examples")]),e._v(" "),t("p",[e._v("RAM/Swap to Load*")]),e._v(" "),t("p",[e._v("LLaMA-7B")]),e._v(" "),t("p",[e._v("9.2GB")]),e._v(" "),t("p",[e._v("10GB")]),e._v(" "),t("p",[e._v("3060 12GB, 3080 10GB")]),e._v(" "),t("p",[e._v("24 GB")]),e._v(" "),t("p",[e._v("LLaMA-13B")]),e._v(" "),t("p",[e._v("16.3GB")]),e._v(" "),t("p",[e._v("20GB")]),e._v(" "),t("p",[e._v("3090, 3090 Ti, 4090")]),e._v(" "),t("p",[e._v("32GB")]),e._v(" "),t("p",[e._v("LLaMA-30B")]),e._v(" "),t("p",[e._v("36GB")]),e._v(" "),t("p",[e._v("40GB")]),e._v(" "),t("p",[e._v("A6000 48GB, A100 40GB")]),e._v(" "),t("p",[e._v("72GB")]),e._v(" "),t("p",[e._v("LLaMA-65B")]),e._v(" "),t("p",[e._v("74GB")]),e._v(" "),t("p",[e._v("80GB")]),e._v(" "),t("p",[e._v("A100 80GB")]),e._v(" "),t("p",[e._v("144GB")]),e._v(" "),t("p",[e._v("*"),t("em",[e._v("System RAM, not VRAM, required to load the model, in addition to having enough VRAM. NOT required to RUN the model. You can use swap space if you do not have enough RAM.")])]),e._v(" "),t("p",[e._v("4-bit Model Requirements")]),e._v(" "),t("p",[e._v("Model")]),e._v(" "),t("p",[e._v("Minimum Total VRAM")]),e._v(" "),t("p",[e._v("Card examples")]),e._v(" "),t("p",[e._v("RAM/Swap to Load")]),e._v(" "),t("p",[e._v("LLaMA-7B")]),e._v(" "),t("p",[e._v("6GB")]),e._v(" "),t("p",[e._v("GTX 1660, 2060, AMD 5700 XT, RTX 3050, 3060")]),e._v(" "),t("p",[e._v("16 GB")]),e._v(" "),t("p",[e._v("LLaMA-13B")]),e._v(" "),t("p",[e._v("10GB")]),e._v(" "),t("p",[e._v("AMD 6900 XT, RTX 2060 12GB, 3060 12GB, 3080, A2000")]),e._v(" "),t("p",[e._v("32 GB")]),e._v(" "),t("p",[e._v("LLaMA-30B")]),e._v(" "),t("p",[e._v("20GB")]),e._v(" "),t("p",[e._v("RTX 3080 20GB, A4500, A5000, 3090, 4090, 6000, Tesla V100")]),e._v(" "),t("p",[e._v("64 GB")]),e._v(" "),t("p",[e._v("LLaMA-65B")]),e._v(" "),t("p",[e._v("40GB")]),e._v(" "),t("p",[e._v("A100 40GB, 2x3090, 2x4090, A40, RTX A6000, 8000")]),e._v(" "),t("p",[e._v("128 GB")]),e._v(" "),t("p",[e._v("Installing Windows Subsystem for Linux (WSL)")]),e._v(" "),t("blockquote",[t("blockquote",[t("p",[t("strong",[e._v("WSL installation is optional.<< If you do not want to install this, you can skip over to the Windows specific instructions below for 8-bit or 4-bit. This section requires an NVIDIA GPU.")])])])]),e._v(" "),t("p",[e._v('On Windows, you may receive better performance when using WSL. To install WSL using the instructions below, first ensure you are running at least Windows 10 version 2004 and higher (Build 19041 and higher) or Windows 11. To check for this, type info in the search box on your taskbar and then select System Information. Alternatively, hit Windows+R, type msinfo32 into the "Open" field, and then hit enter. Look at "Version" to see what version you are running.')]),e._v(" "),t("p",[t("strong",[e._v("Instructions:")])]),e._v(" "),t("ol",[t("li",[t("p",[e._v("Open Powershell in administrator mode")])]),e._v(" "),t("li",[t("p",[e._v("Enter the following command then restart your machine: wsl --install")])])]),e._v(" "),t("p",[e._v("This command will enable WSL, download and install the lastest Linux Kernel, use WSL2 as default, and download and install the Ubuntu Linux distribution.")]),e._v(" "),t("ol",{attrs:{start:"3"}},[t("li",[t("p",[e._v("After restart, Windows will finish installing Ubuntu. You'll be asked to create a username and password for Ubuntu. It has no bearing on your Windows username.")])]),e._v(" "),t("li",[t("p",[e._v("Windows will not automatically update or upgrade Ubuntu. Update and upgrade your packages by running the following command in the Ubuntu terminal (search for Ubuntu in the Start menu or taskbar and open the app): sudo apt update && sudo apt upgrade")])]),e._v(" "),t("li",[t("p",[e._v("You can now continue by following the Linux setup instructions for LLaMA. "),t("strong",[e._v("Check the necessary troubleshooting info below to resolve errors")]),e._v(". If you plan on using 4-bit LLaMA with WSL, you will need to install the WSL-Ubuntu CUDA toolkit using the instructions below.")])])]),e._v(" "),t("p",[t("strong",[e._v("Extra tips:")])]),e._v(" "),t("p",[e._v("To install conda, run the following inside the Ubuntu environment:")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh\n")])])]),t("p",[e._v("To find the name of a WSL distribution and uninstall it (afterward, you can create a new virtual machine environment by opening the app again):")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("wsl -l\nwsl --unregister <DistributionName>\n")])])]),t("p",[e._v("To access the web UI from another device on your local network, you will need to configure port forwarding:")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("netsh interface portproxy add v4tov4 listenaddress=0.0.0.0 listenport=7860 connectaddress=localhost connectport=7860\n")])])]),t("p",[t("strong",[e._v("Troubleshooting:")])]),e._v(" "),t("p",[e._v("If you will use 4-bit LLaMA with WSL, you must install the WSL-Ubuntu CUDA toolkit, and it must be 11.7. This CUDA toolkit will not overwrite your WSL2 driver unlike the default CUDA toolkit. Follow these steps:")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("sudo apt-key del 7fa2af80 \nwget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-wsl-ubuntu.pin\nsudo mv cuda-wsl-ubuntu.pin /etc/apt/preferences.d/cuda-repository-pin-600\nwget https://developer.download.nvidia.com/compute/cuda/11.7.0/local_installers/cuda-repo-wsl-ubuntu-11-7-local_11.7.0-1_amd64.deb\nsudo dpkg -i cuda-repo-wsl-ubuntu-11-7-local_11.7.0-1_amd64.deb\nsudo cp /var/cuda-repo-wsl-ubuntu-11-7-local/cuda-*-keyring.gpg /usr/share/keyrings/\nsudo apt-get update\nsudo apt-get -y install cuda\n")])])]),t("p",[e._v("In order to avoid a CUDA error when starting the web UI, you will need to apply the following fix as seen in "),t("a",{attrs:{href:"https://github.com/TimDettmers/bitsandbytes/issues/156#issuecomment-1462329713",target:"_blank",rel:"noopener noreferrer"}},[e._v("this comment"),t("OutboundLink")],1),e._v(" and issue "),t("a",{attrs:{href:"https://github.com/oobabooga/text-generation-webui/issues/400#issuecomment-1474876859",target:"_blank",rel:"noopener noreferrer"}},[e._v("#400"),t("OutboundLink")],1),e._v(":")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("cd /home/USERNAME/miniconda3/envs/textgen/lib/python3.10/site-packages/bitsandbytes/\ncp libbitsandbytes_cuda117.so libbitsandbytes_cpu.so\nconda install cudatoolkit\n")])])]),t("p",[e._v("If for some reason installing the WSL-Ubuntu CUDA toolkit does not work for you, "),t("a",{attrs:{href:"https://github.com/oobabooga/text-generation-webui/issues/416#issuecomment-1475078571",target:"_blank",rel:"noopener noreferrer"}},[e._v("this alternate fix"),t("OutboundLink")],1),e._v(" should resolve any errors relating to that.")]),e._v(" "),t("p",[e._v("You may also need to create symbolic links to get everything working correctly. Do not do this if the above commands resolve your errors. To create the symlinks, follow the instructions "),t("a",{attrs:{href:"https://github.com/microsoft/WSL/issues/5548#issuecomment-1292858815",target:"_blank",rel:"noopener noreferrer"}},[e._v("here"),t("OutboundLink")],1),e._v(" then restart your machine.")]),e._v(" "),t("p",[e._v("Installing 8-bit LLaMA with text-generation-webui")]),e._v(" "),t("p",[e._v("Linux:")]),e._v(" "),t("ol",[t("li",[t("p",[e._v("Follow the "),t("a",{attrs:{href:"https://github.com/oobabooga/text-generation-webui",target:"_blank",rel:"noopener noreferrer"}},[e._v("instructions here"),t("OutboundLink")],1),e._v(' under "Installation"')])]),e._v(" "),t("li",[t("p",[e._v("Download the desired Hugging Face converted model for LLaMA "),t("a",{attrs:{href:"https://huggingface.co/decapoda-research",target:"_blank",rel:"noopener noreferrer"}},[e._v("here"),t("OutboundLink")],1)])]),e._v(" "),t("li",[t("p",[e._v("Copy the entire model folder, for example llama-13b-hf, into text-generation-webui\\models")])]),e._v(" "),t("li",[t("p",[e._v("Run the following command in your conda environment: python server.py --model llama-13b-hf --load-in-8bit")])])]),e._v(" "),t("p",[e._v("Windows:")]),e._v(" "),t("ol",[t("li",[t("p",[e._v("Install "),t("a",{attrs:{href:"https://docs.conda.io/en/latest/miniconda.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("miniconda"),t("OutboundLink")],1)])]),e._v(" "),t("li",[t("p",[e._v("Activate conda via powershell, replacing USERNAME with your username: powershell -ExecutionPolicy ByPass -NoExit -Command \"& 'C:\\Users\\USERNAME\\miniconda3\\shell\\condabin\\conda-hook.ps1' ; conda activate 'C:\\Users\\USERNAME\\miniconda3' \"")])]),e._v(" "),t("li",[t("p",[e._v("Follow the "),t("a",{attrs:{href:"https://github.com/oobabooga/text-generation-webui",target:"_blank",rel:"noopener noreferrer"}},[e._v("instructions here"),t("OutboundLink")],1),e._v(' under "Installation", starting with the step "Create a new conda environment."')])]),e._v(" "),t("li",[t("p",[e._v("Download the desired Hugging Face converted model for LLaMA "),t("a",{attrs:{href:"https://huggingface.co/decapoda-research",target:"_blank",rel:"noopener noreferrer"}},[e._v("here"),t("OutboundLink")],1)])]),e._v(" "),t("li",[t("p",[e._v("Copy the entire model folder, for example llama-13b-hf, into text-generation-webui\\models")])]),e._v(" "),t("li",[t("p",[e._v("Download "),t("a",{attrs:{href:"https://github.com/DeXtmL/bitsandbytes-win-prebuilt",target:"_blank",rel:"noopener noreferrer"}},[e._v("libbitsandbytes_cuda116.dll"),t("OutboundLink")],1),e._v(" and put it in C:\\Users\\xxx\\miniconda3\\envs\\textgen\\lib\\site-packages\\bitsandbytes\\")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("In")]),e._v(" \\bitsandbytes\\cuda_setup\\main.py "),t("strong",[e._v("search for:")]),e._v(" "),t("code",[e._v("if not torch.cuda.is_available(): return 'libsbitsandbytes_cpu.so', None, None, None, None")]),e._v(" "),t("strong",[e._v("and replace with:")]),e._v(" "),t("code",[e._v("if torch.cuda.is_available(): return 'libbitsandbytes_cuda116.dll', None, None, None, None")])])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("In")]),e._v(" \\bitsandbytes\\cuda_setup\\main.py "),t("strong",[e._v("search for this")]),e._v(" "),t("strong",[e._v("twice:")]),e._v(" "),t("code",[e._v("self.lib = ct.cdll.LoadLibrary(binary_path)")]),e._v(" "),t("strong",[e._v("and replace with:")]),e._v(" "),t("code",[e._v("self.lib = ct.cdll.LoadLibrary(str(binary_path))")])])]),e._v(" "),t("li",[t("p",[e._v("Run the following command in your conda environment: python server.py --model llama-13b-hf --load-in-8bit")])])]),e._v(" "),t("p",[t("strong",[e._v("Note:")]),e._v(" for decapoda-research models, you must "),t("strong",[e._v("change")]),e._v(' "tokenizer_class": "LLaMATokenizer" '),t("strong",[e._v("to")]),e._v(' "tokenizer_class": "LlamaTokenizer" '),t("strong",[e._v("in")]),e._v(" text-generation-webui/models/llama-13b-hf/tokenizer_config.json")]),e._v(" "),t("p",[e._v("Installing 4-bit LLaMA with text-generation-webui")]),e._v(" "),t("p",[e._v("Linux:")]),e._v(" "),t("ol",[t("li",[t("p",[e._v("Follow the "),t("a",{attrs:{href:"https://github.com/oobabooga/text-generation-webui",target:"_blank",rel:"noopener noreferrer"}},[e._v("instructions here"),t("OutboundLink")],1),e._v(' under "Installation"')])]),e._v(" "),t("li",[t("p",[e._v("Continue with the 4-bit specific "),t("a",{attrs:{href:"https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model#4-bit-mode",target:"_blank",rel:"noopener noreferrer"}},[e._v("instructions here"),t("OutboundLink")],1)])])]),e._v(" "),t("p",[e._v("Windows (Step-by-Step):")]),e._v(" "),t("ol",[t("li",[t("p",[e._v("Install Build Tools for Visual Studio 2019 (has to be 2019) "),t("a",{attrs:{href:"https://learn.microsoft.com/en-us/visualstudio/releases/2019/history#release-dates-and-build-numbers",target:"_blank",rel:"noopener noreferrer"}},[e._v("here"),t("OutboundLink")],1),e._v('. Check "Desktop development with C++" when installing.')])]),e._v(" "),t("li",[t("p",[e._v("Install "),t("a",{attrs:{href:"https://docs.conda.io/en/latest/miniconda.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("miniconda"),t("OutboundLink")],1)])]),e._v(" "),t("li",[t("p",[e._v("Install Git from the "),t("a",{attrs:{href:"https://git-scm.com/download/win",target:"_blank",rel:"noopener noreferrer"}},[e._v("website"),t("OutboundLink")],1),e._v(" or simply with cmd prompt: winget install --id Git.Git -e --source winget")])]),e._v(" "),t("li",[t("p",[e._v('Open "x64 native tools command prompt" as admin')])]),e._v(" "),t("li",[t("p",[e._v("Activate conda, replacing USERNAME with your username: powershell -ExecutionPolicy ByPass -NoExit -Command \"& 'C:\\Users\\USERNAME\\miniconda3\\shell\\condabin\\conda-hook.ps1' ; conda activate 'C:\\Users\\USERNAME\\miniconda3' \"")])]),e._v(" "),t("li",[t("p",[e._v("conda create -n textgen python=3.10.9")])]),e._v(" "),t("li",[t("p",[e._v("conda activate textgen")])]),e._v(" "),t("li",[t("p",[e._v("conda install cuda -c nvidia/label/cuda-11.3.0 -c nvidia/label/cuda-11.3.1")])]),e._v(" "),t("li",[t("p",[e._v("git clone "),t("a",{attrs:{href:"https://github.com/oobabooga/text-generation-webui",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://github.com/oobabooga/text-generation-webui"),t("OutboundLink")],1)])]),e._v(" "),t("li",[t("p",[e._v("cd text-generation-webui")])]),e._v(" "),t("li",[t("p",[e._v("pip install -r requirements.txt")])]),e._v(" "),t("li",[t("p",[e._v("pip install torch==1.12+cu113 -f "),t("a",{attrs:{href:"https://download.pytorch.org/whl/torch_stable.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://download.pytorch.org/whl/torch_stable.html"),t("OutboundLink")],1)])]),e._v(" "),t("li",[t("p",[e._v("mkdir repositories")])]),e._v(" "),t("li",[t("p",[e._v("cd repositories")])]),e._v(" "),t("li",[t("p",[e._v("git clone "),t("a",{attrs:{href:"https://github.com/qwopqwop200/GPTQ-for-LLaMa",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://github.com/qwopqwop200/GPTQ-for-LLaMa"),t("OutboundLink")],1)])]),e._v(" "),t("li",[t("p",[e._v("cd GPTQ-for-LLaMa")])]),e._v(" "),t("li",[t("p",[e._v("git reset --hard c589c5456cc1c9e96065a5d285f8e3fac2cdb0fd")])]),e._v(" "),t("li",[t("p",[e._v("pip install ninja")])]),e._v(" "),t("li",[t("p",[e._v("$env:DISTUTILS_USE_SDK=1")])]),e._v(" "),t("li",[t("p",[e._v("python setup_cuda.py install")])]),e._v(" "),t("li",[t("p",[e._v("Download the 4-bit model of your choice and place it directly into your models folder. For instance, models/llama-13b-4bit-128g. The links for the updated 4-bit models are listed below in the models directory section. If you will use 7B 4-bit, download "),t("strong",[e._v("without group-size")]),e._v(". For 13B 4-bit and up, download "),t("strong",[e._v("with group-size")]),e._v(".")])]),e._v(" "),t("li",[t("p",[e._v("Run the following command in your conda environment: "),t("em",[t("strong",[e._v("without group-size")])]),e._v(" python server.py --model llama-7b-4bit --wbits 4 --no-stream "),t("em",[t("strong",[e._v("with group-size")])]),e._v(" python server.py --model llama-13b-4bit-128g --wbits 4 --groupsize 128 --no-stream")])])]),e._v(" "),t("p",[t("strong",[e._v("Note:")]),e._v(' If you get the error "CUDA Setup failed despite GPU being available", do the patch in steps 6-8 of the 8-bit instructions above.')]),e._v(" "),t("p",[e._v("Using Alpaca-LoRA with text-generation-webui")]),e._v(" "),t("p",[t("strong",[e._v("Make sure to follow the installation instructions for 8-bit LLaMA before using this.")])]),e._v(" "),t("p",[e._v("This is to reproduce the "),t("a",{attrs:{href:"https://crfm.stanford.edu/2023/03/13/alpaca.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("Stanford Alpaca"),t("OutboundLink")],1),e._v(" results using low-rank adaptation (LoRA). The LoRA model produces outputs comparable to the Stanford Alpaca model, which itself can produce results of similar quality to text-davinci-003. You will need the baseline "),t("a",{attrs:{href:"https://huggingface.co/decapoda-research/llama-7b-hf",target:"_blank",rel:"noopener noreferrer"}},[e._v("7B LLaMA model"),t("OutboundLink")],1),e._v(" downloaded and placed inside the models folder before following these steps.")]),e._v(" "),t("p",[t("strong",[e._v("Instructions:")])]),e._v(" "),t("ol",[t("li",[t("p",[e._v("Navigate to the text-generation-webui folder")])]),e._v(" "),t("li",[t("p",[e._v("Ensure it's up to date with: git pull "),t("a",{attrs:{href:"https://github.com/oobabooga/text-generation-webui",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://github.com/oobabooga/text-generation-webui"),t("OutboundLink")],1)])]),e._v(" "),t("li",[t("p",[e._v("Re-install the requirements "),t("em",[e._v("if needed")]),e._v(": pip install -r requirements.txt")])]),e._v(" "),t("li",[t("p",[e._v("Navigate to the loras folder and download the LoRA with: git lfs install && git clone "),t("a",{attrs:{href:"https://huggingface.co/tloen/alpaca-lora-7b",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://huggingface.co/tloen/alpaca-lora-7b"),t("OutboundLink")],1)])]),e._v(" "),t("li",[t("p",[e._v("Load LLaMa-7B in "),t("strong",[e._v("8-bit mode only")]),e._v(": python server.py --model llama-7b-hf --load-in-8bit")])]),e._v(" "),t("li",[t("p",[e._v("Select the LoRA in the Parameters tab")])])]),e._v(" "),t("p",[t("strong",[e._v("Notes:")])]),e._v(" "),t("p",[e._v('For this particular LoRA, the prompt must be formatted like this (the starting line must be below "Response"):')]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("Below is an instruction that describes a task. Write a response that appropriately completes the request.\n### Instruction:\nTell me about alpacas.\n### Response:\n<your cursor should be on this line>\n")])])]),t("p",[e._v("Message from the creator:")]),e._v(" "),t("blockquote",[t("p",[e._v("We're continually fixing bugs and conducting training runs, and the weights on the Hugging Face Hub are being updated accordingly. In particular, those facing issues with response lengths should make sure that they have the latest version of the weights and code.")])]),e._v(" "),t("p",[e._v("Tips and Output Settings in text-generation-webui")]),e._v(" "),t("ul",[t("li",[t("p",[e._v("For a ChatGPT/CharacterAI style chat, pass --cai-chat to server.py. For more info on flags, check "),t("a",{attrs:{href:"https://github.com/oobabooga/text-generation-webui#starting-the-web-ui",target:"_blank",rel:"noopener noreferrer"}},[e._v("here"),t("OutboundLink")],1),e._v(".")])]),e._v(" "),t("li",[t("p",[e._v("Character cards can be used to guide responses toward a desired output and improve results.")])]),e._v(" "),t("li",[t("p",[e._v("For a more creative chat, use: temp 0.72, rep pen 1.1, top_k 0, and top_p 0.73")])]),e._v(" "),t("li",[t("p",[e._v("For a more precise chat, use temp 0.7, repetition_penalty 1.1764705882352942 (1/0.85), top_k 40, and top_p 0.1")])]),e._v(" "),t("li",[t("p",[e._v("The "),t("a",{attrs:{href:"https://www.reddit.com/r/LocalLLaMA/wiki/index",target:"_blank",rel:"noopener noreferrer"}},[e._v("Getting Started"),t("OutboundLink")],1),e._v(" page of the wiki has a few extra tips on prompts and parameters.")])])]),e._v(" "),t("p",[e._v("For a quick reference, here is an example chat with LLaMA 13B:")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://preview.redd.it/yhyuzgg6d8oa1.png?width=634&format=png&auto=webp&v=enabled&s=e703895ad863c57239d9d4602f105558ce6f1a8f",target:"_blank",rel:"noopener noreferrer"}},[t("img",{attrs:{src:"https://preview.redd.it/yhyuzgg6d8oa1.png?width=634&format=png&auto=webp&v=enabled&s=e703895ad863c57239d9d4602f105558ce6f1a8f",alt:"r/LocalLLaMA - How to install LLaMA: 8-bit and 4-bit"}}),t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("Other ways to run LLaMA")]),e._v(" "),t("p",[t("strong",[e._v("If you have the hardware, it is recommended to use text-generation-webui for the best user experience.")])]),e._v(" "),t("p",[t("a",{attrs:{href:"https://github.com/ggerganov/llama.cpp",target:"_blank",rel:"noopener noreferrer"}},[e._v("llama.cpp"),t("OutboundLink")],1),e._v(": a plain C/C++ implementation that runs on the CPU. There is full documentation on the GitHub page for getting started with it.")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://github.com/antimatter15/alpaca.cpp",target:"_blank",rel:"noopener noreferrer"}},[e._v("alpaca.cpp"),t("OutboundLink")],1),e._v(": a quick and easy way to try a reproduction of the Stanford Alpaca model. The GitHub page explains the setup process. With the introduction of Alpaca into llama.cpp, alpaca.cpp is deprecated but functional.")]),e._v(" "),t("p",[e._v("Models Directory")]),e._v(" "),t("p",[t("strong",[e._v("Standard")]),e._v(" LLaMA models")]),e._v(" "),t("p",[e._v("7B: "),t("a",{attrs:{href:"https://huggingface.co/decapoda-research/llama-7b-hf",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://huggingface.co/decapoda-research/llama-7b-hf"),t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("13B: "),t("a",{attrs:{href:"https://huggingface.co/decapoda-research/llama-13b-hf",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://huggingface.co/decapoda-research/llama-13b-hf"),t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("30B: "),t("a",{attrs:{href:"https://huggingface.co/decapoda-research/llama-30b-hf",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://huggingface.co/decapoda-research/llama-30b-hf"),t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("65B: "),t("a",{attrs:{href:"https://huggingface.co/decapoda-research/llama-65b-hf",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://huggingface.co/decapoda-research/llama-65b-hf"),t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("7B-65B 4-bit without group-size (torrent file): "),t("a",{attrs:{href:"https://github.com/oobabooga/text-generation-webui/files/11069779/LLaMA-HF-4bit.zip",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://github.com/oobabooga/text-generation-webui/files/11069779/LLaMA-HF-4bit.zip"),t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("7B-65B 4-bit with group-size (torrent file): "),t("a",{attrs:{href:"https://github.com/oobabooga/text-generation-webui/files/11070361/LLaMA-HF-4bit-128g.zip",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://github.com/oobabooga/text-generation-webui/files/11070361/LLaMA-HF-4bit-128g.zip"),t("OutboundLink")],1)]),e._v(" "),t("p",[t("strong",[e._v("Finetuned")]),e._v(" LLaMA models and LoRAs")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://huggingface.co/tloen/alpaca-lora-7b",target:"_blank",rel:"noopener noreferrer"}},[e._v("Alpaca-LoRA 7B"),t("OutboundLink")],1)]),e._v(" "),t("p",[t("a",{attrs:{href:"https://huggingface.co/chansung/alpaca-lora-13b",target:"_blank",rel:"noopener noreferrer"}},[e._v("Alpaca LoRA 13B"),t("OutboundLink")],1)]),e._v(" "),t("p",[t("a",{attrs:{href:"https://huggingface.co/chansung/alpaca-lora-30b",target:"_blank",rel:"noopener noreferrer"}},[e._v("Alpaca LoRA 30B"),t("OutboundLink")],1)]),e._v(" "),t("p",[t("a",{attrs:{href:"https://huggingface.co/chavinlo/alpaca-native",target:"_blank",rel:"noopener noreferrer"}},[e._v("Alpaca Native (7B)"),t("OutboundLink")],1)]),e._v(" "),t("p",[t("a",{attrs:{href:"https://huggingface.co/ozcur/alpaca-native-4bit",target:"_blank",rel:"noopener noreferrer"}},[e._v("Alpaca Native 4-bit (7B)"),t("OutboundLink")],1)]),e._v(" "),t("p",[t("a",{attrs:{href:"https://huggingface.co/nomic-ai/gpt4all-lora",target:"_blank",rel:"noopener noreferrer"}},[e._v("gpt4all LoRA 7B"),t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("Resources used for this guide")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://github.com/oobabooga/text-generation-webui",target:"_blank",rel:"noopener noreferrer"}},[e._v("GitHub - oobabooga/text-generation-webui"),t("OutboundLink")],1)]),e._v(" "),t("p",[t("a",{attrs:{href:"https://github.com/oobabooga/text-generation-webui/issues/147",target:"_blank",rel:"noopener noreferrer"}},[e._v("Support for LLaMA models · Issue #147"),t("OutboundLink")],1)]),e._v(" "),t("p",[t("a",{attrs:{href:"https://huggingface.co/decapoda-research",target:"_blank",rel:"noopener noreferrer"}},[e._v("Hugging Face Models"),t("OutboundLink")],1)]),e._v(" "),t("p",[t("a",{attrs:{href:"https://github.com/tloen/alpaca-lora",target:"_blank",rel:"noopener noreferrer"}},[e._v("Alpaca-LoRA"),t("OutboundLink")],1)]),e._v(" "),t("p",[t("a",{attrs:{href:"https://huggingface.co/docs/api-inference/detailed_parameters",target:"_blank",rel:"noopener noreferrer"}},[e._v("Detailed parameters"),t("OutboundLink")],1)]),e._v(" "),t("p",[t("a",{attrs:{href:"https://github.com/qwopqwop200/GPTQ-for-LLaMa",target:"_blank",rel:"noopener noreferrer"}},[e._v("GPTQ for LLaMA"),t("OutboundLink")],1)]),e._v(" "),t("p",[t("a",{attrs:{href:"https://rentry.org/llama-tard-v2",target:"_blank",rel:"noopener noreferrer"}},[e._v("Rentry"),t("OutboundLink")],1)]),e._v(" "),t("p",[t("a",{attrs:{href:"https://docs.nvidia.com/cuda/wsl-user-guide/index.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("NVIDIA GPU Accelerated Computing on WSL 2"),t("OutboundLink")],1)]),e._v(" "),t("p",[t("a",{attrs:{href:"https://gist.github.com/shawwn/03ee2422c41ef253cbef61d8c317d9ab",target:"_blank",rel:"noopener noreferrer"}},[e._v("65b_sample.txt"),t("OutboundLink")],1)])])}),[],!1,null,null,null);t.default=n.exports}}]);