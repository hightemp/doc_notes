(window.webpackJsonp=window.webpackJsonp||[]).push([[31],{303:function(e,t,r){"use strict";r.r(t);var a=r(14),o=Object(a.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("p",[e._v("https://pytorch.org/tutorials/intermediate/ddp_tutorial.html")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel",target:"_blank",rel:"noopener noreferrer"}},[e._v("DistributedDataParallel"),t("OutboundLink")],1),e._v(" (DDP) реализует параллелизм данных на уровне модулей, который может выполняться на нескольких компьютерах. Приложения, использующие DDP, должны порождать несколько процессов и создавать один экземпляр DDP для каждого процесса. DDP использует коллективные коммуникации в пакете "),t("a",{attrs:{href:"https://pytorch.org/tutorials/intermediate/dist_tuto.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("torch.distributed"),t("OutboundLink")],1),e._v(" для синхронизации градиентов и буферов. В частности, DDP регистрирует хук автоградации для каждого параметра, заданного параметром, "),t("code",[e._v("model.parameters()")]),e._v("и хук срабатывает, когда соответствующий градиент вычисляется в обратном проходе. Затем DDP использует этот сигнал для запуска градиентной синхронизации между процессами. Дополнительные сведения см. в "),t("a",{attrs:{href:"https://pytorch.org/docs/master/notes/ddp.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("примечаниях к дизайну DDP ."),t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("Рекомендуемый способ использования DDP — создание одного процесса для каждой реплики модели, при этом реплика модели может охватывать несколько устройств. Процессы DDP можно размещать на одном и том же компьютере или на нескольких компьютерах, но устройства GPU не могут быть общими для процессов. Это руководство начинается с базового варианта использования DDP, а затем демонстрирует более сложные варианты использования, включая модели с контрольными точками и объединение DDP с параллельными моделями.")]),e._v(" "),t("p",[e._v("ПРИМЕЧАНИЕ")]),e._v(" "),t("p",[e._v("Код в этом руководстве выполняется на сервере с 8 графическими процессорами, но его можно легко обобщить на другие среды.")]),e._v(" "),t("h2",{attrs:{id:"сравнение-между-dataparallelиdistributeddataparallel"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#сравнение-между-dataparallelиdistributeddataparallel"}},[e._v("#")]),e._v(" Сравнение между "),t("code",[e._v("DataParallel")]),e._v("и"),t("code",[e._v("DistributedDataParallel")]),t("a",{attrs:{href:"https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#comparison-between-dataparallel-and-distributeddataparallel",target:"_blank",rel:"noopener noreferrer"}},[t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("Прежде чем мы углубимся, давайте проясним, почему, несмотря на дополнительную сложность, вы можете использовать "),t("code",[e._v("DistributedDataParallel")]),e._v("over "),t("code",[e._v("DataParallel")]),e._v(":")]),e._v(" "),t("ul",[t("li",[t("p",[e._v("Во-первых, "),t("code",[e._v("DataParallel")]),e._v("это однопроцессный, многопоточный и работает только на одной машине, тогда как "),t("code",[e._v("DistributedDataParallel")]),e._v("он многопроцессный и работает как для одно-, так и для многомашинного обучения. "),t("code",[e._v("DataParallel")]),e._v("обычно медленнее, чем "),t("code",[e._v("DistributedDataParallel")]),e._v("даже на одной машине, из-за соперничества GIL между потоками, реплицированной модели для каждой итерации и дополнительных накладных расходов, связанных с разбрасыванием входных данных и сбором выходных данных.")])]),e._v(" "),t("li",[t("p",[e._v("Напомним из "),t("a",{attrs:{href:"https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("предыдущего руководства"),t("OutboundLink")],1),e._v(" , что если ваша модель слишком велика для одного GPU, вы должны использовать "),t("strong",[e._v("параллельную модель")]),e._v(" , чтобы разделить ее на несколько GPU. "),t("code",[e._v("DistributedDataParallel")]),e._v("работает с "),t("strong",[e._v("моделью параллельно")]),e._v(" ; "),t("code",[e._v("DataParallel")]),e._v("в это время нет. Когда DDP сочетается с модельным параллелизмом, каждый процесс DDP будет использовать модельный параллелизм, а все процессы в совокупности будут использовать параллелизм данных.")])]),e._v(" "),t("li",[t("p",[e._v("Если ваша модель должна охватывать несколько компьютеров или если ваш вариант использования не соответствует парадигме параллелизма данных, см. "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/rpc.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("RPC API"),t("OutboundLink")],1),e._v(" для более общей поддержки распределенного обучения.")])])]),e._v(" "),t("h2",{attrs:{id:"базовыи-вариант-использования"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#базовыи-вариант-использования"}},[e._v("#")]),e._v(" Базовый вариант использования"),t("a",{attrs:{href:"https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#basic-use-case",target:"_blank",rel:"noopener noreferrer"}},[t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("Чтобы создать модуль DDP, вы должны сначала правильно настроить группы процессов. Более подробную информацию можно найти в "),t("a",{attrs:{href:"https://pytorch.org/tutorials/intermediate/dist_tuto.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("Написание распределенных приложений с помощью PyTorch"),t("OutboundLink")],1),e._v(" .")]),e._v(" "),t("p",[e._v("import os\nimport sys\nimport tempfile\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.multiprocessing as mp")]),e._v(" "),t("p",[e._v("from torch.nn.parallel import DistributedDataParallel as DDP")]),e._v(" "),t("h1",{attrs:{id:"on-windows-platform-the-torch-distributed-package-only"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#on-windows-platform-the-torch-distributed-package-only"}},[e._v("#")]),e._v(" On Windows platform, the torch.distributed package only")]),e._v(" "),t("h1",{attrs:{id:"supports-gloo-backend-filestore-and-tcpstore"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#supports-gloo-backend-filestore-and-tcpstore"}},[e._v("#")]),e._v(" supports Gloo backend, FileStore and TcpStore.")]),e._v(" "),t("h1",{attrs:{id:"for-filestore-set-init-method-parameter-in-init-process-group"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#for-filestore-set-init-method-parameter-in-init-process-group"}},[e._v("#")]),e._v(" For FileStore, set init_method parameter in init_process_group")]),e._v(" "),t("h1",{attrs:{id:"to-a-local-file-example-as-follow"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#to-a-local-file-example-as-follow"}},[e._v("#")]),e._v(" to a local file. Example as follow:")]),e._v(" "),t("h1",{attrs:{id:"init-method-file-f-libtmp-some-file"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#init-method-file-f-libtmp-some-file"}},[e._v("#")]),e._v(' init_method="file:///f:/libtmp/some_file"')]),e._v(" "),t("h1",{attrs:{id:"dist-init-process-group"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#dist-init-process-group"}},[e._v("#")]),e._v(" dist.init_process_group(")]),e._v(" "),t("h1",{attrs:{id:"gloo"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#gloo"}},[e._v("#")]),e._v(' "gloo",')]),e._v(" "),t("h1",{attrs:{id:"rank-rank"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#rank-rank"}},[e._v("#")]),e._v(" rank=rank,")]),e._v(" "),t("h1",{attrs:{id:"init-method-init-method"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#init-method-init-method"}},[e._v("#")]),e._v(" init_method=init_method,")]),e._v(" "),t("h1",{attrs:{id:"world-size-world-size"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#world-size-world-size"}},[e._v("#")]),e._v(" world_size=world_size)")]),e._v(" "),t("h1",{attrs:{id:"for-tcpstore-same-way-as-on-linux"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#for-tcpstore-same-way-as-on-linux"}},[e._v("#")]),e._v(" For TcpStore, same way as on Linux.")]),e._v(" "),t("p",[e._v("def setup(rank, world_size):\nos.environ['MASTER_ADDR'] = 'localhost'\nos.environ['MASTER_PORT'] = '12355'")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",[t("code",[e._v('# initialize the process group\ndist.init_process_group("gloo", rank=rank, world_size=world_size)\n')])])]),t("p",[e._v("def cleanup():\ndist.destroy_process_group()")]),e._v(" "),t("p",[e._v("Теперь давайте создадим игрушечный модуль, обернем его DDP и скормим ему фиктивные входные данные. Обратите внимание, что поскольку DDP передает состояния модели от процесса ранга 0 всем остальным процессам в конструкторе DDP, вам не нужно беспокоиться о том, что разные процессы DDP начинаются с разных начальных значений параметров модели.")]),e._v(" "),t("p",[e._v("class ToyModel(nn.Module):\ndef "),t("strong",[e._v("init")]),e._v("(self):\nsuper(ToyModel, self)."),t("strong",[e._v("init")]),e._v("()\nself.net1 = nn.Linear(10, 10)\nself.relu = nn.ReLU()\nself.net2 = nn.Linear(10, 5)")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",[t("code",[e._v("def forward(self, x):\n    return self.net2(self.relu(self.net1(x)))\n")])])]),t("p",[e._v('def demo_basic(rank, world_size):\nprint(f"Running basic DDP example on rank {rank}.")\nsetup(rank, world_size)')]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",[t("code",[e._v("# create model and move it to GPU with id rank\nmodel = ToyModel().to(rank)\nddp_model = DDP(model, device_ids=[rank])\n\nloss_fn = nn.MSELoss()\noptimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\noptimizer.zero_grad()\noutputs = ddp_model(torch.randn(20, 10))\nlabels = torch.randn(20, 5).to(rank)\nloss_fn(outputs, labels).backward()\noptimizer.step()\n\ncleanup()\n")])])]),t("p",[e._v("def run_demo(demo_fn, world_size):\nmp.spawn(demo_fn,\nargs=(world_size,),\nnprocs=world_size,\njoin=True)")]),e._v(" "),t("p",[e._v("Как видите, DDP оборачивает детали распределенной связи более низкого уровня и предоставляет чистый API, как если бы это была локальная модель. Связь с синхронизацией градиента происходит во время обратного прохода и перекрывается с обратным вычислением. Когда "),t("code",[e._v("backward()")]),e._v("возвращается, "),t("code",[e._v("param.grad")]),e._v("уже содержит синхронизированный тензор градиента. В базовых случаях использования DDP требуется всего несколько дополнительных LoC для настройки группы процессов. При применении DDP к более сложным случаям использования следует соблюдать некоторые предостережения.")]),e._v(" "),t("h2",{attrs:{id:"неравномерная-скорость-обработки"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#неравномерная-скорость-обработки"}},[e._v("#")]),e._v(" Неравномерная скорость обработки"),t("a",{attrs:{href:"https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#skewed-processing-speeds",target:"_blank",rel:"noopener noreferrer"}},[t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("В DDP конструктор, прямой и обратный проходы являются распределенными точками синхронизации. Предполагается, что разные процессы запускают одинаковое количество синхронизаций, достигают этих точек синхронизации в одном и том же порядке и входят в каждую точку синхронизации примерно в одно и то же время. В противном случае быстрые процессы могут прибыть раньше и превысить время ожидания отставших. Следовательно, пользователи несут ответственность за балансировку распределения рабочей нагрузки между процессами. Иногда перекосы в скорости обработки неизбежны, например, из-за сетевых задержек, конфликтов за ресурсы или непредсказуемых скачков рабочей нагрузки. Чтобы избежать тайм-аутов в таких ситуациях, убедитесь, что вы передаете достаточно большое "),t("code",[e._v("timeout")]),e._v("значение при вызове "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group",target:"_blank",rel:"noopener noreferrer"}},[e._v("init_process_group"),t("OutboundLink")],1),e._v(" .")]),e._v(" "),t("h2",{attrs:{id:"сохранение-и-загрузка-контрольных-точек"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#сохранение-и-загрузка-контрольных-точек"}},[e._v("#")]),e._v(" Сохранение и загрузка контрольных точек"),t("a",{attrs:{href:"https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#save-and-load-checkpoints",target:"_blank",rel:"noopener noreferrer"}},[t("OutboundLink")],1)]),e._v(" "),t("p",[t("code",[e._v("torch.save")]),e._v("Модули и "),t("code",[e._v("torch.load")]),e._v("контрольные точки обычно используются во время обучения и восстановления после контрольных точек. Подробнее см. в "),t("a",{attrs:{href:"https://pytorch.org/tutorials/beginner/saving_loading_models.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("разделе СОХРАНЕНИЕ И ЗАГРУЗКА МОДЕЛЕЙ ."),t("OutboundLink")],1),e._v(" При использовании DDP одной из оптимизаций является сохранение модели только в одном процессе, а затем загрузка ее во все процессы, что снижает накладные расходы на запись. Это правильно, потому что все процессы начинаются с одних и тех же параметров, а градиенты синхронизируются в обратных проходах, и, следовательно, оптимизаторы должны продолжать устанавливать для параметров одни и те же значения. Если вы используете эту оптимизацию, убедитесь, что ни один процесс не начнет загружаться до завершения сохранения. Кроме того, при загрузке модуля необходимо предоставить соответствующий "),t("code",[e._v("map_location")]),e._v(" аргумент, чтобы предотвратить доступ процесса к чужим устройствам. Если "),t("code",[e._v("map_location")]),e._v(" отсутствует,"),t("code",[e._v("torch.load")]),e._v("сначала загрузит модуль в ЦП, а затем скопирует каждый параметр туда, где он был сохранен, что приведет к тому, что все процессы на одной машине будут использовать один и тот же набор устройств. Дополнительные сведения о восстановлении после сбоев и поддержке эластичности см. в "),t("a",{attrs:{href:"https://pytorch.org/elastic",target:"_blank",rel:"noopener noreferrer"}},[e._v("TorchElastic"),t("OutboundLink")],1),e._v(" .")]),e._v(" "),t("p",[e._v('def demo_checkpoint(rank, world_size):\nprint(f"Running DDP checkpoint example on rank {rank}.")\nsetup(rank, world_size)')]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",[t("code",[e._v("model = ToyModel().to(rank)\nddp_model = DDP(model, device_ids=[rank])\n\nCHECKPOINT_PATH = tempfile.gettempdir() + \"/model.checkpoint\"\nif rank == 0:\n    # All processes should see same parameters as they all start from same\n    # random parameters and gradients are synchronized in backward passes.\n    # Therefore, saving it in one process is sufficient.\n    torch.save(ddp_model.state_dict(), CHECKPOINT_PATH)\n\n# Use a barrier() to make sure that process 1 loads the model after process\n# 0 saves it.\ndist.barrier()\n# configure map_location properly\nmap_location = {'cuda:%d' % 0: 'cuda:%d' % rank}\nddp_model.load_state_dict(\n    torch.load(CHECKPOINT_PATH, map_location=map_location))\n\nloss_fn = nn.MSELoss()\noptimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\noptimizer.zero_grad()\noutputs = ddp_model(torch.randn(20, 10))\nlabels = torch.randn(20, 5).to(rank)\n\nloss_fn(outputs, labels).backward()\noptimizer.step()\n\n# Not necessary to use a dist.barrier() to guard the file deletion below\n# as the AllReduce ops in the backward pass of DDP already served as\n# a synchronization.\n\nif rank == 0:\n    os.remove(CHECKPOINT_PATH)\n\ncleanup()\n")])])]),t("h2",{attrs:{id:"сочетание-ddp-с-параллелизмом-моделеи"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#сочетание-ddp-с-параллелизмом-моделеи"}},[e._v("#")]),e._v(" Сочетание DDP с параллелизмом моделей"),t("a",{attrs:{href:"https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#combining-ddp-with-model-parallelism",target:"_blank",rel:"noopener noreferrer"}},[t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("DDP также работает с моделями с несколькими графическими процессорами. DDP-обёртка моделей с несколькими GPU особенно полезна при обучении больших моделей с огромным объёмом данных.")]),e._v(" "),t("p",[e._v("class ToyMpModel(nn.Module):\ndef "),t("strong",[e._v("init")]),e._v("(self, dev0, dev1):\nsuper(ToyMpModel, self)."),t("strong",[e._v("init")]),e._v("()\nself.dev0 = dev0\nself.dev1 = dev1\nself.net1 = torch.nn.Linear(10, 10).to(dev0)\nself.relu = torch.nn.ReLU()\nself.net2 = torch.nn.Linear(10, 5).to(dev1)")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",[t("code",[e._v("def forward(self, x):\n    x = x.to(self.dev0)\n    x = self.relu(self.net1(x))\n    x = x.to(self.dev1)\n    return self.net2(x)\n")])])]),t("p",[e._v("При передаче модели с несколькими графическими процессорами в DDP "),t("code",[e._v("device_ids")]),e._v("НЕ "),t("code",[e._v("output_device")]),e._v(" ДОЛЖЕН устанавливаться. Входные и выходные данные будут помещены в соответствующие устройства либо приложением, либо "),t("code",[e._v("forward()")]),e._v("методом модели.")]),e._v(" "),t("p",[e._v('def demo_model_parallel(rank, world_size):\nprint(f"Running DDP with model parallel example on rank {rank}.")\nsetup(rank, world_size)')]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",[t("code",[e._v("# setup mp_model and devices for this process\ndev0 = (rank * 2) % world_size\ndev1 = (rank * 2 + 1) % world_size\nmp_model = ToyMpModel(dev0, dev1)\nddp_mp_model = DDP(mp_model)\n\nloss_fn = nn.MSELoss()\noptimizer = optim.SGD(ddp_mp_model.parameters(), lr=0.001)\n\noptimizer.zero_grad()\n# outputs will be on dev1\noutputs = ddp_mp_model(torch.randn(20, 10))\nlabels = torch.randn(20, 5).to(dev1)\nloss_fn(outputs, labels).backward()\noptimizer.step()\n\ncleanup()\n")])])]),t("p",[e._v("if "),t("strong",[e._v("name")]),e._v(' == "'),t("strong",[e._v("main")]),e._v('":\nn_gpus = torch.cuda.device_count()\nassert n_gpus >= 2, f"Requires at least 2 GPUs to run, but got {n_gpus}"\nworld_size = n_gpus\nrun_demo(demo_basic, world_size)\nrun_demo(demo_checkpoint, world_size)\nrun_demo(demo_model_parallel, world_size)')]),e._v(" "),t("h2",{attrs:{id:"инициализируите-ddp-с-помощью-torch-distributed-run-torchrun"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#инициализируите-ddp-с-помощью-torch-distributed-run-torchrun"}},[e._v("#")]),e._v(" Инициализируйте DDP с помощью torch.distributed.run/torchrun"),t("a",{attrs:{href:"https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#initialize-ddp-with-torch-distributed-run-torchrun",target:"_blank",rel:"noopener noreferrer"}},[t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("Мы можем использовать PyTorch Elastic, чтобы упростить код DDP и упростить инициализацию задания. Давайте все же воспользуемся примером Toymodel и создадим файл с именем "),t("code",[e._v("elastic_ddp.py")]),e._v(".")]),e._v(" "),t("p",[e._v("import torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.optim as optim")]),e._v(" "),t("p",[e._v("from torch.nn.parallel import DistributedDataParallel as DDP")]),e._v(" "),t("p",[e._v("class ToyModel(nn.Module):\ndef "),t("strong",[e._v("init")]),e._v("(self):\nsuper(ToyModel, self)."),t("strong",[e._v("init")]),e._v("()\nself.net1 = nn.Linear(10, 10)\nself.relu = nn.ReLU()\nself.net2 = nn.Linear(10, 5)")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",[t("code",[e._v("def forward(self, x):\n    return self.net2(self.relu(self.net1(x)))\n")])])]),t("p",[e._v('def demo_basic():\ndist.init_process_group("nccl")\nrank = dist.get_rank()\nprint(f"Start running basic DDP example on rank {rank}.")')]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",[t("code",[e._v("# create model and move it to GPU with id rank\ndevice_id = rank % torch.cuda.device_count()\nmodel = ToyModel().to(device_id)\nddp_model = DDP(model, device_ids=[device_id])\n\nloss_fn = nn.MSELoss()\noptimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\noptimizer.zero_grad()\noutputs = ddp_model(torch.randn(20, 10))\nlabels = torch.randn(20, 5).to(device_id)\nloss_fn(outputs, labels).backward()\noptimizer.step()\n")])])]),t("p",[e._v("if "),t("strong",[e._v("name")]),e._v(' == "'),t("strong",[e._v("main")]),e._v('":\ndemo_basic()')]),e._v(" "),t("p",[e._v("Затем можно запустить команду "),t("a",{attrs:{href:"https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#id1",target:"_blank",rel:"noopener noreferrer"}},[t("code",[e._v("torch elastic/torchrun<https://pytorch.org/docs/stable/elastic/quickstart.html>")]),e._v("__"),t("OutboundLink")],1),e._v(" на всех узлах, чтобы инициализировать задание DDP, созданное выше:")]),e._v(" "),t("p",[e._v("torchrun --nnodes=2 --nproc_per_node=8 --rdzv_id=100 --rdzv_backend=c10d --rdzv_endpoint=$MASTER_ADDR:29400 elastic_ddp.py")]),e._v(" "),t("p",[e._v("Мы запускаем сценарий DDP на двух хостах, и на каждом хосте мы запускаем 8 процессов, то есть мы запускаем его на 16 графических процессорах. Обратите внимание, что они "),t("code",[e._v("$MASTER_ADDR")]),e._v("должны быть одинаковыми для всех узлов.")]),e._v(" "),t("p",[e._v("Здесь torchrun запустит 8 процессов и вызовет "),t("code",[e._v("elastic_ddp.py")]),e._v(" каждый процесс на узле, на котором он запущен, но пользователю также необходимо применить инструменты управления кластером, такие как slurm, чтобы фактически запустить эту команду на 2 узлах.")]),e._v(" "),t("p",[e._v("Например, в кластере с поддержкой SLURM мы можем написать скрипт для запуска приведенной выше команды и установить его "),t("code",[e._v("MASTER_ADDR")]),e._v("как:")]),e._v(" "),t("p",[e._v("export MASTER_ADDR=$(scontrol show hostname ${SLURM_NODELIST} | head -n 1)")]),e._v(" "),t("p",[e._v("Затем мы можем просто запустить этот скрипт с помощью команды SLURM: . Конечно, это всего лишь пример; вы можете выбрать свои собственные инструменты планирования кластера, чтобы инициировать задание torchrun."),t("code",[e._v("srun --nodes=2 ./torchrun_script.sh")])]),e._v(" "),t("p",[e._v("Для получения дополнительной информации об эластичном беге можно проверить этот "),t("a",{attrs:{href:"https://pytorch.org/docs/stable/elastic/quickstart.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("документ с кратким руководством"),t("OutboundLink")],1),e._v(" , чтобы узнать больше.")])])}),[],!1,null,null,null);t.default=o.exports}}]);