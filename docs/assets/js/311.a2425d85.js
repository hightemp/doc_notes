(window.webpackJsonp=window.webpackJsonp||[]).push([[311],{583:function(e,t,r){"use strict";r.r(t);var a=r(14),n=Object(a.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("p",[e._v("https://github.com/kadekillary/scraping-with-rust")]),e._v(" "),t("p",[e._v("In this post I'm going to explore web scraping in Rust through a basic "),t("a",{attrs:{href:"https://news.ycombinator.com/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Hacker News"),t("OutboundLink")],1),e._v(" CLI. My hope is to point out resources for future Rustaceans interested in web scraping. Plus, highlight Rust's viability as a scripting language for everyday use. Lastly, feel free to send through a PR to help improve the repo or demos.")]),e._v(" "),t("blockquote",[t("p",[e._v("Note: for a simplififed recent version - "),t("a",{attrs:{href:"https://github.com/kadekillary/scraping-with-rust/tree/master/hack-scraper",target:"_blank",rel:"noopener noreferrer"}},[e._v("here"),t("OutboundLink")],1)])]),e._v(" "),t("h2",{attrs:{id:"scraping-ecosystem"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#scraping-ecosystem"}},[e._v("#")]),e._v(" "),t("a",{attrs:{href:"https://github.com/kadekillary/scraping-with-rust#scraping-ecosystem",target:"_blank",rel:"noopener noreferrer"}},[t("OutboundLink")],1),e._v("Scraping Ecosystem")]),e._v(" "),t("p",[e._v("Typically, when faced with web scraping most people don't run to a low-level systems programming language. Given the relative simplicity of scraping it would appear to be overkill. However, Rust makes this process fairly painless.")]),e._v(" "),t("p",[e._v("The main libraries, or crates, I'll be utilizing are the following:")]),e._v(" "),t("ul",[t("li",[t("p",[t("a",{attrs:{href:"https://github.com/seanmonstar/reqwest",target:"_blank",rel:"noopener noreferrer"}},[e._v("reqwest"),t("OutboundLink")],1)]),e._v(" "),t("blockquote",[t("p",[e._v("An easy and powerful Rust HTTP Client")])])]),e._v(" "),t("li",[t("p",[t("a",{attrs:{href:"https://github.com/programble/scraper",target:"_blank",rel:"noopener noreferrer"}},[e._v("scraper"),t("OutboundLink")],1)]),e._v(" "),t("blockquote",[t("p",[e._v("HTML parsing and querying with CSS selectors")])])]),e._v(" "),t("li",[t("p",[t("a",{attrs:{href:"https://github.com/utkarshkukreti/select.rs",target:"_blank",rel:"noopener noreferrer"}},[e._v("select.rs"),t("OutboundLink")],1)]),e._v(" "),t("blockquote",[t("p",[e._v("A Rust library to extract useful data from HTML documents, suitable for web scraping")])])])]),e._v(" "),t("p",[e._v("I'll present a couple different scripts to get a feel for each crate.")]),e._v(" "),t("h2",{attrs:{id:"grabbing-all-links"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#grabbing-all-links"}},[e._v("#")]),e._v(" "),t("a",{attrs:{href:"https://github.com/kadekillary/scraping-with-rust#grabbing-all-links",target:"_blank",rel:"noopener noreferrer"}},[t("OutboundLink")],1),e._v("Grabbing All Links")]),e._v(" "),t("p",[e._v("The first script will perform a fairly basic task: grabbing all links from the page. For this, we'll utilize "),t("code",[e._v("reqwest")]),e._v(" and "),t("code",[e._v("select.rs")]),e._v(". As you can see the syntax is fairly concise and straightforward.")]),e._v(" "),t("p",[e._v("cargo run --example grab_all_links")]),e._v(" "),t("p",[e._v("extern crate reqwest;\nextern crate select;")]),e._v(" "),t("p",[e._v("use select::document::Document;\nuse select::predicate::Name;")]),e._v(" "),t("p",[e._v('fn main() {\nhacker_news("https://news.ycombinator.com");\n}')]),e._v(" "),t("p",[e._v("fn hacker_news(url: &str) {\nlet mut resp = reqwest::get(url).unwrap();\nassert!(resp.status().is_success());")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",[t("code",[e._v('Document::from_read(resp)\n    .unwrap()\n    .find(Name("a"))\n    .filter_map(|n| n.attr("href"))\n    .for_each(|x| println!("{}", x));\n')])])]),t("p",[e._v("}")]),e._v(" "),t("p",[e._v("The main things to note are "),t("code",[e._v("unwrap()")]),e._v(" and the "),t("code",[e._v("|x|")]),e._v(" notation. The first is Rust's way of telling the compiler we don't care about error handling right now. "),t("code",[e._v("unwrap()")]),e._v(" will give us the value out of an "),t("code",[e._v("Option<T>")]),e._v(" for "),t("code",[e._v("Some(v)")]),e._v(", however if the value is "),t("code",[e._v("None")]),e._v(" the function will panic - not ideal for production settings. This is a common pattern when developing. The second notation is Rust's lambda syntax. Other than that, it's fairly straightforward. We send a get request to the Hacker News home page, then read in the HTML response to Document. Next we find all links and print them. If you run this you'll see the following:")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://camo.githubusercontent.com/24078c241b65d7b5d925baaa46d6d387c3c04db0b30c3fb94156e9674626bbcb/68747470733a2f2f692e696d6775722e636f6d2f645a49616b36542e706e67",target:"_blank",rel:"noopener noreferrer"}},[t("img",{attrs:{src:"https://camo.githubusercontent.com/24078c241b65d7b5d925baaa46d6d387c3c04db0b30c3fb94156e9674626bbcb/68747470733a2f2f692e696d6775722e636f6d2f645a49616b36542e706e67",alt:"all-links"}}),t("OutboundLink")],1)]),e._v(" "),t("h2",{attrs:{id:"using-css-selectors"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#using-css-selectors"}},[e._v("#")]),e._v(" "),t("a",{attrs:{href:"https://github.com/kadekillary/scraping-with-rust#using-css-selectors",target:"_blank",rel:"noopener noreferrer"}},[t("OutboundLink")],1),e._v("Using CSS Selectors")]),e._v(" "),t("p",[e._v("For the second example we'll use the "),t("code",[e._v("scraper")]),e._v(" crate. The main advantage of "),t("code",[e._v("scraper")]),e._v(" is using CSS selectors. A great tool for this is the Chrome extension "),t("a",{attrs:{href:"http://selectorgadget.com/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Selector Gadget"),t("OutboundLink")],1),e._v(". This extension makes grabbing elements trivial. All you'll need to do is navigate to your page of interest, click the icon and select.")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://camo.githubusercontent.com/eb0b49d2464d1ff8f0abc69f3db440e201f6fe21673c7fa5b3244451317fe2eb/68747470733a2f2f692e696d6775722e636f6d2f4e65354b5051452e706e67",target:"_blank",rel:"noopener noreferrer"}},[t("img",{attrs:{src:"https://camo.githubusercontent.com/eb0b49d2464d1ff8f0abc69f3db440e201f6fe21673c7fa5b3244451317fe2eb/68747470733a2f2f692e696d6775722e636f6d2f4e65354b5051452e706e67",alt:"css-select"}}),t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("Now that we know the post headline translates to "),t("code",[e._v(".storylink")]),e._v(" we can retrieve it with ease.")]),e._v(" "),t("blockquote",[t("p",[e._v("Note: not working at the moment - use as reference")])]),e._v(" "),t("p",[e._v("extern crate reqwest;\nextern crate scraper;")]),e._v(" "),t("p",[e._v("// importation syntax\nuse scraper::{Html, Selector};")]),e._v(" "),t("p",[e._v('fn main() {\nhn_headlines("https://news.ycombinator.com");\n}')]),e._v(" "),t("p",[e._v("fn hn_headlines(url: &str) {")]),e._v(" "),t("p",[e._v("let mut resp = reqwest::get(url).unwrap();\nassert!(resp.status().is_success());")]),e._v(" "),t("p",[e._v('let body = resp.text().unwrap();\n// parses string of HTML as a document\nlet fragment = Html::parse_document(&body);\n// parses based on a CSS selector\nlet stories = Selector::parse(".storylink").unwrap();')]),e._v(" "),t("p",[e._v('// iterate over elements matching our selector\nfor story in fragment.select(&stories) {\n// grab the headline text and place into a vector\nlet story_txt = story.text().collect::<Vec<_>>();\nprintln!("{:?}", story_txt);\n}\n}')]),e._v(" "),t("p",[e._v("Perhaps the most foreign part of this syntax is the "),t("code",[e._v("::")]),e._v(" annotations. The symbol merely designates a path. So, "),t("code",[e._v("Html::parse_document")]),e._v(" allows us to know that "),t("code",[e._v("parse_document()")]),e._v(" is a method on the "),t("code",[e._v("Html")]),e._v(" struct, which is from the crate "),t("code",[e._v("scraper")]),e._v(". Other than that, we read our get request's response into a document, specified our selector, and then looped over every instance collecting the headline in a vector and printed to stdout. The example output is below.")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://camo.githubusercontent.com/495d41587279abfd68266bc6ea24200cd7f68e5294f67c2d6795eb67be51cf81/68747470733a2f2f692e696d6775722e636f6d2f3958636b3867562e706e67",target:"_blank",rel:"noopener noreferrer"}},[t("img",{attrs:{src:"https://camo.githubusercontent.com/495d41587279abfd68266bc6ea24200cd7f68e5294f67c2d6795eb67be51cf81/68747470733a2f2f692e696d6775722e636f6d2f3958636b3867562e706e67",alt:"scraper-headline"}}),t("OutboundLink")],1)]),e._v(" "),t("h2",{attrs:{id:"more-than-one-attribute"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#more-than-one-attribute"}},[e._v("#")]),e._v(" "),t("a",{attrs:{href:"https://github.com/kadekillary/scraping-with-rust#more-than-one-attribute",target:"_blank",rel:"noopener noreferrer"}},[t("OutboundLink")],1),e._v("More Than One Attribute")]),e._v(" "),t("p",[e._v("At this point, all we've really done is grab a single element from a page, rather boring. In order to get something that can aid in the construction of the final project we'll need multiple attributes. We'll switch back to using the "),t("code",[e._v("select.rs")]),e._v(" crate for this task. This is due to an increased level of control over specifying exactly what we want.")]),e._v(" "),t("p",[e._v("The first thing to do in this situation is inspect the element of the page. Specifically, we want to know what our post section is called.")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://camo.githubusercontent.com/29af823af1855a6339bc89bc42991c767927fe408606331f24abc8a0d6ebf6e7/68747470733a2f2f692e696d6775722e636f6d2f716f634c6845322e6a7067",target:"_blank",rel:"noopener noreferrer"}},[t("img",{attrs:{src:"https://camo.githubusercontent.com/29af823af1855a6339bc89bc42991c767927fe408606331f24abc8a0d6ebf6e7/68747470733a2f2f692e696d6775722e636f6d2f716f634c6845322e6a7067",alt:"inspect"}}),t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("From the image it's pretty clear it's a class called "),t("code",[e._v('"athing"')]),e._v(". We need the top level attribute in order to iterate through every occurrence and select our desired fields.")]),e._v(" "),t("p",[e._v("cargo run --example rank_story_link")]),e._v(" "),t("p",[e._v("extern crate reqwest;\nextern crate select;")]),e._v(" "),t("p",[e._v("use select::document::Document;\nuse select::predicate::{Class, Name, Predicate};")]),e._v(" "),t("p",[e._v('fn main() {\nhacker_news("https://news.ycombinator.com");\n}')]),e._v(" "),t("p",[e._v("fn hacker_news(url: &str) {")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",[t("code",[e._v('let resp = reqwest::get(url).unwrap();\nassert!(resp.status().is_success());\n\nlet document = Document::from_read(resp).unwrap();\n\n// finding all instances of our class of interest\nfor node in document.find(Class("athing")) {\n    // grabbing the story rank\n    let rank = node.find(Class("rank")).next().unwrap();\n    // finding class, then selecting article title\n    let story = node.find(Class("title").descendant(Name("a")))\n        .next()\n        .unwrap()\n        .text();\n    // printing out | rank | story headline\n    println!("\\n | {} | {}\\n", rank.text(), story);\n    // same as above\n    let url = node.find(Class("title").descendant(Name("a"))).next().unwrap();\n    // however, we don\'t grab text\n    // instead find the "href" attribute, which gives us the url\n    println!("{:?}\\n", url.attr("href").unwrap());\n}\n')])])]),t("p",[e._v("}")]),e._v(" "),t("p",[e._v("We've now got a working scraper that will gives us the rank, headline and url. However, UI is important, so let's have a go at adding some visual flair.")]),e._v(" "),t("h2",{attrs:{id:"adding-some-panache"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#adding-some-panache"}},[e._v("#")]),e._v(" "),t("a",{attrs:{href:"https://github.com/kadekillary/scraping-with-rust#adding-some-panache",target:"_blank",rel:"noopener noreferrer"}},[t("OutboundLink")],1),e._v("Adding Some Panache")]),e._v(" "),t("p",[e._v("This next part will build off of the "),t("a",{attrs:{href:"https://github.com/phsym/prettytable-rs",target:"_blank",rel:"noopener noreferrer"}},[e._v("PrettyTable"),t("OutboundLink")],1),e._v(" crate. PrettyTable is a rust library to print aligned and formatted tables, as seen below.")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("+---------+------+---------+\n| ABC     | DEFG | HIJKLMN |\n+---------+------+---------+\n| foobar  | bar  | foo     |\n+---------+------+---------+\n| foobar2 | bar2 | foo2    |\n+---------+------+---------+\n")])])]),t("p",[e._v("One of the benefits of PrettyTable is it's ability add custom formatting. Thus, for our example we will add an orange background for a consistent look.")]),e._v(" "),t("p",[e._v("cargo run --example final_demo")]),e._v(" "),t("p",[e._v("// specifying we'll be using a macro from\n// the prettytable crate (ex: row!())\n#[macro_use]\nextern crate prettytable;\nextern crate reqwest;\nextern crate select;")]),e._v(" "),t("p",[e._v("use select::document::Document;\nuse select::predicate::{Class, Name, Predicate};\nuse prettytable::Table;")]),e._v(" "),t("p",[e._v('fn main() {\nhacker_news("https://news.ycombinator.com");\n}')]),e._v(" "),t("p",[e._v("fn hacker_news(url: &str) {")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",[t("code",[e._v('let resp = reqwest::get(url).unwrap();\nassert!(resp.status().is_success());\n\nlet document = Document::from_read(resp).unwrap();\n\nlet mut table = Table::new();\n\n// same as before\nfor node in document.find(Class("athing")) {\n    let rank = node.find(Class("rank")).next().unwrap();\n    let story = node.find(Class("title").descendant(Name("a")))\n        .next()\n        .unwrap()\n        .text();\n    let url = node.find(Class("title").descendant(Name("a")))\n        .next()\n        .unwrap();\n    let url_txt = url.attr("href").unwrap();\n    // shorten strings to make table aesthetically appealing\n    // otherwise table will look mangled by long URLs\n    let url_trim = url_txt.trim_left_matches(\'/\');\n    let rank_story = format!(" | {} | {}", rank.text(), story);\n    // [FdBybl->] specifies row formatting\n    // F (foreground) d (black text)\n    // B (background) y (yellow text) l (left-align)\n    table.add_row(row![FdBybl->rank_story]);\n    table.add_row(row![Fy->url_trim]);\n}\n// print table to stdout\ntable.printstd();\n')])])]),t("p",[e._v("}")]),e._v(" "),t("p",[e._v("The end result of running this script is as follows:")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://camo.githubusercontent.com/7724ef7fecea3407e393be90d8bd440a4ea143d74dfb3a7eb4f0dde226bc0ac1/68747470733a2f2f692e696d6775722e636f6d2f72376b384830442e706e67",target:"_blank",rel:"noopener noreferrer"}},[t("img",{attrs:{src:"https://camo.githubusercontent.com/7724ef7fecea3407e393be90d8bd440a4ea143d74dfb3a7eb4f0dde226bc0ac1/68747470733a2f2f692e696d6775722e636f6d2f72376b384830442e706e67",alt:"final"}}),t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("Hopefully, this brief intro serves as a good jumping off point for exploring Rust as an everyday tool. Despite Rust being a statically typed, compiled, and non-gc language it remains a joy to work with, especially "),t("a",{attrs:{href:"https://doc.rust-lang.org/cargo/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Cargo"),t("OutboundLink")],1),e._v(" - Rust's package manager. If you are considering learning a low level language for speed concerns, and are coming from a high-level language such as Python or Javasciprt, Rust is a fabolous choice.")]),e._v(" "),t("p",[t("em",[e._v("Here are a few resources to get up and running")]),e._v(":")]),e._v(" "),t("ul",[t("li",[t("a",{attrs:{href:"https://doc.rust-lang.org/book/second-edition/",target:"_blank",rel:"noopener noreferrer"}},[e._v("The Book"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://www.amazon.com/Programming-Rust-Fast-Systems-Development/dp/1491927283/ref=sr_1_1?ie=UTF8&qid=1515194775&sr=8-1&keywords=programming+rust",target:"_blank",rel:"noopener noreferrer"}},[e._v("Programming Rust"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://rustbyexample.com/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Rust by Example"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://rust-lang-nursery.github.io/rust-cookbook/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Rust Cookbook"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://users.rust-lang.org/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Rust Forum"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://www.reddit.com/r/rust/",target:"_blank",rel:"noopener noreferrer"}},[e._v("r/rust"),t("OutboundLink")],1)])])])}),[],!1,null,null,null);t.default=n.exports}}]);